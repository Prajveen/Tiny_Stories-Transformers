{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import sentencepiece as spm\n",
    "import gensim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json('train_data.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One day, a little girl named Lily found a need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there was a little car named...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One day, a little fish named Fin was swimming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, in a land full of trees, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there was a little girl name...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  One day, a little girl named Lily found a need...\n",
       "1  Once upon a time, there was a little car named...\n",
       "2  One day, a little fish named Fin was swimming ...\n",
       "3  Once upon a time, in a land full of trees, the...\n",
       "4  Once upon a time, there was a little girl name..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One day, a little girl named Lily found a need...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  One day, a little girl named Lily found a need..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df[:1]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inputs_and_labels(df, sp):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        tokens = text.split()  # Tokenize the text by splitting on whitespace\n",
    "\n",
    "        # Generate input by adding start of sentence token at the beginning\n",
    "        input_sequence = tokens\n",
    "\n",
    "        # Generate label by adding end of sentence token at the end\n",
    "        label_sequence = tokens\n",
    "\n",
    "        inputs.append(input_sequence)\n",
    "        labels.append(label_sequence)\n",
    "\n",
    "    # Convert input and label sequences to strings\n",
    "    input_strings = [' '.join(sequence) for sequence in inputs]\n",
    "    label_strings = [' '.join(sequence) for sequence in labels]\n",
    "\n",
    "    # Tokenize input strings and add <sos> token at the beginning\n",
    "    tokenized_inputs = []\n",
    "    input_ids = []\n",
    "    for sequence in input_strings:\n",
    "        tokenized_sequence = sp.encode_as_pieces(sequence)\n",
    "        tokenized_sequence = ['<sos>'] + tokenized_sequence  # Add <sos> token manually\n",
    "        input_ids.append([sp.piece_to_id('<sos>')] + sp.encode_as_ids(sequence))  # Get token IDs\n",
    "        tokenized_inputs.append(tokenized_sequence)\n",
    "\n",
    "    # Tokenize label strings and add </sos> token at the end\n",
    "    tokenized_labels = []\n",
    "    label_ids = []\n",
    "    for sequence in label_strings:\n",
    "        tokenized_sequence = sp.encode_as_pieces(sequence)\n",
    "        tokenized_sequence.append('</sos>')  # Add </sos> token manually at the end\n",
    "        label_ids.append(sp.encode_as_ids(sequence) + [sp.piece_to_id('</sos>')])  # Get token IDs\n",
    "        tokenized_labels.append(tokenized_sequence)\n",
    "\n",
    "    # Print tokenized input and label sequences\n",
    "    for i in range(len(inputs)):\n",
    "        print(\"Input Text:\", input_strings[i])\n",
    "        print(\"Tokenized Input:\", tokenized_inputs[i])\n",
    "        print(\"Input IDs:\", input_ids[i])\n",
    "        print(\"Label Text:\", label_strings[i])\n",
    "        print(\"Tokenized Label:\", tokenized_labels[i])\n",
    "        print(\"Label IDs:\", label_ids[i])\n",
    "\n",
    "    return input_ids, label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt. Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\" Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n",
      "Tokenized Input: ['<sos>', '▁one', '▁day', ',', '▁a', '▁little', '▁girl', '▁name', 'd', '▁lily', '▁found', '▁a', '▁needle', '▁in', '▁her', '▁room', '.', '▁she', '▁knew', '▁it', '▁was', '▁difficult', '▁to', '▁play', '▁with', '▁it', '▁', 'because', '▁it', '▁was', '▁sharp', '.', '▁lily', '▁wanted', '▁to', '▁share', '▁the', '▁needle', '▁with', '▁her', '▁mom', ',', '▁so', '▁she', '▁could', '▁sew', '▁a', '▁button', '▁on', '▁her', '▁shirt', '.', '▁lily', '▁went', '▁to', '▁her', '▁mom', '▁and', '▁said', ',', '▁\"', 'mom', ',', '▁i', '▁found', '▁this', '▁needle', '.', '▁can', '▁you', '▁share', '▁it', '▁with', '▁me', '▁and', '▁sew', '▁my', '▁shirt', '?\"', '▁her', '▁mom', '▁smiled', '▁and', '▁said', ',', '▁\"', 'yes', ',', '▁lily', ',', '▁we', '▁can', '▁share', '▁the', '▁needle', '▁and', '▁fix', '▁your', '▁shirt', '.\"', '▁together', ',', '▁they', '▁shared', '▁the', '▁needle', '▁and', '▁sew', 'ed', '▁the', '▁button', '▁on', '▁lily', \"'\", 's', '▁shirt', '.', '▁it', '▁was', '▁not', '▁difficult', '▁for', '▁them', '▁', 'because', '▁they', '▁were', '▁sharing', '▁and', '▁helping', '▁each', '▁other', '.', '▁after', '▁they', '▁finished', ',', '▁lily', '▁thank', 'ed', '▁her', '▁mom', '▁for', '▁sharing', '▁the', '▁needle', '▁and', '▁fix', 'ing', '▁her', '▁shirt', '.', '▁they', '▁both', '▁felt', '▁happy', '▁', 'because', '▁they', '▁had', '▁shared', '▁and', '▁worked', '▁together', '.']\n",
      "Input IDs: [0, 38, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4, 1614, 24, 14, 43, 6, 23, 11, 94, 2599, 8, 1293, 32, 14, 802, 3, 31, 68, 7, 14, 43, 5, 18, 6, 16, 749, 6, 49, 119, 149, 1614, 3, 66, 25, 259, 12, 24, 145, 5, 2599, 140, 802, 82, 14, 43, 76, 5, 18, 6, 16, 257, 6, 31, 6, 96, 66, 259, 4, 1614, 5, 524, 129, 802, 46, 104, 6, 13, 678, 4, 1614, 5, 2599, 20, 4, 1293, 32, 31, 17, 15, 802, 3, 12, 9, 60, 1455, 36, 64, 19, 230, 13, 50, 1901, 5, 820, 183, 125, 3, 167, 13, 589, 6, 31, 146, 20, 14, 43, 36, 1901, 4, 1614, 5, 524, 55, 14, 802, 3, 13, 261, 90, 44, 19, 230, 13, 29, 678, 5, 521, 104, 3]\n",
      "Label Text: One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt. Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\" Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n",
      "Tokenized Label: ['▁one', '▁day', ',', '▁a', '▁little', '▁girl', '▁name', 'd', '▁lily', '▁found', '▁a', '▁needle', '▁in', '▁her', '▁room', '.', '▁she', '▁knew', '▁it', '▁was', '▁difficult', '▁to', '▁play', '▁with', '▁it', '▁', 'because', '▁it', '▁was', '▁sharp', '.', '▁lily', '▁wanted', '▁to', '▁share', '▁the', '▁needle', '▁with', '▁her', '▁mom', ',', '▁so', '▁she', '▁could', '▁sew', '▁a', '▁button', '▁on', '▁her', '▁shirt', '.', '▁lily', '▁went', '▁to', '▁her', '▁mom', '▁and', '▁said', ',', '▁\"', 'mom', ',', '▁i', '▁found', '▁this', '▁needle', '.', '▁can', '▁you', '▁share', '▁it', '▁with', '▁me', '▁and', '▁sew', '▁my', '▁shirt', '?\"', '▁her', '▁mom', '▁smiled', '▁and', '▁said', ',', '▁\"', 'yes', ',', '▁lily', ',', '▁we', '▁can', '▁share', '▁the', '▁needle', '▁and', '▁fix', '▁your', '▁shirt', '.\"', '▁together', ',', '▁they', '▁shared', '▁the', '▁needle', '▁and', '▁sew', 'ed', '▁the', '▁button', '▁on', '▁lily', \"'\", 's', '▁shirt', '.', '▁it', '▁was', '▁not', '▁difficult', '▁for', '▁them', '▁', 'because', '▁they', '▁were', '▁sharing', '▁and', '▁helping', '▁each', '▁other', '.', '▁after', '▁they', '▁finished', ',', '▁lily', '▁thank', 'ed', '▁her', '▁mom', '▁for', '▁sharing', '▁the', '▁needle', '▁and', '▁fix', 'ing', '▁her', '▁shirt', '.', '▁they', '▁both', '▁felt', '▁happy', '▁', 'because', '▁they', '▁had', '▁shared', '▁and', '▁worked', '▁together', '.', '</sos>']\n",
      "Label IDs: [38, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4, 1614, 24, 14, 43, 6, 23, 11, 94, 2599, 8, 1293, 32, 14, 802, 3, 31, 68, 7, 14, 43, 5, 18, 6, 16, 749, 6, 49, 119, 149, 1614, 3, 66, 25, 259, 12, 24, 145, 5, 2599, 140, 802, 82, 14, 43, 76, 5, 18, 6, 16, 257, 6, 31, 6, 96, 66, 259, 4, 1614, 5, 524, 129, 802, 46, 104, 6, 13, 678, 4, 1614, 5, 2599, 20, 4, 1293, 32, 31, 17, 15, 802, 3, 12, 9, 60, 1455, 36, 64, 19, 230, 13, 50, 1901, 5, 820, 183, 125, 3, 167, 13, 589, 6, 31, 146, 20, 14, 43, 36, 1901, 4, 1614, 5, 524, 55, 14, 802, 3, 13, 261, 90, 44, 19, 230, 13, 29, 678, 5, 521, 104, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('small_m.model')\n",
    "\n",
    "source_ids, target_ids = generate_inputs_and_labels(train_df, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum ID value in source: 2599\n",
      "Maximum ID value in labels: 2599\n"
     ]
    }
   ],
   "source": [
    "# Flatten the list of lists\n",
    "flat_ids = [item for sublist in source_ids for item in sublist]\n",
    "\n",
    "# Find the maximum ID\n",
    "max_id = max(flat_ids)\n",
    "print(\"Maximum ID value in source:\", max_id)\n",
    "\n",
    "# Flatten the list of lists\n",
    "flat_ids = [item for sublist in target_ids for item in sublist]\n",
    "\n",
    "# Find the maximum ID\n",
    "max_id = max(flat_ids)\n",
    "print(\"Maximum ID value in labels:\", max_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "        pe = self._init_pe(max_seq_length, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_length = x.size(1)\n",
    "        pe = self.pe[:seq_length, :] if seq_length <= self.max_seq_length else self._init_pe(seq_length, self.d_model)\n",
    "        return x + pe\n",
    "\n",
    "    def _init_pe(self, seq_length, d_model):\n",
    "        pe = torch.zeros(seq_length, d_model)\n",
    "        position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        return pe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, decoder_input, decoder_mask):\n",
    "        attn_output = self.self_attn(decoder_input, decoder_input, decoder_input, decoder_mask)\n",
    "        x = self.norm1(decoder_input + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, tgt):\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return tgt_mask\n",
    "\n",
    "    def forward(self, decoder_input):\n",
    "        decoder_mask = self.generate_mask(decoder_input)\n",
    "        decoder_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(decoder_input)))\n",
    "\n",
    "        dec_output = decoder_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, decoder_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data shape: torch.Size([1, 165])\n",
      "Target data shape: torch.Size([1, 165])\n",
      "tensor([[   0,   38,   28,    6,    8,   37,   53,   86,   34,   31,  119,    8,\n",
      "         1614,   21,   14,  198,    3,   11,  185,   12,    9, 1455,    7,   54,\n",
      "           24,   12,   19,  230,   12,    9, 1316,    3,   31,   59,    7,  259,\n",
      "            4, 1614,   24,   14,   43,    6,   23,   11,   94, 2599,    8, 1293,\n",
      "           32,   14,  802,    3,   31,   68,    7,   14,   43,    5,   18,    6,\n",
      "           16,  749,    6,   49,  119,  149, 1614,    3,   66,   25,  259,   12,\n",
      "           24,  145,    5, 2599,  140,  802,   82,   14,   43,   76,    5,   18,\n",
      "            6,   16,  257,    6,   31,    6,   96,   66,  259,    4, 1614,    5,\n",
      "          524,  129,  802,   46,  104,    6,   13,  678,    4, 1614,    5, 2599,\n",
      "           20,    4, 1293,   32,   31,   17,   15,  802,    3,   12,    9,   60,\n",
      "         1455,   36,   64,   19,  230,   13,   50, 1901,    5,  820,  183,  125,\n",
      "            3,  167,   13,  589,    6,   31,  146,   20,   14,   43,   36, 1901,\n",
      "            4, 1614,    5,  524,   55,   14,  802,    3,   13,  261,   90,   44,\n",
      "           19,  230,   13,   29,  678,    5,  521,  104,    3]])\n",
      "tensor([[  38,   28,    6,    8,   37,   53,   86,   34,   31,  119,    8, 1614,\n",
      "           21,   14,  198,    3,   11,  185,   12,    9, 1455,    7,   54,   24,\n",
      "           12,   19,  230,   12,    9, 1316,    3,   31,   59,    7,  259,    4,\n",
      "         1614,   24,   14,   43,    6,   23,   11,   94, 2599,    8, 1293,   32,\n",
      "           14,  802,    3,   31,   68,    7,   14,   43,    5,   18,    6,   16,\n",
      "          749,    6,   49,  119,  149, 1614,    3,   66,   25,  259,   12,   24,\n",
      "          145,    5, 2599,  140,  802,   82,   14,   43,   76,    5,   18,    6,\n",
      "           16,  257,    6,   31,    6,   96,   66,  259,    4, 1614,    5,  524,\n",
      "          129,  802,   46,  104,    6,   13,  678,    4, 1614,    5, 2599,   20,\n",
      "            4, 1293,   32,   31,   17,   15,  802,    3,   12,    9,   60, 1455,\n",
      "           36,   64,   19,  230,   13,   50, 1901,    5,  820,  183,  125,    3,\n",
      "          167,   13,  589,    6,   31,  146,   20,   14,   43,   36, 1901,    4,\n",
      "         1614,    5,  524,   55,   14,  802,    3,   13,  261,   90,   44,   19,\n",
      "          230,   13,   29,  678,    5,  521,  104,    3,    0]])\n"
     ]
    }
   ],
   "source": [
    "# Convert lists to tensors\n",
    "source_data = torch.tensor(source_ids)\n",
    "target_data = torch.tensor(target_ids)\n",
    "\n",
    "# Print shapes\n",
    "print(\"Source data shape:\", source_data.shape)\n",
    "print(\"Target data shape:\", target_data.shape)\n",
    "\n",
    "print(source_data)\n",
    "print(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.713146209716797\n",
      "Epoch: 2, Loss: 8.490992546081543\n",
      "Epoch: 3, Loss: 8.327680587768555\n",
      "Epoch: 4, Loss: 8.103431701660156\n",
      "Epoch: 5, Loss: 7.946131229400635\n",
      "Epoch: 6, Loss: 7.802438259124756\n",
      "Epoch: 7, Loss: 7.6724348068237305\n",
      "Epoch: 8, Loss: 7.544612407684326\n",
      "Epoch: 9, Loss: 7.40691614151001\n",
      "Epoch: 10, Loss: 7.304545879364014\n",
      "Epoch: 11, Loss: 7.183355331420898\n",
      "Epoch: 12, Loss: 7.0761542320251465\n",
      "Epoch: 13, Loss: 6.965683460235596\n",
      "Epoch: 14, Loss: 6.849248886108398\n",
      "Epoch: 15, Loss: 6.7380146980285645\n",
      "Epoch: 16, Loss: 6.609580993652344\n",
      "Epoch: 17, Loss: 6.499388217926025\n",
      "Epoch: 18, Loss: 6.3643479347229\n",
      "Epoch: 19, Loss: 6.234854698181152\n",
      "Epoch: 20, Loss: 6.099053382873535\n",
      "Epoch: 21, Loss: 5.992619037628174\n",
      "Epoch: 22, Loss: 5.836920261383057\n",
      "Epoch: 23, Loss: 5.724179744720459\n",
      "Epoch: 24, Loss: 5.594740867614746\n",
      "Epoch: 25, Loss: 5.4804792404174805\n",
      "Epoch: 26, Loss: 5.3604960441589355\n",
      "Epoch: 27, Loss: 5.252935409545898\n",
      "Epoch: 28, Loss: 5.144591808319092\n",
      "Epoch: 29, Loss: 5.011971473693848\n",
      "Epoch: 30, Loss: 4.897127628326416\n",
      "Epoch: 31, Loss: 4.811193466186523\n",
      "Epoch: 32, Loss: 4.6751275062561035\n",
      "Epoch: 33, Loss: 4.574216365814209\n",
      "Epoch: 34, Loss: 4.440730571746826\n",
      "Epoch: 35, Loss: 4.342915058135986\n",
      "Epoch: 36, Loss: 4.248982906341553\n",
      "Epoch: 37, Loss: 4.1501545906066895\n",
      "Epoch: 38, Loss: 4.043149471282959\n",
      "Epoch: 39, Loss: 3.9722390174865723\n",
      "Epoch: 40, Loss: 3.8784828186035156\n",
      "Epoch: 41, Loss: 3.7701103687286377\n",
      "Epoch: 42, Loss: 3.6790242195129395\n",
      "Epoch: 43, Loss: 3.6051483154296875\n",
      "Epoch: 44, Loss: 3.5047247409820557\n",
      "Epoch: 45, Loss: 3.4307005405426025\n",
      "Epoch: 46, Loss: 3.3568873405456543\n",
      "Epoch: 47, Loss: 3.263046979904175\n",
      "Epoch: 48, Loss: 3.2083725929260254\n",
      "Epoch: 49, Loss: 3.1349081993103027\n",
      "Epoch: 50, Loss: 3.0568368434906006\n",
      "Epoch: 51, Loss: 3.017357587814331\n",
      "Epoch: 52, Loss: 2.9329464435577393\n",
      "Epoch: 53, Loss: 2.872149705886841\n",
      "Epoch: 54, Loss: 2.795377492904663\n",
      "Epoch: 55, Loss: 2.7682440280914307\n",
      "Epoch: 56, Loss: 2.6554343700408936\n",
      "Epoch: 57, Loss: 2.6110806465148926\n",
      "Epoch: 58, Loss: 2.5656678676605225\n",
      "Epoch: 59, Loss: 2.501333475112915\n",
      "Epoch: 60, Loss: 2.435018539428711\n",
      "Epoch: 61, Loss: 2.3924343585968018\n",
      "Epoch: 62, Loss: 2.304799795150757\n",
      "Epoch: 63, Loss: 2.293152332305908\n",
      "Epoch: 64, Loss: 2.2525830268859863\n",
      "Epoch: 65, Loss: 2.124685764312744\n",
      "Epoch: 66, Loss: 2.112422466278076\n",
      "Epoch: 67, Loss: 2.0845367908477783\n",
      "Epoch: 68, Loss: 2.033066511154175\n",
      "Epoch: 69, Loss: 1.9769316911697388\n",
      "Epoch: 70, Loss: 1.903058648109436\n",
      "Epoch: 71, Loss: 1.8805882930755615\n",
      "Epoch: 72, Loss: 1.833286166191101\n",
      "Epoch: 73, Loss: 1.7843635082244873\n",
      "Epoch: 74, Loss: 1.7303310632705688\n",
      "Epoch: 75, Loss: 1.7269107103347778\n",
      "Epoch: 76, Loss: 1.6281064748764038\n",
      "Epoch: 77, Loss: 1.6207820177078247\n",
      "Epoch: 78, Loss: 1.5610663890838623\n",
      "Epoch: 79, Loss: 1.5378508567810059\n",
      "Epoch: 80, Loss: 1.496500849723816\n",
      "Epoch: 81, Loss: 1.458106517791748\n",
      "Epoch: 82, Loss: 1.3960374593734741\n",
      "Epoch: 83, Loss: 1.377747893333435\n",
      "Epoch: 84, Loss: 1.3305370807647705\n",
      "Epoch: 85, Loss: 1.2855654954910278\n",
      "Epoch: 86, Loss: 1.252960443496704\n",
      "Epoch: 87, Loss: 1.2277692556381226\n",
      "Epoch: 88, Loss: 1.19570791721344\n",
      "Epoch: 89, Loss: 1.173485517501831\n",
      "Epoch: 90, Loss: 1.10931396484375\n",
      "Epoch: 91, Loss: 1.1148039102554321\n",
      "Epoch: 92, Loss: 1.0733084678649902\n",
      "Epoch: 93, Loss: 1.0436797142028809\n",
      "Epoch: 94, Loss: 0.9998900294303894\n",
      "Epoch: 95, Loss: 1.0045157670974731\n",
      "Epoch: 96, Loss: 0.9436824321746826\n",
      "Epoch: 97, Loss: 0.9281591773033142\n",
      "Epoch: 98, Loss: 0.8835258483886719\n",
      "Epoch: 99, Loss: 0.8839043378829956\n",
      "Epoch: 100, Loss: 0.8642457127571106\n",
      "Epoch: 101, Loss: 0.8311101198196411\n",
      "Epoch: 102, Loss: 0.8109006881713867\n",
      "Epoch: 103, Loss: 0.7859606146812439\n",
      "Epoch: 104, Loss: 0.7241721153259277\n",
      "Epoch: 105, Loss: 0.7355208992958069\n",
      "Epoch: 106, Loss: 0.7179283499717712\n",
      "Epoch: 107, Loss: 0.6914590001106262\n",
      "Epoch: 108, Loss: 0.6612168550491333\n",
      "Epoch: 109, Loss: 0.6603214740753174\n",
      "Epoch: 110, Loss: 0.6425201892852783\n",
      "Epoch: 111, Loss: 0.6227415800094604\n",
      "Epoch: 112, Loss: 0.6063621044158936\n",
      "Epoch: 113, Loss: 0.5843306183815002\n",
      "Epoch: 114, Loss: 0.5517823100090027\n",
      "Epoch: 115, Loss: 0.5350715517997742\n",
      "Epoch: 116, Loss: 0.5288674235343933\n",
      "Epoch: 117, Loss: 0.5454243421554565\n",
      "Epoch: 118, Loss: 0.5089917778968811\n",
      "Epoch: 119, Loss: 0.4869688153266907\n",
      "Epoch: 120, Loss: 0.45518651604652405\n",
      "Epoch: 121, Loss: 0.45383894443511963\n",
      "Epoch: 122, Loss: 0.4372793138027191\n",
      "Epoch: 123, Loss: 0.4226512014865875\n",
      "Epoch: 124, Loss: 0.4117138087749481\n",
      "Epoch: 125, Loss: 0.41730621457099915\n",
      "Epoch: 126, Loss: 0.4158972203731537\n",
      "Epoch: 127, Loss: 0.39504310488700867\n",
      "Epoch: 128, Loss: 0.37326425313949585\n",
      "Epoch: 129, Loss: 0.3521570563316345\n",
      "Epoch: 130, Loss: 0.35349711775779724\n",
      "Epoch: 131, Loss: 0.3586897850036621\n",
      "Epoch: 132, Loss: 0.3635673224925995\n",
      "Epoch: 133, Loss: 0.3707459270954132\n",
      "Epoch: 134, Loss: 0.3129766583442688\n",
      "Epoch: 135, Loss: 0.30199989676475525\n",
      "Epoch: 136, Loss: 0.31083935499191284\n",
      "Epoch: 137, Loss: 0.30170610547065735\n",
      "Epoch: 138, Loss: 0.3152411878108978\n",
      "Epoch: 139, Loss: 0.3095552921295166\n",
      "Epoch: 140, Loss: 0.2822704017162323\n",
      "Epoch: 141, Loss: 0.26541388034820557\n",
      "Epoch: 142, Loss: 0.25903627276420593\n",
      "Epoch: 143, Loss: 0.2452710121870041\n",
      "Epoch: 144, Loss: 0.24025137722492218\n",
      "Epoch: 145, Loss: 0.25885164737701416\n",
      "Epoch: 146, Loss: 0.23535819351673126\n",
      "Epoch: 147, Loss: 0.2481408417224884\n",
      "Epoch: 148, Loss: 0.2422836571931839\n",
      "Epoch: 149, Loss: 0.21038301289081573\n",
      "Epoch: 150, Loss: 0.21746593713760376\n",
      "Epoch: 151, Loss: 0.20688389241695404\n",
      "Epoch: 152, Loss: 0.221299409866333\n",
      "Epoch: 153, Loss: 0.21286164224147797\n",
      "Epoch: 154, Loss: 0.1816810965538025\n",
      "Epoch: 155, Loss: 0.17563341557979584\n",
      "Epoch: 156, Loss: 0.18596623837947845\n",
      "Epoch: 157, Loss: 0.17812661826610565\n",
      "Epoch: 158, Loss: 0.16962558031082153\n",
      "Epoch: 159, Loss: 0.1702103316783905\n",
      "Epoch: 160, Loss: 0.16208556294441223\n",
      "Epoch: 161, Loss: 0.16523127257823944\n",
      "Epoch: 162, Loss: 0.1624167561531067\n",
      "Epoch: 163, Loss: 0.15808840095996857\n",
      "Epoch: 164, Loss: 0.15248362720012665\n",
      "Epoch: 165, Loss: 0.1356755644083023\n",
      "Epoch: 166, Loss: 0.14033137261867523\n",
      "Epoch: 167, Loss: 0.14012068510055542\n",
      "Epoch: 168, Loss: 0.13816292583942413\n",
      "Epoch: 169, Loss: 0.13932442665100098\n",
      "Epoch: 170, Loss: 0.13426221907138824\n",
      "Epoch: 171, Loss: 0.12521743774414062\n",
      "Epoch: 172, Loss: 0.13524113595485687\n",
      "Epoch: 173, Loss: 0.11616078019142151\n",
      "Epoch: 174, Loss: 0.11669669300317764\n",
      "Epoch: 175, Loss: 0.11781153082847595\n",
      "Epoch: 176, Loss: 0.1261596381664276\n",
      "Epoch: 177, Loss: 0.11401854455471039\n",
      "Epoch: 178, Loss: 0.1156153678894043\n",
      "Epoch: 179, Loss: 0.10871510207653046\n",
      "Epoch: 180, Loss: 0.1014263778924942\n",
      "Epoch: 181, Loss: 0.11560560017824173\n",
      "Epoch: 182, Loss: 0.10180911421775818\n",
      "Epoch: 183, Loss: 0.09816857427358627\n",
      "Epoch: 184, Loss: 0.09618633985519409\n",
      "Epoch: 185, Loss: 0.10484280437231064\n",
      "Epoch: 186, Loss: 0.09170957654714584\n",
      "Epoch: 187, Loss: 0.09809929132461548\n",
      "Epoch: 188, Loss: 0.08857019245624542\n",
      "Epoch: 189, Loss: 0.0854608342051506\n",
      "Epoch: 190, Loss: 0.08572285622358322\n",
      "Epoch: 191, Loss: 0.09129729866981506\n",
      "Epoch: 192, Loss: 0.07964339107275009\n",
      "Epoch: 193, Loss: 0.08406593650579453\n",
      "Epoch: 194, Loss: 0.08509303629398346\n",
      "Epoch: 195, Loss: 0.09598646312952042\n",
      "Epoch: 196, Loss: 0.07981570065021515\n",
      "Epoch: 197, Loss: 0.08788131922483444\n",
      "Epoch: 198, Loss: 0.07488733530044556\n",
      "Epoch: 199, Loss: 0.07511627674102783\n",
      "Epoch: 200, Loss: 0.07864061743021011\n",
      "Epoch: 201, Loss: 0.07614047080278397\n",
      "Epoch: 202, Loss: 0.07145173847675323\n",
      "Epoch: 203, Loss: 0.07008937001228333\n",
      "Epoch: 204, Loss: 0.07282961159944534\n",
      "Epoch: 205, Loss: 0.06661563366651535\n",
      "Epoch: 206, Loss: 0.07436960935592651\n",
      "Epoch: 207, Loss: 0.06588926911354065\n",
      "Epoch: 208, Loss: 0.06577251851558685\n",
      "Epoch: 209, Loss: 0.06739481538534164\n",
      "Epoch: 210, Loss: 0.06689466536045074\n",
      "Epoch: 211, Loss: 0.06822369992733002\n",
      "Epoch: 212, Loss: 0.07492346316576004\n",
      "Epoch: 213, Loss: 0.06215086951851845\n",
      "Epoch: 214, Loss: 0.06439373642206192\n",
      "Epoch: 215, Loss: 0.06580571830272675\n",
      "Epoch: 216, Loss: 0.06630810350179672\n",
      "Epoch: 217, Loss: 0.06284291297197342\n",
      "Epoch: 218, Loss: 0.055961351841688156\n",
      "Epoch: 219, Loss: 0.060089513659477234\n",
      "Epoch: 220, Loss: 0.05397135391831398\n",
      "Epoch: 221, Loss: 0.06666295975446701\n",
      "Epoch: 222, Loss: 0.0526374913752079\n",
      "Epoch: 223, Loss: 0.04357679560780525\n",
      "Epoch: 224, Loss: 0.05605002120137215\n",
      "Epoch: 225, Loss: 0.04616135358810425\n",
      "Epoch: 226, Loss: 0.048721570521593094\n",
      "Epoch: 227, Loss: 0.05265352129936218\n",
      "Epoch: 228, Loss: 0.05550532042980194\n",
      "Epoch: 229, Loss: 0.054250024259090424\n",
      "Epoch: 230, Loss: 0.04534588381648064\n",
      "Epoch: 231, Loss: 0.0499781109392643\n",
      "Epoch: 232, Loss: 0.04252032935619354\n",
      "Epoch: 233, Loss: 0.04480592906475067\n",
      "Epoch: 234, Loss: 0.04287468641996384\n",
      "Epoch: 235, Loss: 0.057897090911865234\n",
      "Epoch: 236, Loss: 0.04198157414793968\n",
      "Epoch: 237, Loss: 0.04536477476358414\n",
      "Epoch: 238, Loss: 0.048397358506917953\n",
      "Epoch: 239, Loss: 0.04227644205093384\n",
      "Epoch: 240, Loss: 0.04325729236006737\n",
      "Epoch: 241, Loss: 0.03699662163853645\n",
      "Epoch: 242, Loss: 0.04019859805703163\n",
      "Epoch: 243, Loss: 0.03937842324376106\n",
      "Epoch: 244, Loss: 0.04175056144595146\n",
      "Epoch: 245, Loss: 0.0383625365793705\n",
      "Epoch: 246, Loss: 0.037877656519412994\n",
      "Epoch: 247, Loss: 0.0415646955370903\n",
      "Epoch: 248, Loss: 0.03456428274512291\n",
      "Epoch: 249, Loss: 0.03336867317557335\n",
      "Epoch: 250, Loss: 0.0318065881729126\n",
      "Epoch: 251, Loss: 0.054029565304517746\n",
      "Epoch: 252, Loss: 0.031410474330186844\n",
      "Epoch: 253, Loss: 0.03839236497879028\n",
      "Epoch: 254, Loss: 0.03629465773701668\n",
      "Epoch: 255, Loss: 0.03701479732990265\n",
      "Epoch: 256, Loss: 0.029888970777392387\n",
      "Epoch: 257, Loss: 0.03184688463807106\n",
      "Epoch: 258, Loss: 0.03496202453970909\n",
      "Epoch: 259, Loss: 0.030333830043673515\n",
      "Epoch: 260, Loss: 0.03433774411678314\n",
      "Epoch: 261, Loss: 0.02840084210038185\n",
      "Epoch: 262, Loss: 0.027853773906826973\n",
      "Epoch: 263, Loss: 0.028843605890870094\n",
      "Epoch: 264, Loss: 0.029510335996747017\n",
      "Epoch: 265, Loss: 0.047563619911670685\n",
      "Epoch: 266, Loss: 0.03332757577300072\n",
      "Epoch: 267, Loss: 0.03701496124267578\n",
      "Epoch: 268, Loss: 0.02773902751505375\n",
      "Epoch: 269, Loss: 0.02658461034297943\n",
      "Epoch: 270, Loss: 0.026090720668435097\n",
      "Epoch: 271, Loss: 0.03213861957192421\n",
      "Epoch: 272, Loss: 0.03418393433094025\n",
      "Epoch: 273, Loss: 0.024788416922092438\n",
      "Epoch: 274, Loss: 0.024023694917559624\n",
      "Epoch: 275, Loss: 0.0362800695002079\n",
      "Epoch: 276, Loss: 0.026492545381188393\n",
      "Epoch: 277, Loss: 0.023012546822428703\n",
      "Epoch: 278, Loss: 0.02393721602857113\n",
      "Epoch: 279, Loss: 0.021700134500861168\n",
      "Epoch: 280, Loss: 0.03391849622130394\n",
      "Epoch: 281, Loss: 0.022662967443466187\n",
      "Epoch: 282, Loss: 0.02269464172422886\n",
      "Epoch: 283, Loss: 0.023789163678884506\n",
      "Epoch: 284, Loss: 0.023091863840818405\n",
      "Epoch: 285, Loss: 0.027117129415273666\n",
      "Epoch: 286, Loss: 0.029384270310401917\n",
      "Epoch: 287, Loss: 0.019364401698112488\n",
      "Epoch: 288, Loss: 0.022069010883569717\n",
      "Epoch: 289, Loss: 0.021241508424282074\n",
      "Epoch: 290, Loss: 0.020474089309573174\n",
      "Epoch: 291, Loss: 0.022995581850409508\n",
      "Epoch: 292, Loss: 0.02510656975209713\n",
      "Epoch: 293, Loss: 0.020978346467018127\n",
      "Epoch: 294, Loss: 0.024352986365556717\n",
      "Epoch: 295, Loss: 0.021354522556066513\n",
      "Epoch: 296, Loss: 0.019095486029982567\n",
      "Epoch: 297, Loss: 0.017916226759552956\n",
      "Epoch: 298, Loss: 0.022810986265540123\n",
      "Epoch: 299, Loss: 0.017369765788316727\n",
      "Epoch: 300, Loss: 0.01945202238857746\n",
      "Epoch: 301, Loss: 0.023201720789074898\n",
      "Epoch: 302, Loss: 0.020073069259524345\n",
      "Epoch: 303, Loss: 0.018366612493991852\n",
      "Epoch: 304, Loss: 0.018161125481128693\n",
      "Epoch: 305, Loss: 0.020009297877550125\n",
      "Epoch: 306, Loss: 0.017949901521205902\n",
      "Epoch: 307, Loss: 0.027102889493107796\n",
      "Epoch: 308, Loss: 0.016973307356238365\n",
      "Epoch: 309, Loss: 0.016514483839273453\n",
      "Epoch: 310, Loss: 0.014578834176063538\n",
      "Epoch: 311, Loss: 0.018910503014922142\n",
      "Epoch: 312, Loss: 0.017930280417203903\n",
      "Epoch: 313, Loss: 0.017019057646393776\n",
      "Epoch: 314, Loss: 0.01486843079328537\n",
      "Epoch: 315, Loss: 0.026307737454771996\n",
      "Epoch: 316, Loss: 0.01576506532728672\n",
      "Epoch: 317, Loss: 0.018624432384967804\n",
      "Epoch: 318, Loss: 0.019221067428588867\n",
      "Epoch: 319, Loss: 0.01676858402788639\n",
      "Epoch: 320, Loss: 0.01645447500050068\n",
      "Epoch: 321, Loss: 0.01805763505399227\n",
      "Epoch: 322, Loss: 0.013424958102405071\n",
      "Epoch: 323, Loss: 0.012701677158474922\n",
      "Epoch: 324, Loss: 0.015057618729770184\n",
      "Epoch: 325, Loss: 0.0364188551902771\n",
      "Epoch: 326, Loss: 0.013465902768075466\n",
      "Epoch: 327, Loss: 0.01276889257133007\n",
      "Epoch: 328, Loss: 0.012240687385201454\n",
      "Epoch: 329, Loss: 0.013342427089810371\n",
      "Epoch: 330, Loss: 0.013143301010131836\n",
      "Epoch: 331, Loss: 0.014406715519726276\n",
      "Epoch: 332, Loss: 0.012868090532720089\n",
      "Epoch: 333, Loss: 0.01609591394662857\n",
      "Epoch: 334, Loss: 0.013149111531674862\n",
      "Epoch: 335, Loss: 0.0146247036755085\n",
      "Epoch: 336, Loss: 0.012337547726929188\n",
      "Epoch: 337, Loss: 0.011592025868594646\n",
      "Epoch: 338, Loss: 0.013408800587058067\n",
      "Epoch: 339, Loss: 0.011638606898486614\n",
      "Epoch: 340, Loss: 0.01181735098361969\n",
      "Epoch: 341, Loss: 0.011090751737356186\n",
      "Epoch: 342, Loss: 0.011749016121029854\n",
      "Epoch: 343, Loss: 0.011760847643017769\n",
      "Epoch: 344, Loss: 0.009502403438091278\n",
      "Epoch: 345, Loss: 0.010274996049702168\n",
      "Epoch: 346, Loss: 0.009848644956946373\n",
      "Epoch: 347, Loss: 0.011199027299880981\n",
      "Epoch: 348, Loss: 0.012238588184118271\n",
      "Epoch: 349, Loss: 0.00932356622070074\n",
      "Epoch: 350, Loss: 0.008795147761702538\n",
      "Epoch: 351, Loss: 0.009012274444103241\n",
      "Epoch: 352, Loss: 0.010756729170680046\n",
      "Epoch: 353, Loss: 0.009450246579945087\n",
      "Epoch: 354, Loss: 0.008769894950091839\n",
      "Epoch: 355, Loss: 0.009511880576610565\n",
      "Epoch: 356, Loss: 0.008805740624666214\n",
      "Epoch: 357, Loss: 0.009966601617634296\n",
      "Epoch: 358, Loss: 0.009048300795257092\n",
      "Epoch: 359, Loss: 0.008468679152429104\n",
      "Epoch: 360, Loss: 0.008278408087790012\n",
      "Epoch: 361, Loss: 0.008355556055903435\n",
      "Epoch: 362, Loss: 0.007875985465943813\n",
      "Epoch: 363, Loss: 0.010100788436830044\n",
      "Epoch: 364, Loss: 0.0076292529702186584\n",
      "Epoch: 365, Loss: 0.010900949127972126\n",
      "Epoch: 366, Loss: 0.007740363944321871\n",
      "Epoch: 367, Loss: 0.007489725016057491\n",
      "Epoch: 368, Loss: 0.0074590821750462055\n",
      "Epoch: 369, Loss: 0.00830319244414568\n",
      "Epoch: 370, Loss: 0.007287843152880669\n",
      "Epoch: 371, Loss: 0.008375996723771095\n",
      "Epoch: 372, Loss: 0.007992491126060486\n",
      "Epoch: 373, Loss: 0.006690617185086012\n",
      "Epoch: 374, Loss: 0.008884462527930737\n",
      "Epoch: 375, Loss: 0.007447604089975357\n",
      "Epoch: 376, Loss: 0.008092992007732391\n",
      "Epoch: 377, Loss: 0.005792940501123667\n",
      "Epoch: 378, Loss: 0.007645103149116039\n",
      "Epoch: 379, Loss: 0.006997940596193075\n",
      "Epoch: 380, Loss: 0.00755214923992753\n",
      "Epoch: 381, Loss: 0.00706059392541647\n",
      "Epoch: 382, Loss: 0.00833915825933218\n",
      "Epoch: 383, Loss: 0.007015075068920851\n",
      "Epoch: 384, Loss: 0.00874131266027689\n",
      "Epoch: 385, Loss: 0.007178161293268204\n",
      "Epoch: 386, Loss: 0.01058815885335207\n",
      "Epoch: 387, Loss: 0.008056145161390305\n",
      "Epoch: 388, Loss: 0.009578789584338665\n",
      "Epoch: 389, Loss: 0.007092901039868593\n",
      "Epoch: 390, Loss: 0.007694037165492773\n",
      "Epoch: 391, Loss: 0.007537342607975006\n",
      "Epoch: 392, Loss: 0.006708038970828056\n",
      "Epoch: 393, Loss: 0.006323657929897308\n",
      "Epoch: 394, Loss: 0.005911261774599552\n",
      "Epoch: 395, Loss: 0.006056238431483507\n",
      "Epoch: 396, Loss: 0.006176833063364029\n",
      "Epoch: 397, Loss: 0.006730095017701387\n",
      "Epoch: 398, Loss: 0.006295579019933939\n",
      "Epoch: 399, Loss: 0.030035056173801422\n",
      "Epoch: 400, Loss: 0.006707187741994858\n",
      "Epoch: 401, Loss: 0.011103816330432892\n",
      "Epoch: 402, Loss: 0.011008731089532375\n",
      "Epoch: 403, Loss: 0.03109772689640522\n",
      "Epoch: 404, Loss: 0.0074482262134552\n",
      "Epoch: 405, Loss: 0.005900512915104628\n",
      "Epoch: 406, Loss: 0.005853304173797369\n",
      "Epoch: 407, Loss: 0.008067307062447071\n",
      "Epoch: 408, Loss: 0.00783800520002842\n",
      "Epoch: 409, Loss: 0.021268188953399658\n",
      "Epoch: 410, Loss: 0.010691054165363312\n",
      "Epoch: 411, Loss: 0.009478590451180935\n",
      "Epoch: 412, Loss: 0.0067520346492528915\n",
      "Epoch: 413, Loss: 0.007675373926758766\n",
      "Epoch: 414, Loss: 0.025089098140597343\n",
      "Epoch: 415, Loss: 0.006613546051084995\n",
      "Epoch: 416, Loss: 0.008010286837816238\n",
      "Epoch: 417, Loss: 0.006799083668738604\n",
      "Epoch: 418, Loss: 0.005156833678483963\n",
      "Epoch: 419, Loss: 0.008947381749749184\n",
      "Epoch: 420, Loss: 0.04071574658155441\n",
      "Epoch: 421, Loss: 0.006895557977259159\n",
      "Epoch: 422, Loss: 0.006263886112719774\n",
      "Epoch: 423, Loss: 0.005519071128219366\n",
      "Epoch: 424, Loss: 0.012795573100447655\n",
      "Epoch: 425, Loss: 0.006522636394947767\n",
      "Epoch: 426, Loss: 0.005258047953248024\n",
      "Epoch: 427, Loss: 0.006040739361196756\n",
      "Epoch: 428, Loss: 0.004314099904149771\n",
      "Epoch: 429, Loss: 0.005803087260574102\n",
      "Epoch: 430, Loss: 0.006494786590337753\n",
      "Epoch: 431, Loss: 0.006540139205753803\n",
      "Epoch: 432, Loss: 0.006095795892179012\n",
      "Epoch: 433, Loss: 0.006245234981179237\n",
      "Epoch: 434, Loss: 0.005368566140532494\n",
      "Epoch: 435, Loss: 0.004070855211466551\n",
      "Epoch: 436, Loss: 0.005721616093069315\n",
      "Epoch: 437, Loss: 0.0038541657850146294\n",
      "Epoch: 438, Loss: 0.005799165461212397\n",
      "Epoch: 439, Loss: 0.011504950001835823\n",
      "Epoch: 440, Loss: 0.005470847710967064\n",
      "Epoch: 441, Loss: 0.005095009692013264\n",
      "Epoch: 442, Loss: 0.005599908530712128\n",
      "Epoch: 443, Loss: 0.004204640630632639\n",
      "Epoch: 444, Loss: 0.0037968975957483053\n",
      "Epoch: 445, Loss: 0.007858075201511383\n",
      "Epoch: 446, Loss: 0.0046370867639780045\n",
      "Epoch: 447, Loss: 0.0036938085686415434\n",
      "Epoch: 448, Loss: 0.007431870326399803\n",
      "Epoch: 449, Loss: 0.01190199889242649\n",
      "Epoch: 450, Loss: 0.006580649875104427\n",
      "Epoch: 451, Loss: 0.004315902013331652\n",
      "Epoch: 452, Loss: 0.004340990446507931\n",
      "Epoch: 453, Loss: 0.004623216576874256\n",
      "Epoch: 454, Loss: 0.005137705709785223\n",
      "Epoch: 455, Loss: 0.007224487606436014\n",
      "Epoch: 456, Loss: 0.005557989701628685\n",
      "Epoch: 457, Loss: 0.00414550956338644\n",
      "Epoch: 458, Loss: 0.005595539230853319\n",
      "Epoch: 459, Loss: 0.019880184903740883\n",
      "Epoch: 460, Loss: 0.003492470597848296\n",
      "Epoch: 461, Loss: 0.0035254901740700006\n",
      "Epoch: 462, Loss: 0.009624244645237923\n",
      "Epoch: 463, Loss: 0.004044710658490658\n",
      "Epoch: 464, Loss: 0.011965430341660976\n",
      "Epoch: 465, Loss: 0.004633967764675617\n",
      "Epoch: 466, Loss: 0.004034318029880524\n",
      "Epoch: 467, Loss: 0.003941363655030727\n",
      "Epoch: 468, Loss: 0.003967552445828915\n",
      "Epoch: 469, Loss: 0.003402796806767583\n",
      "Epoch: 470, Loss: 0.0038309108931571245\n",
      "Epoch: 471, Loss: 0.003387871664017439\n",
      "Epoch: 472, Loss: 0.003043423406779766\n",
      "Epoch: 473, Loss: 0.0037060892209410667\n",
      "Epoch: 474, Loss: 0.00413026986643672\n",
      "Epoch: 475, Loss: 0.003941500559449196\n",
      "Epoch: 476, Loss: 0.004812613595277071\n",
      "Epoch: 477, Loss: 0.003613096196204424\n",
      "Epoch: 478, Loss: 0.0031849397346377373\n",
      "Epoch: 479, Loss: 0.004126532003283501\n",
      "Epoch: 480, Loss: 0.003024417906999588\n",
      "Epoch: 481, Loss: 0.0028407727368175983\n",
      "Epoch: 482, Loss: 0.0038254857063293457\n",
      "Epoch: 483, Loss: 0.0025308679323643446\n",
      "Epoch: 484, Loss: 0.0032651457004249096\n",
      "Epoch: 485, Loss: 0.0034552740398794413\n",
      "Epoch: 486, Loss: 0.002873782766982913\n",
      "Epoch: 487, Loss: 0.003885226557031274\n",
      "Epoch: 488, Loss: 0.004273173864930868\n",
      "Epoch: 489, Loss: 0.002727004000917077\n",
      "Epoch: 490, Loss: 0.0025373329408466816\n",
      "Epoch: 491, Loss: 0.002961712656542659\n",
      "Epoch: 492, Loss: 0.0024440856650471687\n",
      "Epoch: 493, Loss: 0.002748814644291997\n",
      "Epoch: 494, Loss: 0.0024489962961524725\n",
      "Epoch: 495, Loss: 0.008408601395785809\n",
      "Epoch: 496, Loss: 0.003476642305031419\n",
      "Epoch: 497, Loss: 0.0021106242202222347\n",
      "Epoch: 498, Loss: 0.0024270096328109503\n",
      "Epoch: 499, Loss: 0.0019844849593937397\n",
      "Epoch: 500, Loss: 0.0021892304066568613\n",
      "Epoch: 501, Loss: 0.019891265779733658\n",
      "Epoch: 502, Loss: 0.004419112112373114\n",
      "Epoch: 503, Loss: 0.0037524572107940912\n",
      "Epoch: 504, Loss: 0.021998882293701172\n",
      "Epoch: 505, Loss: 0.0030019916594028473\n",
      "Epoch: 506, Loss: 0.04203055053949356\n",
      "Epoch: 507, Loss: 0.0035573651548475027\n",
      "Epoch: 508, Loss: 0.012152627110481262\n",
      "Epoch: 509, Loss: 0.002873568329960108\n",
      "Epoch: 510, Loss: 0.0031814530957490206\n",
      "Epoch: 511, Loss: 0.0026905073318630457\n",
      "Epoch: 512, Loss: 0.002396528609097004\n",
      "Epoch: 513, Loss: 0.0021993238478899\n",
      "Epoch: 514, Loss: 0.006096984725445509\n",
      "Epoch: 515, Loss: 0.00390566885471344\n",
      "Epoch: 516, Loss: 0.003226622473448515\n",
      "Epoch: 517, Loss: 0.0030841813422739506\n",
      "Epoch: 518, Loss: 0.0040245032869279385\n",
      "Epoch: 519, Loss: 0.0026053444016724825\n",
      "Epoch: 520, Loss: 0.022570833563804626\n",
      "Epoch: 521, Loss: 0.00281257601454854\n",
      "Epoch: 522, Loss: 0.01529892347753048\n",
      "Epoch: 523, Loss: 0.0030696315225213766\n",
      "Epoch: 524, Loss: 0.00312221166677773\n",
      "Epoch: 525, Loss: 0.008596619591116905\n",
      "Epoch: 526, Loss: 0.004485856741666794\n",
      "Epoch: 527, Loss: 0.007863781414926052\n",
      "Epoch: 528, Loss: 0.005390567239373922\n",
      "Epoch: 529, Loss: 0.006798655726015568\n",
      "Epoch: 530, Loss: 0.0030150776728987694\n",
      "Epoch: 531, Loss: 0.002330656396225095\n",
      "Epoch: 532, Loss: 0.003086582524701953\n",
      "Epoch: 533, Loss: 0.0030471645295619965\n",
      "Epoch: 534, Loss: 0.004084478598088026\n",
      "Epoch: 535, Loss: 0.005547529086470604\n",
      "Epoch: 536, Loss: 0.004019319079816341\n",
      "Epoch: 537, Loss: 0.0027630929835140705\n",
      "Epoch: 538, Loss: 0.0023225946351885796\n",
      "Epoch: 539, Loss: 0.002311629941686988\n",
      "Epoch: 540, Loss: 0.0023729922249913216\n",
      "Epoch: 541, Loss: 0.0020208628848195076\n",
      "Epoch: 542, Loss: 0.003117201616987586\n",
      "Epoch: 543, Loss: 0.0027028177864849567\n",
      "Epoch: 544, Loss: 0.002720318268984556\n",
      "Epoch: 545, Loss: 0.0025833058170974255\n",
      "Epoch: 546, Loss: 0.00314726447686553\n",
      "Epoch: 547, Loss: 0.0022105963435024023\n",
      "Epoch: 548, Loss: 0.0038283350877463818\n",
      "Epoch: 549, Loss: 0.007353024557232857\n",
      "Epoch: 550, Loss: 0.0023663933388888836\n",
      "Epoch: 551, Loss: 0.002465987578034401\n",
      "Epoch: 552, Loss: 0.0019277555402368307\n",
      "Epoch: 553, Loss: 0.0021533409599214792\n",
      "Epoch: 554, Loss: 0.0019081853097304702\n",
      "Epoch: 555, Loss: 0.0021254979074001312\n",
      "Epoch: 556, Loss: 0.003972574137151241\n",
      "Epoch: 557, Loss: 0.0028347508050501347\n",
      "Epoch: 558, Loss: 0.0069185784086585045\n",
      "Epoch: 559, Loss: 0.004423893056809902\n",
      "Epoch: 560, Loss: 0.002514983993023634\n",
      "Epoch: 561, Loss: 0.0015302416868507862\n",
      "Epoch: 562, Loss: 0.001806958462111652\n",
      "Epoch: 563, Loss: 0.001473990734666586\n",
      "Epoch: 564, Loss: 0.0020227187778800726\n",
      "Epoch: 565, Loss: 0.003065419616177678\n",
      "Epoch: 566, Loss: 0.001687877462245524\n",
      "Epoch: 567, Loss: 0.0017022016691043973\n",
      "Epoch: 568, Loss: 0.0024663512594997883\n",
      "Epoch: 569, Loss: 0.0016784628387540579\n",
      "Epoch: 570, Loss: 0.0015710270963609219\n",
      "Epoch: 571, Loss: 0.0020905942656099796\n",
      "Epoch: 572, Loss: 0.0017044463893398643\n",
      "Epoch: 573, Loss: 0.02308810129761696\n",
      "Epoch: 574, Loss: 0.001225887332111597\n",
      "Epoch: 575, Loss: 0.0013478383189067245\n",
      "Epoch: 576, Loss: 0.002673008479177952\n",
      "Epoch: 577, Loss: 0.001751153962686658\n",
      "Epoch: 578, Loss: 0.0012948382645845413\n",
      "Epoch: 579, Loss: 0.005401097238063812\n",
      "Epoch: 580, Loss: 0.0021654395386576653\n",
      "Epoch: 581, Loss: 0.001289458479732275\n",
      "Epoch: 582, Loss: 0.0015138707822188735\n",
      "Epoch: 583, Loss: 0.0017561413114890456\n",
      "Epoch: 584, Loss: 0.014882751740515232\n",
      "Epoch: 585, Loss: 0.0037445672787725925\n",
      "Epoch: 586, Loss: 0.053413040935993195\n",
      "Epoch: 587, Loss: 0.0015287199057638645\n",
      "Epoch: 588, Loss: 0.00140810024458915\n",
      "Epoch: 589, Loss: 0.0016691399505361915\n",
      "Epoch: 590, Loss: 0.0014121937565505505\n",
      "Epoch: 591, Loss: 0.001484010135754943\n",
      "Epoch: 592, Loss: 0.0015760513488203287\n",
      "Epoch: 593, Loss: 0.0019921166822314262\n",
      "Epoch: 594, Loss: 0.0016274128574877977\n",
      "Epoch: 595, Loss: 0.002330637304112315\n",
      "Epoch: 596, Loss: 0.001586456666700542\n",
      "Epoch: 597, Loss: 0.0040274630300700665\n",
      "Epoch: 598, Loss: 0.001972196390852332\n",
      "Epoch: 599, Loss: 0.004049982409924269\n",
      "Epoch: 600, Loss: 0.0015198979526758194\n",
      "Epoch: 601, Loss: 0.004649215843528509\n",
      "Epoch: 602, Loss: 0.0019634610507637262\n",
      "Epoch: 603, Loss: 0.001749698189087212\n",
      "Epoch: 604, Loss: 0.001959731802344322\n",
      "Epoch: 605, Loss: 0.0014311623526737094\n",
      "Epoch: 606, Loss: 0.0020134635269641876\n",
      "Epoch: 607, Loss: 0.0011659725569188595\n",
      "Epoch: 608, Loss: 0.0013226771261543036\n",
      "Epoch: 609, Loss: 0.0016774635296314955\n",
      "Epoch: 610, Loss: 0.0013434347929432988\n",
      "Epoch: 611, Loss: 0.0013818014413118362\n",
      "Epoch: 612, Loss: 0.003494874807074666\n",
      "Epoch: 613, Loss: 0.0011386859696358442\n",
      "Epoch: 614, Loss: 0.0012977919541299343\n",
      "Epoch: 615, Loss: 0.01635417342185974\n",
      "Epoch: 616, Loss: 0.0010544316610321403\n",
      "Epoch: 617, Loss: 0.0013671142514795065\n",
      "Epoch: 618, Loss: 0.001396025880239904\n",
      "Epoch: 619, Loss: 0.0015773079358041286\n",
      "Epoch: 620, Loss: 0.0035404253285378218\n",
      "Epoch: 621, Loss: 0.007254157215356827\n",
      "Epoch: 622, Loss: 0.001179142971523106\n",
      "Epoch: 623, Loss: 0.006908480543643236\n",
      "Epoch: 624, Loss: 0.0012021820293739438\n",
      "Epoch: 625, Loss: 0.0016657700762152672\n",
      "Epoch: 626, Loss: 0.0008343904046341777\n",
      "Epoch: 627, Loss: 0.001138131134212017\n",
      "Epoch: 628, Loss: 0.001317982212640345\n",
      "Epoch: 629, Loss: 0.0014119950355961919\n",
      "Epoch: 630, Loss: 0.0009168752003461123\n",
      "Epoch: 631, Loss: 0.0010331518715247512\n",
      "Epoch: 632, Loss: 0.0030153628904372454\n",
      "Epoch: 633, Loss: 0.0023446050472557545\n",
      "Epoch: 634, Loss: 0.0022412464022636414\n",
      "Epoch: 635, Loss: 0.0017871626187115908\n",
      "Epoch: 636, Loss: 0.0010795947164297104\n",
      "Epoch: 637, Loss: 0.0010177161311730742\n",
      "Epoch: 638, Loss: 0.0011922449339181185\n",
      "Epoch: 639, Loss: 0.0008893182966858149\n",
      "Epoch: 640, Loss: 0.001621050643734634\n",
      "Epoch: 641, Loss: 0.0009819942060858011\n",
      "Epoch: 642, Loss: 0.0018122663022950292\n",
      "Epoch: 643, Loss: 0.00149748963303864\n",
      "Epoch: 644, Loss: 0.0007326006889343262\n",
      "Epoch: 645, Loss: 0.0033934684470295906\n",
      "Epoch: 646, Loss: 0.023512238636612892\n",
      "Epoch: 647, Loss: 0.0015501719899475574\n",
      "Epoch: 648, Loss: 0.0013154507614672184\n",
      "Epoch: 649, Loss: 0.0008758381009101868\n",
      "Epoch: 650, Loss: 0.0011978567345067859\n",
      "Epoch: 651, Loss: 0.022060060873627663\n",
      "Epoch: 652, Loss: 0.002865539165213704\n",
      "Epoch: 653, Loss: 0.0014556164387613535\n",
      "Epoch: 654, Loss: 0.0016130761941894889\n",
      "Epoch: 655, Loss: 0.0011299196630716324\n",
      "Epoch: 656, Loss: 0.0008982523577287793\n",
      "Epoch: 657, Loss: 0.03141941875219345\n",
      "Epoch: 658, Loss: 0.0009210889111272991\n",
      "Epoch: 659, Loss: 0.009053308516740799\n",
      "Epoch: 660, Loss: 0.0010157765354961157\n",
      "Epoch: 661, Loss: 0.0011895818170160055\n",
      "Epoch: 662, Loss: 0.0038993204943835735\n",
      "Epoch: 663, Loss: 0.003614839632064104\n",
      "Epoch: 664, Loss: 0.0011904325801879168\n",
      "Epoch: 665, Loss: 0.017904656007885933\n",
      "Epoch: 666, Loss: 0.0019081344362348318\n",
      "Epoch: 667, Loss: 0.026004549115896225\n",
      "Epoch: 668, Loss: 0.006981214974075556\n",
      "Epoch: 669, Loss: 0.001230102265253663\n",
      "Epoch: 670, Loss: 0.0016070989659056067\n",
      "Epoch: 671, Loss: 0.018023421987891197\n",
      "Epoch: 672, Loss: 0.0015937818679958582\n",
      "Epoch: 673, Loss: 0.009143218398094177\n",
      "Epoch: 674, Loss: 0.018314843997359276\n",
      "Epoch: 675, Loss: 0.0018855996895581484\n",
      "Epoch: 676, Loss: 0.0012414897792041302\n",
      "Epoch: 677, Loss: 0.0011158641427755356\n",
      "Epoch: 678, Loss: 0.0026584372390061617\n",
      "Epoch: 679, Loss: 0.003318095812574029\n",
      "Epoch: 680, Loss: 0.0018456407124176621\n",
      "Epoch: 681, Loss: 0.009191789664328098\n",
      "Epoch: 682, Loss: 0.03617145121097565\n",
      "Epoch: 683, Loss: 0.00151962845120579\n",
      "Epoch: 684, Loss: 0.00550052197650075\n",
      "Epoch: 685, Loss: 0.0016489345580339432\n",
      "Epoch: 686, Loss: 0.0010953223099932075\n",
      "Epoch: 687, Loss: 0.0016122681554406881\n",
      "Epoch: 688, Loss: 0.0011154781095683575\n",
      "Epoch: 689, Loss: 0.0025400500744581223\n",
      "Epoch: 690, Loss: 0.00580946821719408\n",
      "Epoch: 691, Loss: 0.0027969025541096926\n",
      "Epoch: 692, Loss: 0.006456087809056044\n",
      "Epoch: 693, Loss: 0.0020449762232601643\n",
      "Epoch: 694, Loss: 0.0017007756978273392\n",
      "Epoch: 695, Loss: 0.0008747443207539618\n",
      "Epoch: 696, Loss: 0.0015753774205222726\n",
      "Epoch: 697, Loss: 0.004502282477915287\n",
      "Epoch: 698, Loss: 0.001061469898559153\n",
      "Epoch: 699, Loss: 0.0008904340793378651\n",
      "Epoch: 700, Loss: 0.0013965906109660864\n",
      "Epoch: 701, Loss: 0.0012949110241606832\n",
      "Epoch: 702, Loss: 0.0008117091492749751\n",
      "Epoch: 703, Loss: 0.003292558016255498\n",
      "Epoch: 704, Loss: 0.0014114619698375463\n",
      "Epoch: 705, Loss: 0.0013798490399494767\n",
      "Epoch: 706, Loss: 0.0008523433934897184\n",
      "Epoch: 707, Loss: 0.002986073261126876\n",
      "Epoch: 708, Loss: 0.0010150871239602566\n",
      "Epoch: 709, Loss: 0.001634018961340189\n",
      "Epoch: 710, Loss: 0.0008557919645681977\n",
      "Epoch: 711, Loss: 0.0012664285022765398\n",
      "Epoch: 712, Loss: 0.001132480101659894\n",
      "Epoch: 713, Loss: 0.023533375933766365\n",
      "Epoch: 714, Loss: 0.0008684339700266719\n",
      "Epoch: 715, Loss: 0.0008060504915192723\n",
      "Epoch: 716, Loss: 0.0007267680484801531\n",
      "Epoch: 717, Loss: 0.0009265904664061964\n",
      "Epoch: 718, Loss: 0.0009745963616296649\n",
      "Epoch: 719, Loss: 0.0014823322417214513\n",
      "Epoch: 720, Loss: 0.004079060163348913\n",
      "Epoch: 721, Loss: 0.0008674205746501684\n",
      "Epoch: 722, Loss: 0.001463893917389214\n",
      "Epoch: 723, Loss: 0.0007514720782637596\n",
      "Epoch: 724, Loss: 0.0010876198066398501\n",
      "Epoch: 725, Loss: 0.0018860966665670276\n",
      "Epoch: 726, Loss: 0.004396405536681414\n",
      "Epoch: 727, Loss: 0.0029101227410137653\n",
      "Epoch: 728, Loss: 0.0014606757322326303\n",
      "Epoch: 729, Loss: 0.0010046332608908415\n",
      "Epoch: 730, Loss: 0.0009788500610738993\n",
      "Epoch: 731, Loss: 0.005065300036221743\n",
      "Epoch: 732, Loss: 0.002341056242585182\n",
      "Epoch: 733, Loss: 0.0010792177636176348\n",
      "Epoch: 734, Loss: 0.0013161861570551991\n",
      "Epoch: 735, Loss: 0.0016690718475729227\n",
      "Epoch: 736, Loss: 0.0015100657474249601\n",
      "Epoch: 737, Loss: 0.0010681053390726447\n",
      "Epoch: 738, Loss: 0.0006290774326771498\n",
      "Epoch: 739, Loss: 0.000950184534303844\n",
      "Epoch: 740, Loss: 0.0013355721021071076\n",
      "Epoch: 741, Loss: 0.0008310516714118421\n",
      "Epoch: 742, Loss: 0.003173267003148794\n",
      "Epoch: 743, Loss: 0.0013815423008054495\n",
      "Epoch: 744, Loss: 0.001064602518454194\n",
      "Epoch: 745, Loss: 0.0011964126024395227\n",
      "Epoch: 746, Loss: 0.0006701439851894975\n",
      "Epoch: 747, Loss: 0.0006096765864640474\n",
      "Epoch: 748, Loss: 0.002133990405127406\n",
      "Epoch: 749, Loss: 0.0005259828758426011\n",
      "Epoch: 750, Loss: 0.0007286977488547564\n",
      "Epoch: 751, Loss: 0.001167542184703052\n",
      "Epoch: 752, Loss: 0.001396189327351749\n",
      "Epoch: 753, Loss: 0.0006762336706742644\n",
      "Epoch: 754, Loss: 0.0007572261965833604\n",
      "Epoch: 755, Loss: 0.0025701457634568214\n",
      "Epoch: 756, Loss: 0.00043582552461884916\n",
      "Epoch: 757, Loss: 0.0008463683188892901\n",
      "Epoch: 758, Loss: 0.000604904314968735\n",
      "Epoch: 759, Loss: 0.0011471181642264128\n",
      "Epoch: 760, Loss: 0.0008695368887856603\n",
      "Epoch: 761, Loss: 0.0010950193973258138\n",
      "Epoch: 762, Loss: 0.00044282202725298703\n",
      "Epoch: 763, Loss: 0.0005629348452202976\n",
      "Epoch: 764, Loss: 0.0004317268030717969\n",
      "Epoch: 765, Loss: 0.00232256343588233\n",
      "Epoch: 766, Loss: 0.0004833842976950109\n",
      "Epoch: 767, Loss: 0.0006261030212044716\n",
      "Epoch: 768, Loss: 0.000730592233594507\n",
      "Epoch: 769, Loss: 0.00045729795238003135\n",
      "Epoch: 770, Loss: 0.0005438490188680589\n",
      "Epoch: 771, Loss: 0.0005544867599382997\n",
      "Epoch: 772, Loss: 0.0020297381561249495\n",
      "Epoch: 773, Loss: 0.0005934405489824712\n",
      "Epoch: 774, Loss: 0.0005896227667108178\n",
      "Epoch: 775, Loss: 0.000686550629325211\n",
      "Epoch: 776, Loss: 0.0014628632925450802\n",
      "Epoch: 777, Loss: 0.0005728763644583523\n",
      "Epoch: 778, Loss: 0.0006868211203254759\n",
      "Epoch: 779, Loss: 0.000810799072496593\n",
      "Epoch: 780, Loss: 0.0004453248402569443\n",
      "Epoch: 781, Loss: 0.00047755168634466827\n",
      "Epoch: 782, Loss: 0.000427516846684739\n",
      "Epoch: 783, Loss: 0.0006932020769454539\n",
      "Epoch: 784, Loss: 0.0008673439151607454\n",
      "Epoch: 785, Loss: 0.0007622127304784954\n",
      "Epoch: 786, Loss: 0.0004774036642629653\n",
      "Epoch: 787, Loss: 0.000459029310150072\n",
      "Epoch: 788, Loss: 0.0006214441382326186\n",
      "Epoch: 789, Loss: 0.0011927173472940922\n",
      "Epoch: 790, Loss: 0.0003950749523937702\n",
      "Epoch: 791, Loss: 0.0007769374642521143\n",
      "Epoch: 792, Loss: 0.00040570340934209526\n",
      "Epoch: 793, Loss: 0.00037847444764338434\n",
      "Epoch: 794, Loss: 0.0007362476317211986\n",
      "Epoch: 795, Loss: 0.0003976228181272745\n",
      "Epoch: 796, Loss: 0.0003570708504412323\n",
      "Epoch: 797, Loss: 0.00028931492124684155\n",
      "Epoch: 798, Loss: 0.0003008102357853204\n",
      "Epoch: 799, Loss: 0.0003998812462668866\n",
      "Epoch: 800, Loss: 0.0005863651167601347\n",
      "Epoch: 801, Loss: 0.0008713850402273238\n",
      "Epoch: 802, Loss: 0.0002732639550231397\n",
      "Epoch: 803, Loss: 0.0003691494057420641\n",
      "Epoch: 804, Loss: 0.00041551614413037896\n",
      "Epoch: 805, Loss: 0.0005226226057857275\n",
      "Epoch: 806, Loss: 0.00029438803903758526\n",
      "Epoch: 807, Loss: 0.00025535968597978354\n",
      "Epoch: 808, Loss: 0.000470280327135697\n",
      "Epoch: 809, Loss: 0.00036573302350007\n",
      "Epoch: 810, Loss: 0.0003272742033004761\n",
      "Epoch: 811, Loss: 0.0007551428861916065\n",
      "Epoch: 812, Loss: 0.0011607920750975609\n",
      "Epoch: 813, Loss: 0.000274715363048017\n",
      "Epoch: 814, Loss: 0.00023745106591377407\n",
      "Epoch: 815, Loss: 0.0007130414596758783\n",
      "Epoch: 816, Loss: 0.0005887238075956702\n",
      "Epoch: 817, Loss: 0.0003211454313714057\n",
      "Epoch: 818, Loss: 0.0004377028963062912\n",
      "Epoch: 819, Loss: 0.0017236629500985146\n",
      "Epoch: 820, Loss: 0.0006927882204763591\n",
      "Epoch: 821, Loss: 0.0005206947098486125\n",
      "Epoch: 822, Loss: 0.00040401326259598136\n",
      "Epoch: 823, Loss: 0.00031413850956596434\n",
      "Epoch: 824, Loss: 0.0002721079799812287\n",
      "Epoch: 825, Loss: 0.0002075050288112834\n",
      "Epoch: 826, Loss: 0.0004401491896715015\n",
      "Epoch: 827, Loss: 0.0002990257344208658\n",
      "Epoch: 828, Loss: 0.0002202945906901732\n",
      "Epoch: 829, Loss: 0.0007187146111391485\n",
      "Epoch: 830, Loss: 0.0036680910270661116\n",
      "Epoch: 831, Loss: 0.00036116529372520745\n",
      "Epoch: 832, Loss: 0.00025425682542845607\n",
      "Epoch: 833, Loss: 0.00045292239519767463\n",
      "Epoch: 834, Loss: 0.00040928268572315574\n",
      "Epoch: 835, Loss: 0.0004317088460084051\n",
      "Epoch: 836, Loss: 0.00026491854805499315\n",
      "Epoch: 837, Loss: 0.0012772056506946683\n",
      "Epoch: 838, Loss: 0.01015476044267416\n",
      "Epoch: 839, Loss: 0.0018998924642801285\n",
      "Epoch: 840, Loss: 0.006636716425418854\n",
      "Epoch: 841, Loss: 0.005390835460275412\n",
      "Epoch: 842, Loss: 0.03557758033275604\n",
      "Epoch: 843, Loss: 0.0004929070710204542\n",
      "Epoch: 844, Loss: 0.0006872524973005056\n",
      "Epoch: 845, Loss: 0.0009030391811393201\n",
      "Epoch: 846, Loss: 0.0004045847454108298\n",
      "Epoch: 847, Loss: 0.005266876425594091\n",
      "Epoch: 848, Loss: 0.0009188373223878443\n",
      "Epoch: 849, Loss: 0.00040189875289797783\n",
      "Epoch: 850, Loss: 0.0005583339952863753\n",
      "Epoch: 851, Loss: 0.003585411701351404\n",
      "Epoch: 852, Loss: 0.0009535551071166992\n",
      "Epoch: 853, Loss: 0.0022741344291716814\n",
      "Epoch: 854, Loss: 0.00972876138985157\n",
      "Epoch: 855, Loss: 0.000895302917342633\n",
      "Epoch: 856, Loss: 0.008347420021891594\n",
      "Epoch: 857, Loss: 0.002333219861611724\n",
      "Epoch: 858, Loss: 0.0007629106403328478\n",
      "Epoch: 859, Loss: 0.005251194350421429\n",
      "Epoch: 860, Loss: 0.0017410609871149063\n",
      "Epoch: 861, Loss: 0.0008701997576281428\n",
      "Epoch: 862, Loss: 0.001859139883890748\n",
      "Epoch: 863, Loss: 0.00131272675935179\n",
      "Epoch: 864, Loss: 0.019171005114912987\n",
      "Epoch: 865, Loss: 0.001475027296692133\n",
      "Epoch: 866, Loss: 0.00045598388533107936\n",
      "Epoch: 867, Loss: 0.0008449351880699396\n",
      "Epoch: 868, Loss: 0.001540796598419547\n",
      "Epoch: 869, Loss: 0.0009733128827065229\n",
      "Epoch: 870, Loss: 0.0012732704635709524\n",
      "Epoch: 871, Loss: 0.06441090255975723\n",
      "Epoch: 872, Loss: 0.0024656786117702723\n",
      "Epoch: 873, Loss: 0.010184277780354023\n",
      "Epoch: 874, Loss: 0.0025882709305733442\n",
      "Epoch: 875, Loss: 0.0029850348364561796\n",
      "Epoch: 876, Loss: 0.0018889422062784433\n",
      "Epoch: 877, Loss: 0.0006286547868512571\n",
      "Epoch: 878, Loss: 0.002399640856310725\n",
      "Epoch: 879, Loss: 0.001415841281414032\n",
      "Epoch: 880, Loss: 0.014507309533655643\n",
      "Epoch: 881, Loss: 0.0019721726421266794\n",
      "Epoch: 882, Loss: 0.0010821548057720065\n",
      "Epoch: 883, Loss: 0.009994366206228733\n",
      "Epoch: 884, Loss: 0.024140650406479836\n",
      "Epoch: 885, Loss: 0.001108333352021873\n",
      "Epoch: 886, Loss: 0.0012360124383121729\n",
      "Epoch: 887, Loss: 0.0006350345211103559\n",
      "Epoch: 888, Loss: 0.0010452806018292904\n",
      "Epoch: 889, Loss: 0.0019380548037588596\n",
      "Epoch: 890, Loss: 0.0017094482900574803\n",
      "Epoch: 891, Loss: 0.016794612631201744\n",
      "Epoch: 892, Loss: 0.0014893083134666085\n",
      "Epoch: 893, Loss: 0.005337457172572613\n",
      "Epoch: 894, Loss: 0.003867256920784712\n",
      "Epoch: 895, Loss: 0.0011160570429638028\n",
      "Epoch: 896, Loss: 0.0008807014091871679\n",
      "Epoch: 897, Loss: 0.00449781259521842\n",
      "Epoch: 898, Loss: 0.001048050937242806\n",
      "Epoch: 899, Loss: 0.002164892153814435\n",
      "Epoch: 900, Loss: 0.002081214217469096\n",
      "Epoch: 901, Loss: 0.0010981073137372732\n",
      "Epoch: 902, Loss: 0.0011757146567106247\n",
      "Epoch: 903, Loss: 0.0024335505440831184\n",
      "Epoch: 904, Loss: 0.005123923998326063\n",
      "Epoch: 905, Loss: 0.002332650125026703\n",
      "Epoch: 906, Loss: 0.000597126258071512\n",
      "Epoch: 907, Loss: 0.0006766284932382405\n",
      "Epoch: 908, Loss: 0.0005701769259758294\n",
      "Epoch: 909, Loss: 0.00044747418724000454\n",
      "Epoch: 910, Loss: 0.0005073126521892846\n",
      "Epoch: 911, Loss: 0.0005134083912707865\n",
      "Epoch: 912, Loss: 0.0005128310294821858\n",
      "Epoch: 913, Loss: 0.00048111978685483336\n",
      "Epoch: 914, Loss: 0.005660240072757006\n",
      "Epoch: 915, Loss: 0.0010394242126494646\n",
      "Epoch: 916, Loss: 0.0009339048410765827\n",
      "Epoch: 917, Loss: 0.0003507284272927791\n",
      "Epoch: 918, Loss: 0.0004622319829650223\n",
      "Epoch: 919, Loss: 0.0007647279417142272\n",
      "Epoch: 920, Loss: 0.0008916001534089446\n",
      "Epoch: 921, Loss: 0.0011674539418891072\n",
      "Epoch: 922, Loss: 0.0004438547184690833\n",
      "Epoch: 923, Loss: 0.00044012864236719906\n",
      "Epoch: 924, Loss: 0.0004090624861419201\n",
      "Epoch: 925, Loss: 0.0027970836963504553\n",
      "Epoch: 926, Loss: 0.0012283084215596318\n",
      "Epoch: 927, Loss: 0.0008487628074362874\n",
      "Epoch: 928, Loss: 0.0016654065111652017\n",
      "Epoch: 929, Loss: 0.00048078352119773626\n",
      "Epoch: 930, Loss: 0.014920797199010849\n",
      "Epoch: 931, Loss: 0.0004066719557158649\n",
      "Epoch: 932, Loss: 0.0008060294203460217\n",
      "Epoch: 933, Loss: 0.0006868664640933275\n",
      "Epoch: 934, Loss: 0.00044927699491381645\n",
      "Epoch: 935, Loss: 0.0014953429345041513\n",
      "Epoch: 936, Loss: 0.0007249455084092915\n",
      "Epoch: 937, Loss: 0.00041949909063987434\n",
      "Epoch: 938, Loss: 0.00035477158962748945\n",
      "Epoch: 939, Loss: 0.0004661177808884531\n",
      "Epoch: 940, Loss: 0.0003759549872484058\n",
      "Epoch: 941, Loss: 0.0003630375722423196\n",
      "Epoch: 942, Loss: 0.0016913420986384153\n",
      "Epoch: 943, Loss: 0.00045853579649701715\n",
      "Epoch: 944, Loss: 0.0005492503987625241\n",
      "Epoch: 945, Loss: 0.013129529543220997\n",
      "Epoch: 946, Loss: 0.0006002540467306972\n",
      "Epoch: 947, Loss: 0.0017924975836649537\n",
      "Epoch: 948, Loss: 0.0053378320299088955\n",
      "Epoch: 949, Loss: 0.0007231688359752297\n",
      "Epoch: 950, Loss: 0.0003672924649436027\n",
      "Epoch: 951, Loss: 0.0004328927316237241\n",
      "Epoch: 952, Loss: 0.0004186657606624067\n",
      "Epoch: 953, Loss: 0.00029300240566954017\n",
      "Epoch: 954, Loss: 0.0003438884159550071\n",
      "Epoch: 955, Loss: 0.0006682180101051927\n",
      "Epoch: 956, Loss: 0.00039843193371780217\n",
      "Epoch: 957, Loss: 0.01262765284627676\n",
      "Epoch: 958, Loss: 0.0017177918925881386\n",
      "Epoch: 959, Loss: 0.0005803082603961229\n",
      "Epoch: 960, Loss: 0.0007052915170788765\n",
      "Epoch: 961, Loss: 0.0003217975900042802\n",
      "Epoch: 962, Loss: 0.00044065300608053803\n",
      "Epoch: 963, Loss: 0.0004265920433681458\n",
      "Epoch: 964, Loss: 0.0003301924734842032\n",
      "Epoch: 965, Loss: 0.0006271999445743859\n",
      "Epoch: 966, Loss: 0.0022293305955827236\n",
      "Epoch: 967, Loss: 0.0005144354654476047\n",
      "Epoch: 968, Loss: 0.00033285131212323904\n",
      "Epoch: 969, Loss: 0.0003699967637658119\n",
      "Epoch: 970, Loss: 0.00028736123931594193\n",
      "Epoch: 971, Loss: 0.0009171841666102409\n",
      "Epoch: 972, Loss: 0.0003273820911999792\n",
      "Epoch: 973, Loss: 0.0005787912523373961\n",
      "Epoch: 974, Loss: 0.000947516062296927\n",
      "Epoch: 975, Loss: 0.0003147590032313019\n",
      "Epoch: 976, Loss: 0.00024337982176803052\n",
      "Epoch: 977, Loss: 0.00026368716498836875\n",
      "Epoch: 978, Loss: 0.0003388607583474368\n",
      "Epoch: 979, Loss: 0.00043786989408545196\n",
      "Epoch: 980, Loss: 0.0002416924253338948\n",
      "Epoch: 981, Loss: 0.0003086583747062832\n",
      "Epoch: 982, Loss: 0.00032281861058436334\n",
      "Epoch: 983, Loss: 0.00028170831501483917\n",
      "Epoch: 984, Loss: 0.0004504495591390878\n",
      "Epoch: 985, Loss: 0.00029122750856913626\n",
      "Epoch: 986, Loss: 0.0002119194541592151\n",
      "Epoch: 987, Loss: 0.000570454983972013\n",
      "Epoch: 988, Loss: 0.0005276953452266753\n",
      "Epoch: 989, Loss: 0.00041211541974917054\n",
      "Epoch: 990, Loss: 0.00014294072752818465\n",
      "Epoch: 991, Loss: 0.024248246103525162\n",
      "Epoch: 992, Loss: 0.00025767870829440653\n",
      "Epoch: 993, Loss: 0.00036747075500898063\n",
      "Epoch: 994, Loss: 0.00032715435372665524\n",
      "Epoch: 995, Loss: 0.0002355507022002712\n",
      "Epoch: 996, Loss: 0.00019904939108528197\n",
      "Epoch: 997, Loss: 0.0007772492826916277\n",
      "Epoch: 998, Loss: 0.00027606921503320336\n",
      "Epoch: 999, Loss: 0.012498373165726662\n",
      "Epoch: 1000, Loss: 0.0031192153692245483\n"
     ]
    }
   ],
   "source": [
    "tgt_vocab_size = 5000\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "num_layers = 3\n",
    "d_ff = 128\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "# Initialize the model\n",
    "transformer = Transformer( tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(source_data)\n",
    "    \n",
    "    # Adjust target_data to have the same length as model output\n",
    "    target_data_adjusted = target_data[:, :output.size(1)]\n",
    "    \n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), target_data_adjusted.contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output is tensor([[[-6.0123, -4.6745, -4.3603,  ..., -4.3819, -5.9473, -5.8897]]]), \n",
      "output shape is torch.Size([1, 1, 5000]) \n",
      "predicted_token is 28\n",
      "generated_sequence is [0, 28]\n",
      "-------\n",
      "output is tensor([[[-5.2795, -3.9530, -4.3246,  ..., -4.3539, -5.1141, -4.8554],\n",
      "         [-4.8325, -4.0280, -4.1898,  ..., -4.8621, -4.9755, -4.4617]]]), \n",
      "output shape is torch.Size([1, 2, 5000]) \n",
      "predicted_token is 6\n",
      "generated_sequence is [0, 28, 6]\n",
      "-------\n",
      "output is tensor([[[-3.2425, -3.2849, -2.8890,  ..., -3.4632, -4.0306, -3.6390],\n",
      "         [-4.2873, -3.5796, -3.6699,  ..., -4.0730, -4.6746, -3.6146],\n",
      "         [-3.4823, -3.0382, -4.0119,  ..., -4.2278, -4.2283, -4.1791]]]), \n",
      "output shape is torch.Size([1, 3, 5000]) \n",
      "predicted_token is 8\n",
      "generated_sequence is [0, 28, 6, 8]\n",
      "-------\n",
      "output is tensor([[[-4.0704, -3.7313, -4.4022,  ..., -4.7641, -4.1839, -4.3341],\n",
      "         [-4.5957, -4.8304, -4.8339,  ..., -4.5680, -4.9796, -5.0161],\n",
      "         [-3.9045, -4.2701, -4.7003,  ..., -5.1627, -4.3758, -4.7417],\n",
      "         [-3.0923, -3.3152, -3.8590,  ..., -3.1320, -3.1764, -4.2980]]]), \n",
      "output shape is torch.Size([1, 4, 5000]) \n",
      "predicted_token is 37\n",
      "generated_sequence is [0, 28, 6, 8, 37]\n",
      "-------\n",
      "output is tensor([[[-4.5550, -4.0399, -4.1297,  ..., -5.0659, -4.0458, -4.1788],\n",
      "         [-4.6488, -3.9588, -4.3158,  ..., -3.9094, -4.5118, -4.3486],\n",
      "         [-3.9837, -3.5703, -4.6668,  ..., -3.8986, -4.2980, -4.9026],\n",
      "         [-2.4520, -2.7102, -3.3330,  ..., -2.8382, -2.7218, -3.8629],\n",
      "         [-3.0077, -3.4447, -3.0564,  ..., -4.1512, -2.7337, -2.2453]]]), \n",
      "output shape is torch.Size([1, 5, 5000]) \n",
      "predicted_token is 53\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53]\n",
      "-------\n",
      "output is tensor([[[-4.5871, -4.1884, -4.5389,  ..., -5.0817, -4.4521, -4.9721],\n",
      "         [-5.0572, -4.9827, -5.0253,  ..., -5.2168, -5.0010, -4.4036],\n",
      "         [-4.3267, -4.1862, -4.9560,  ..., -5.2342, -4.9158, -5.2750],\n",
      "         [-3.3598, -3.6155, -3.5473,  ..., -3.3670, -4.1014, -4.1045],\n",
      "         [-4.0304, -4.2425, -3.6956,  ..., -4.7004, -3.9423, -3.3462],\n",
      "         [-3.2393, -3.5220, -1.8707,  ..., -3.7622, -2.5015, -2.3802]]]), \n",
      "output shape is torch.Size([1, 6, 5000]) \n",
      "predicted_token is 86\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86]\n",
      "-------\n",
      "output is tensor([[[-4.0650, -3.6129, -3.3003,  ..., -3.8073, -3.8238, -3.6376],\n",
      "         [-3.9776, -4.0392, -3.8388,  ..., -3.9827, -4.3791, -4.1355],\n",
      "         [-4.3137, -3.3212, -4.3061,  ..., -4.5612, -4.2789, -4.1877],\n",
      "         ...,\n",
      "         [-3.2678, -3.5056, -3.3568,  ..., -3.9248, -2.5920, -2.7466],\n",
      "         [-2.5305, -2.9856, -1.6806,  ..., -2.5518, -1.6938, -1.8329],\n",
      "         [-2.5427, -1.7510, -2.1928,  ..., -4.5579, -3.6112, -3.1014]]]), \n",
      "output shape is torch.Size([1, 7, 5000]) \n",
      "predicted_token is 34\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34]\n",
      "-------\n",
      "output is tensor([[[-3.9882, -3.2875, -3.3406,  ..., -4.6283, -4.1758, -3.9237],\n",
      "         [-4.2932, -4.4263, -4.3758,  ..., -4.7049, -4.8061, -4.4468],\n",
      "         [-3.8592, -4.2615, -4.2298,  ..., -4.8553, -4.8191, -4.6601],\n",
      "         ...,\n",
      "         [-3.9590, -3.4633, -2.4043,  ..., -3.6852, -2.9636, -3.5094],\n",
      "         [-2.2459, -2.5203, -1.9276,  ..., -4.3832, -3.1062, -2.1543],\n",
      "         [-4.9435, -4.6800, -3.1801,  ..., -4.4432, -4.6668, -4.4412]]]), \n",
      "output shape is torch.Size([1, 8, 5000]) \n",
      "predicted_token is 31\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31]\n",
      "-------\n",
      "output is tensor([[[-5.4192, -4.4920, -4.5401,  ..., -5.2320, -5.4499, -4.9530],\n",
      "         [-4.9023, -5.0420, -4.2122,  ..., -4.4787, -4.3708, -4.2837],\n",
      "         [-3.7002, -3.3054, -4.5627,  ..., -4.5538, -4.0118, -4.2113],\n",
      "         ...,\n",
      "         [-2.8983, -2.6317, -2.4385,  ..., -4.8004, -3.9890, -2.7488],\n",
      "         [-5.8928, -6.1968, -4.2662,  ..., -5.4270, -5.7066, -5.3315],\n",
      "         [-3.1855, -3.2150, -3.6760,  ..., -4.4763, -3.7726, -3.4362]]]), \n",
      "output shape is torch.Size([1, 9, 5000]) \n",
      "predicted_token is 119\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119]\n",
      "-------\n",
      "output is tensor([[[-3.8873, -3.4099, -3.4138,  ..., -4.1291, -3.9020, -3.7582],\n",
      "         [-4.1425, -3.9017, -3.4541,  ..., -3.6998, -4.2993, -3.2322],\n",
      "         [-4.7106, -5.3416, -4.8324,  ..., -5.3448, -5.0869, -4.8122],\n",
      "         ...,\n",
      "         [-4.7695, -4.7212, -3.2407,  ..., -4.1408, -3.9463, -4.2432],\n",
      "         [-2.9730, -3.0086, -3.8876,  ..., -4.1678, -3.0701, -3.3148],\n",
      "         [-2.4984, -2.7925, -2.6747,  ..., -3.5913, -2.1823, -2.7231]]]), \n",
      "output shape is torch.Size([1, 10, 5000]) \n",
      "predicted_token is 8\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8]\n",
      "-------\n",
      "output is tensor([[[-4.3798, -3.7589, -3.8791,  ..., -4.4011, -4.6511, -3.8997],\n",
      "         [-5.3490, -5.3026, -5.3773,  ..., -5.2716, -5.5775, -5.7129],\n",
      "         [-4.7428, -4.1153, -4.9988,  ..., -4.7263, -4.5711, -5.1081],\n",
      "         ...,\n",
      "         [-2.4589, -2.2800, -3.1230,  ..., -3.9350, -2.7349, -2.6059],\n",
      "         [-4.1995, -4.5231, -3.6176,  ..., -4.7844, -3.4506, -3.7475],\n",
      "         [-4.4416, -4.8818, -3.7513,  ..., -3.7675, -4.7543, -3.9326]]]), \n",
      "output shape is torch.Size([1, 11, 5000]) \n",
      "predicted_token is 1614\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614]\n",
      "-------\n",
      "output is tensor([[[-4.8652, -4.4173, -4.4245,  ..., -5.1977, -5.5184, -4.5982],\n",
      "         [-4.5973, -4.4828, -4.3954,  ..., -4.6898, -4.6237, -4.3570],\n",
      "         [-4.6433, -4.1942, -5.0404,  ..., -5.0955, -4.7714, -5.2123],\n",
      "         ...,\n",
      "         [-4.5835, -4.0830, -4.2708,  ..., -4.9607, -3.5830, -3.3515],\n",
      "         [-3.9759, -4.9974, -3.9622,  ..., -4.3341, -4.4863, -4.3216],\n",
      "         [-3.8203, -3.3432, -3.1964,  ..., -3.3569, -3.3995, -3.1401]]]), \n",
      "output shape is torch.Size([1, 12, 5000]) \n",
      "predicted_token is 21\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21]\n",
      "-------\n",
      "output is tensor([[[-4.4888, -3.8082, -3.6510,  ..., -4.2166, -4.5211, -4.1555],\n",
      "         [-4.9785, -4.5022, -4.3704,  ..., -5.2039, -5.1077, -4.9368],\n",
      "         [-3.5087, -3.9148, -4.1402,  ..., -3.8756, -3.9260, -4.1158],\n",
      "         ...,\n",
      "         [-3.0384, -4.1770, -2.7564,  ..., -2.8607, -3.0335, -3.1957],\n",
      "         [-3.4980, -2.6078, -3.2097,  ..., -2.6092, -2.7642, -3.4250],\n",
      "         [-5.1389, -5.3076, -4.2545,  ..., -4.2412, -3.9962, -4.2866]]]), \n",
      "output shape is torch.Size([1, 13, 5000]) \n",
      "predicted_token is 14\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14]\n",
      "-------\n",
      "output is tensor([[[-5.0765, -4.3889, -4.5659,  ..., -5.0166, -5.4675, -4.5115],\n",
      "         [-4.8126, -4.1322, -4.2358,  ..., -4.1669, -4.5731, -4.3847],\n",
      "         [-3.8671, -3.6938, -4.3187,  ..., -4.1896, -4.1990, -4.4509],\n",
      "         ...,\n",
      "         [-3.2183, -2.2352, -2.9102,  ..., -2.2637, -2.6483, -2.9596],\n",
      "         [-5.2972, -4.2148, -4.1889,  ..., -4.1894, -4.1311, -4.7369],\n",
      "         [-3.1348, -3.0082, -3.4980,  ..., -2.9357, -2.7930, -2.0676]]]), \n",
      "output shape is torch.Size([1, 14, 5000]) \n",
      "predicted_token is 198\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198]\n",
      "-------\n",
      "output is tensor([[[-3.6490, -3.2630, -3.3328,  ..., -3.8391, -4.3187, -2.7701],\n",
      "         [-4.5591, -4.6418, -4.1824,  ..., -5.1327, -4.7382, -4.6643],\n",
      "         [-4.6673, -3.8830, -4.5533,  ..., -4.7806, -4.6340, -5.0281],\n",
      "         ...,\n",
      "         [-4.5875, -4.2621, -4.2601,  ..., -3.7821, -3.4037, -3.6518],\n",
      "         [-2.7111, -2.5239, -3.1162,  ..., -3.5421, -2.5460, -2.2276],\n",
      "         [-3.8574, -4.1670, -3.4005,  ..., -3.7346, -4.6524, -4.3940]]]), \n",
      "output shape is torch.Size([1, 15, 5000]) \n",
      "predicted_token is 3\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3]\n",
      "-------\n",
      "output is tensor([[[-4.2254, -3.7840, -3.8465,  ..., -4.2767, -4.4828, -3.7290],\n",
      "         [-4.0386, -4.2846, -3.7007,  ..., -4.7530, -4.8944, -4.1767],\n",
      "         [-3.4694, -3.4110, -4.1010,  ..., -3.7512, -3.6817, -4.3231],\n",
      "         ...,\n",
      "         [-1.6278, -1.6179, -2.7730,  ..., -2.5152, -1.7300, -1.2455],\n",
      "         [-3.4936, -3.5132, -3.1825,  ..., -3.3430, -4.0640, -3.3099],\n",
      "         [-3.6071, -3.6239, -2.4275,  ..., -2.3403, -2.6028, -2.9401]]]), \n",
      "output shape is torch.Size([1, 16, 5000]) \n",
      "predicted_token is 11\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11]\n",
      "-------\n",
      "output is tensor([[[-4.2359, -4.1860, -4.0093,  ..., -4.1170, -4.6828, -4.0599],\n",
      "         [-4.5687, -4.1909, -4.2016,  ..., -4.5815, -4.3951, -4.5323],\n",
      "         [-4.1460, -3.2206, -4.0037,  ..., -3.9108, -4.5072, -4.4828],\n",
      "         ...,\n",
      "         [-4.1955, -4.4271, -3.5074,  ..., -4.4191, -4.7071, -4.4216],\n",
      "         [-4.0304, -3.9849, -2.2444,  ..., -2.3509, -2.7812, -3.2791],\n",
      "         [-2.8818, -2.3266, -1.2382,  ..., -3.1933, -4.1639, -2.3598]]]), \n",
      "output shape is torch.Size([1, 17, 5000]) \n",
      "predicted_token is 185\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185]\n",
      "-------\n",
      "output is tensor([[[-4.5761, -4.0455, -3.5227,  ..., -4.0561, -4.2689, -3.8499],\n",
      "         [-4.5831, -4.7784, -3.8936,  ..., -4.6634, -4.8982, -4.7557],\n",
      "         [-3.1273, -3.3431, -3.7757,  ..., -4.1458, -4.3480, -4.2346],\n",
      "         ...,\n",
      "         [-4.2451, -4.2509, -2.5037,  ..., -2.7656, -2.9267, -3.1679],\n",
      "         [-2.4703, -2.0821, -1.1777,  ..., -2.4540, -3.4037, -2.3150],\n",
      "         [-4.4323, -4.6503, -3.3878,  ..., -4.6450, -4.6755, -4.1776]]]), \n",
      "output shape is torch.Size([1, 18, 5000]) \n",
      "predicted_token is 12\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12]\n",
      "-------\n",
      "output is tensor([[[-4.6214, -4.3130, -4.2890,  ..., -4.8544, -5.3953, -4.4798],\n",
      "         [-4.7540, -4.6931, -4.5443,  ..., -5.1791, -4.5437, -4.3025],\n",
      "         [-5.0825, -4.9378, -5.1128,  ..., -5.6675, -5.5564, -5.6065],\n",
      "         ...,\n",
      "         [-2.8421, -2.8177, -1.4639,  ..., -2.8894, -3.9009, -2.1230],\n",
      "         [-4.5963, -4.6703, -3.5183,  ..., -4.8855, -4.5272, -4.1347],\n",
      "         [-4.2569, -4.7203, -3.3346,  ..., -2.8856, -3.8803, -4.1506]]]), \n",
      "output shape is torch.Size([1, 19, 5000]) \n",
      "predicted_token is 9\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9]\n",
      "-------\n",
      "output is tensor([[[-3.7242, -3.5539, -2.8399,  ..., -3.2676, -3.5908, -3.5063],\n",
      "         [-4.8542, -5.0681, -4.6068,  ..., -4.9464, -4.6378, -4.3757],\n",
      "         [-4.6043, -3.8426, -4.8587,  ..., -5.1563, -5.5194, -4.8055],\n",
      "         ...,\n",
      "         [-5.1997, -4.9805, -4.1295,  ..., -5.0023, -5.0171, -5.2493],\n",
      "         [-4.5124, -4.7043, -3.4699,  ..., -3.2198, -4.2335, -4.7127],\n",
      "         [-3.8187, -4.3139, -4.0046,  ..., -4.4827, -3.4423, -5.3192]]]), \n",
      "output shape is torch.Size([1, 20, 5000]) \n",
      "predicted_token is 1455\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455]\n",
      "-------\n",
      "output is tensor([[[-4.2237, -3.7479, -3.8148,  ..., -4.3870, -4.6963, -3.9419],\n",
      "         [-4.6444, -4.2408, -4.3993,  ..., -4.2448, -4.7895, -3.8822],\n",
      "         [-3.4777, -2.5507, -3.6307,  ..., -3.3913, -3.3429, -3.7171],\n",
      "         ...,\n",
      "         [-3.5077, -4.3330, -2.9847,  ..., -2.7451, -3.4203, -4.4800],\n",
      "         [-4.2764, -4.2322, -3.3991,  ..., -4.4133, -3.4995, -5.3517],\n",
      "         [-2.9669, -2.3312, -2.2239,  ..., -3.6467, -3.8188, -3.7071]]]), \n",
      "output shape is torch.Size([1, 21, 5000]) \n",
      "predicted_token is 7\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7]\n",
      "-------\n",
      "output is tensor([[[-5.0980, -4.5657, -4.3129,  ..., -4.6805, -4.9243, -4.3954],\n",
      "         [-3.7223, -3.1628, -3.5424,  ..., -4.1438, -3.6938, -3.4264],\n",
      "         [-3.8879, -3.3235, -3.8383,  ..., -4.6245, -4.4828, -4.5609],\n",
      "         ...,\n",
      "         [-2.9945, -3.0027, -2.7606,  ..., -3.3974, -2.2338, -4.7394],\n",
      "         [-3.5243, -3.5618, -2.9493,  ..., -4.5632, -4.5421, -4.0477],\n",
      "         [-3.3562, -3.7611, -3.0065,  ..., -3.1410, -2.7114, -2.5480]]]), \n",
      "output shape is torch.Size([1, 22, 5000]) \n",
      "predicted_token is 54\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54]\n",
      "-------\n",
      "output is tensor([[[-5.1378, -4.8293, -4.5651,  ..., -4.9226, -5.2040, -4.5808],\n",
      "         [-5.2701, -4.5839, -4.9045,  ..., -4.7703, -5.1336, -4.3285],\n",
      "         [-4.0261, -4.2642, -4.1651,  ..., -4.5621, -4.7012, -4.2416],\n",
      "         ...,\n",
      "         [-3.4478, -3.3350, -2.2825,  ..., -4.5810, -4.4878, -4.1441],\n",
      "         [-2.9893, -2.9245, -2.9835,  ..., -3.4485, -2.4532, -2.3512],\n",
      "         [-4.7238, -4.4459, -4.0592,  ..., -2.9403, -4.0477, -4.3990]]]), \n",
      "output shape is torch.Size([1, 23, 5000]) \n",
      "predicted_token is 24\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24]\n",
      "-------\n",
      "output is tensor([[[-5.6061, -4.9937, -5.1829,  ..., -5.0675, -5.3666, -5.5415],\n",
      "         [-4.7436, -4.9214, -5.1549,  ..., -4.5988, -4.8299, -4.9015],\n",
      "         [-3.4006, -3.3340, -4.0022,  ..., -3.5905, -3.4700, -3.6687],\n",
      "         ...,\n",
      "         [-3.2173, -3.4402, -2.9814,  ..., -3.7559, -3.1313, -2.7838],\n",
      "         [-3.7853, -3.1963, -3.1372,  ..., -2.1873, -3.1151, -3.9397],\n",
      "         [-5.1451, -4.2283, -3.8778,  ..., -4.2330, -4.2480, -4.2924]]]), \n",
      "output shape is torch.Size([1, 24, 5000]) \n",
      "predicted_token is 12\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12]\n",
      "-------\n",
      "output is tensor([[[-5.6162, -5.3601, -4.5415,  ..., -5.0058, -5.7522, -5.1140],\n",
      "         [-4.5658, -4.5872, -4.4472,  ..., -4.6783, -4.6869, -4.9609],\n",
      "         [-3.5467, -3.6462, -4.6179,  ..., -4.6913, -4.1098, -4.4278],\n",
      "         ...,\n",
      "         [-3.9130, -3.3639, -3.4665,  ..., -2.5671, -3.3499, -4.0667],\n",
      "         [-5.2003, -4.4294, -3.3983,  ..., -4.4991, -4.2900, -3.8678],\n",
      "         [-5.4073, -4.4588, -4.4666,  ..., -3.7103, -4.9211, -3.9758]]]), \n",
      "output shape is torch.Size([1, 25, 5000]) \n",
      "predicted_token is 19\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19]\n",
      "-------\n",
      "output is tensor([[[-4.6036, -4.5933, -4.2783,  ..., -4.3996, -5.0307, -4.6790],\n",
      "         [-4.7416, -4.6555, -4.8447,  ..., -5.2840, -5.3882, -4.2273],\n",
      "         [-4.0546, -4.1126, -4.7892,  ..., -4.7167, -4.7538, -4.3012],\n",
      "         ...,\n",
      "         [-4.9878, -4.0135, -3.5666,  ..., -3.9325, -4.0535, -4.0262],\n",
      "         [-4.5752, -3.6029, -3.8409,  ..., -3.1814, -4.0425, -2.9699],\n",
      "         [-4.1070, -3.1886, -4.1604,  ..., -3.4753, -4.2086, -3.8567]]]), \n",
      "output shape is torch.Size([1, 26, 5000]) \n",
      "predicted_token is 230\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230]\n",
      "-------\n",
      "output is tensor([[[-4.2541, -4.4188, -4.0932,  ..., -4.4269, -5.0367, -4.3329],\n",
      "         [-4.5288, -4.4570, -4.5989,  ..., -4.4637, -5.0111, -4.2240],\n",
      "         [-3.9155, -4.3560, -4.3678,  ..., -4.1389, -4.0340, -4.7794],\n",
      "         ...,\n",
      "         [-4.8665, -3.5840, -3.5650,  ..., -2.9390, -3.4453, -3.2398],\n",
      "         [-4.6621, -3.7997, -4.2356,  ..., -3.9399, -4.4803, -4.0937],\n",
      "         [-4.9263, -4.5930, -3.3922,  ..., -4.9857, -4.5357, -4.6771]]]), \n",
      "output shape is torch.Size([1, 27, 5000]) \n",
      "predicted_token is 12\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12]\n",
      "-------\n",
      "output is tensor([[[-4.3515, -4.0783, -3.5879,  ..., -3.9890, -4.5050, -4.1042],\n",
      "         [-3.9590, -3.4504, -3.6719,  ..., -4.2454, -4.1455, -4.2274],\n",
      "         [-3.7273, -3.1944, -4.0429,  ..., -4.5490, -4.0343, -4.5057],\n",
      "         ...,\n",
      "         [-3.8422, -2.6653, -3.5192,  ..., -3.6162, -3.9092, -3.0365],\n",
      "         [-4.4713, -3.9705, -2.8928,  ..., -4.5366, -4.0409, -3.8034],\n",
      "         [-3.8959, -3.9232, -3.6357,  ..., -3.7118, -4.3207, -4.6417]]]), \n",
      "output shape is torch.Size([1, 28, 5000]) \n",
      "predicted_token is 9\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9]\n",
      "-------\n",
      "output is tensor([[[-4.5412, -4.2630, -3.4780,  ..., -4.3939, -4.8018, -4.5857],\n",
      "         [-5.7144, -5.0141, -4.9540,  ..., -4.7495, -5.3581, -5.5714],\n",
      "         [-4.4257, -3.6364, -4.8449,  ..., -4.5089, -5.3476, -4.7006],\n",
      "         ...,\n",
      "         [-4.8162, -5.1085, -3.5926,  ..., -5.0689, -4.6401, -4.3612],\n",
      "         [-4.3916, -4.5264, -4.0871,  ..., -4.3238, -4.8140, -5.0413],\n",
      "         [-2.4081, -2.6330, -2.3578,  ..., -2.9161, -2.2525, -3.9743]]]), \n",
      "output shape is torch.Size([1, 29, 5000]) \n",
      "predicted_token is 1316\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316]\n",
      "-------\n",
      "output is tensor([[[-4.9867, -4.8774, -4.6533,  ..., -4.7432, -5.6680, -4.9421],\n",
      "         [-5.1530, -5.0426, -4.7449,  ..., -4.7300, -5.2731, -5.1162],\n",
      "         [-3.8456, -3.7429, -4.4714,  ..., -4.5897, -4.3476, -4.7610],\n",
      "         ...,\n",
      "         [-3.9607, -3.4658, -3.2503,  ..., -3.0454, -4.3146, -4.1420],\n",
      "         [-3.5416, -3.3634, -3.0351,  ..., -3.0326, -2.8935, -5.6322],\n",
      "         [-3.2577, -4.1663, -3.3871,  ..., -3.3321, -3.6984, -4.0978]]]), \n",
      "output shape is torch.Size([1, 30, 5000]) \n",
      "predicted_token is 3\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3]\n",
      "-------\n",
      "output is tensor([[[-5.1619, -4.6139, -4.2923,  ..., -4.2826, -5.4516, -4.3122],\n",
      "         [-5.0110, -4.8282, -4.9484,  ..., -5.6413, -5.0997, -5.2250],\n",
      "         [-4.8601, -4.7439, -4.8467,  ..., -4.7960, -4.3777, -4.5748],\n",
      "         ...,\n",
      "         [-2.5747, -2.6192, -2.6494,  ..., -3.2949, -2.9587, -4.5899],\n",
      "         [-3.4363, -4.6241, -3.9015,  ..., -3.2419, -3.7125, -4.4457],\n",
      "         [-5.0295, -4.9394, -3.8836,  ..., -3.8985, -4.6390, -5.1440]]]), \n",
      "output shape is torch.Size([1, 31, 5000]) \n",
      "predicted_token is 31\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31]\n",
      "-------\n",
      "output is tensor([[[-5.2550, -4.4748, -4.0524,  ..., -4.5723, -4.9482, -4.8260],\n",
      "         [-4.0986, -4.3993, -3.5873,  ..., -4.8557, -4.5324, -3.8020],\n",
      "         [-3.8835, -3.5682, -4.0354,  ..., -4.4215, -4.7474, -4.6429],\n",
      "         ...,\n",
      "         [-3.8556, -4.7382, -3.7696,  ..., -3.4738, -4.3691, -4.2597],\n",
      "         [-5.1624, -5.2029, -3.7003,  ..., -4.0688, -4.7278, -4.8735],\n",
      "         [-4.3726, -3.1201, -3.8692,  ..., -3.3369, -4.2085, -5.2802]]]), \n",
      "output shape is torch.Size([1, 32, 5000]) \n",
      "predicted_token is 59\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59]\n",
      "-------\n",
      "output is tensor([[[-4.3611, -4.0238, -4.0420,  ..., -4.1780, -4.5458, -4.4598],\n",
      "         [-4.2218, -3.3949, -3.9386,  ..., -4.0297, -4.2362, -3.8827],\n",
      "         [-4.0391, -3.6783, -4.3837,  ..., -4.2022, -4.5139, -4.2578],\n",
      "         ...,\n",
      "         [-4.2791, -4.5444, -3.2453,  ..., -3.5749, -3.7211, -4.0099],\n",
      "         [-3.3183, -2.2626, -3.2787,  ..., -2.3482, -2.9346, -4.4379],\n",
      "         [-3.5000, -2.8169, -3.0641,  ..., -4.2587, -4.2823, -3.8616]]]), \n",
      "output shape is torch.Size([1, 33, 5000]) \n",
      "predicted_token is 7\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7]\n",
      "-------\n",
      "output is tensor([[[-4.9973, -4.9150, -4.6293,  ..., -5.0828, -5.4921, -4.9960],\n",
      "         [-4.3860, -4.5390, -4.4081,  ..., -3.9278, -4.3234, -4.5521],\n",
      "         [-3.5702, -3.1314, -4.2790,  ..., -4.0198, -3.9817, -4.3544],\n",
      "         ...,\n",
      "         [-4.2077, -3.1014, -3.4901,  ..., -2.9103, -3.8763, -5.1652],\n",
      "         [-4.1130, -3.9731, -3.6611,  ..., -4.9463, -4.8247, -4.7914],\n",
      "         [-3.4759, -3.8491, -3.1386,  ..., -3.6319, -3.6114, -3.7916]]]), \n",
      "output shape is torch.Size([1, 34, 5000]) \n",
      "predicted_token is 259\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259]\n",
      "-------\n",
      "output is tensor([[[-4.9283, -4.1682, -3.7765,  ..., -4.1690, -4.5193, -3.8127],\n",
      "         [-4.8170, -4.5340, -4.3512,  ..., -4.5118, -4.4284, -4.5868],\n",
      "         [-4.6339, -3.9866, -4.6895,  ..., -4.8525, -5.4948, -5.1405],\n",
      "         ...,\n",
      "         [-2.8356, -1.8052, -2.3591,  ..., -3.6083, -4.3246, -4.2427],\n",
      "         [-3.2263, -3.8729, -2.6636,  ..., -3.6645, -2.9985, -3.5424],\n",
      "         [-4.6365, -4.6161, -4.0540,  ..., -4.4793, -5.2148, -4.6907]]]), \n",
      "output shape is torch.Size([1, 35, 5000]) \n",
      "predicted_token is 4\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4]\n",
      "-------\n",
      "output is tensor([[[-4.9373, -4.3790, -3.8682,  ..., -4.0078, -4.8994, -4.8203],\n",
      "         [-5.0128, -4.0860, -4.3278,  ..., -5.0052, -5.3756, -4.8143],\n",
      "         [-4.2621, -4.3729, -4.9973,  ..., -4.5474, -4.4444, -4.6139],\n",
      "         ...,\n",
      "         [-4.0790, -4.4114, -3.6694,  ..., -4.5469, -4.5878, -4.9432],\n",
      "         [-4.3976, -3.6962, -3.9909,  ..., -5.0341, -5.4346, -4.2834],\n",
      "         [-4.1241, -4.3817, -3.6833,  ..., -3.4426, -4.3210, -3.9322]]]), \n",
      "output shape is torch.Size([1, 36, 5000]) \n",
      "predicted_token is 1614\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4, 1614]\n",
      "-------\n",
      "output is tensor([[[-4.7210, -4.2412, -3.7977,  ..., -4.2166, -4.8360, -4.0107],\n",
      "         [-5.3918, -5.4005, -5.2002,  ..., -5.5616, -5.5869, -5.1591],\n",
      "         [-3.4037, -2.8934, -3.5980,  ..., -3.5172, -3.0995, -3.8857],\n",
      "         ...,\n",
      "         [-4.3888, -3.7056, -3.4086,  ..., -4.3488, -4.5066, -3.6365],\n",
      "         [-3.8154, -3.7281, -3.1162,  ..., -2.5790, -3.3152, -3.4533],\n",
      "         [-3.7524, -3.5976, -3.8573,  ..., -2.0018, -3.3637, -4.4519]]]), \n",
      "output shape is torch.Size([1, 37, 5000]) \n",
      "predicted_token is 24\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4, 1614, 24]\n",
      "-------\n",
      "output is tensor([[[-4.1939, -3.8719, -3.1527,  ..., -3.6087, -4.4440, -3.5676],\n",
      "         [-3.8254, -4.0807, -3.5282,  ..., -4.1930, -4.3387, -3.9023],\n",
      "         [-3.7445, -3.9718, -4.4875,  ..., -4.7439, -5.0962, -4.8568],\n",
      "         ...,\n",
      "         [-4.7025, -4.2506, -3.9079,  ..., -3.4766, -3.9695, -4.1337],\n",
      "         [-3.4390, -2.8308, -3.3727,  ..., -1.8915, -2.9504, -3.9945],\n",
      "         [-4.2970, -4.1935, -3.6753,  ..., -3.7141, -3.3670, -3.8584]]]), \n",
      "output shape is torch.Size([1, 38, 5000]) \n",
      "predicted_token is 14\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4, 1614, 24, 14]\n",
      "-------\n",
      "output is tensor([[[-5.0394, -4.4060, -4.6520,  ..., -4.4059, -5.6543, -4.5715],\n",
      "         [-5.0649, -4.6010, -4.6107,  ..., -5.4761, -5.0366, -4.1335],\n",
      "         [-4.3114, -4.1409, -4.3429,  ..., -4.5861, -3.9986, -4.7863],\n",
      "         ...,\n",
      "         [-3.2387, -2.8732, -3.4258,  ..., -1.4077, -2.4514, -4.1855],\n",
      "         [-4.0194, -3.3544, -3.3294,  ..., -2.4743, -2.5197, -3.3355],\n",
      "         [-3.0720, -3.5458, -3.2722,  ..., -3.4619, -3.5515, -3.1383]]]), \n",
      "output shape is torch.Size([1, 39, 5000]) \n",
      "predicted_token is 43\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4, 1614, 24, 14, 43]\n",
      "-------\n",
      "output is tensor([[[-4.3932, -3.9778, -3.7053,  ..., -3.8728, -4.1893, -3.7298],\n",
      "         [-3.9693, -3.7383, -3.5272,  ..., -4.2293, -4.2857, -3.7914],\n",
      "         [-3.9979, -4.1377, -4.7129,  ..., -4.3439, -4.2549, -4.3649],\n",
      "         ...,\n",
      "         [-4.7027, -4.0363, -4.0953,  ..., -3.2894, -3.0508, -4.1579],\n",
      "         [-2.6697, -3.0691, -3.0457,  ..., -2.4313, -3.0924, -2.3352],\n",
      "         [-5.3127, -4.8746, -4.7344,  ..., -4.6050, -4.8556, -5.5244]]]), \n",
      "output shape is torch.Size([1, 40, 5000]) \n",
      "predicted_token is 6\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4, 1614, 24, 14, 43, 6]\n",
      "-------\n",
      "output is tensor([[[-3.8190, -3.8543, -3.1960,  ..., -3.3206, -4.0309, -3.3193],\n",
      "         [-5.0592, -4.3700, -4.6338,  ..., -4.9681, -5.0698, -4.7575],\n",
      "         [-3.8959, -3.4141, -4.0346,  ..., -3.7682, -3.2960, -4.2136],\n",
      "         ...,\n",
      "         [-1.8668, -2.8147, -2.8096,  ..., -2.3583, -2.9403, -1.9572],\n",
      "         [-5.6623, -5.0783, -5.2693,  ..., -4.4701, -5.0436, -6.1858],\n",
      "         [-3.6138, -3.1160, -3.7748,  ..., -3.6155, -4.0883, -3.1395]]]), \n",
      "output shape is torch.Size([1, 41, 5000]) \n",
      "predicted_token is 23\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4, 1614, 24, 14, 43, 6, 23]\n",
      "-------\n",
      "output is tensor([[[-4.3924, -4.0147, -3.5705,  ..., -3.9600, -4.6974, -3.9040],\n",
      "         [-3.9185, -2.9796, -3.5871,  ..., -3.6644, -3.7148, -4.2333],\n",
      "         [-3.9470, -3.6998, -4.6640,  ..., -4.0853, -4.4386, -4.5596],\n",
      "         ...,\n",
      "         [-4.1917, -4.2601, -3.8833,  ..., -3.7510, -4.2948, -5.3824],\n",
      "         [-4.3360, -3.5704, -4.2101,  ..., -4.8318, -5.0882, -4.4585],\n",
      "         [-3.9758, -3.6684, -3.3618,  ..., -2.8413, -2.9954, -2.9586]]]), \n",
      "output shape is torch.Size([1, 42, 5000]) \n",
      "predicted_token is 11\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4, 1614, 24, 14, 43, 6, 23, 11]\n",
      "-------\n",
      "output is tensor([[[-4.3776, -3.8708, -3.3426,  ..., -4.0791, -4.7697, -3.9819],\n",
      "         [-4.7775, -4.5431, -4.6502,  ..., -4.4892, -5.1284, -4.7375],\n",
      "         [-3.6064, -2.1765, -3.9521,  ..., -3.3838, -3.7649, -3.7676],\n",
      "         ...,\n",
      "         [-4.3086, -3.8765, -3.7939,  ..., -4.6831, -4.2092, -3.7046],\n",
      "         [-3.8685, -3.2772, -2.7847,  ..., -2.5618, -2.2120, -2.5319],\n",
      "         [-3.8560, -4.1248, -2.5559,  ..., -3.8293, -4.7747, -3.4014]]]), \n",
      "output shape is torch.Size([1, 43, 5000]) \n",
      "predicted_token is 94\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4, 1614, 24, 14, 43, 6, 23, 11, 94]\n",
      "-------\n",
      "output is tensor([[[-4.5757, -3.9737, -3.2683,  ..., -3.5876, -4.2909, -3.8866],\n",
      "         [-4.1178, -3.9438, -4.0029,  ..., -4.6085, -4.7982, -5.0471],\n",
      "         [-3.2392, -3.2490, -3.7242,  ..., -3.3576, -3.1559, -3.8397],\n",
      "         ...,\n",
      "         [-3.4790, -3.2958, -2.4612,  ..., -2.4905, -2.1008, -2.2649],\n",
      "         [-2.4535, -2.5233, -1.1408,  ..., -2.6320, -2.9666, -1.6860],\n",
      "         [-5.2991, -4.7515, -3.5411,  ..., -4.0435, -4.6693, -4.8100]]]), \n",
      "output shape is torch.Size([1, 44, 5000]) \n",
      "predicted_token is 2599\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4, 1614, 24, 14, 43, 6, 23, 11, 94, 2599]\n",
      "-------\n",
      "output is tensor([[[-3.3018, -3.5372, -2.5120,  ..., -3.1892, -3.4896, -3.2522],\n",
      "         [-4.5856, -4.7103, -4.2681,  ..., -6.4075, -4.9206, -3.6315],\n",
      "         [-4.0475, -4.0103, -4.8293,  ..., -5.2554, -4.6022, -4.3726],\n",
      "         ...,\n",
      "         [-2.8991, -2.6030, -1.0991,  ..., -2.8058, -3.3872, -2.1980],\n",
      "         [-4.5117, -4.3196, -3.4101,  ..., -4.0615, -3.8218, -4.4142],\n",
      "         [-3.5151, -4.2100, -3.1537,  ..., -2.4748, -2.5784, -2.5653]]]), \n",
      "output shape is torch.Size([1, 45, 5000]) \n",
      "predicted_token is 8\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4, 1614, 24, 14, 43, 6, 23, 11, 94, 2599, 8]\n",
      "-------\n",
      "output is tensor([[[-4.0006, -3.1577, -2.8684,  ..., -3.3007, -3.6935, -3.1136],\n",
      "         [-4.8424, -4.3248, -4.5190,  ..., -4.5036, -4.7030, -4.5559],\n",
      "         [-4.6717, -4.2634, -5.0056,  ..., -5.2805, -4.4277, -5.1014],\n",
      "         ...,\n",
      "         [-5.8239, -4.7877, -4.1372,  ..., -4.7775, -4.4566, -5.2332],\n",
      "         [-4.7189, -4.3626, -2.9350,  ..., -3.0156, -3.2302, -3.6380],\n",
      "         [-4.1012, -4.1281, -3.1611,  ..., -3.6769, -4.6638, -3.9220]]]), \n",
      "output shape is torch.Size([1, 46, 5000]) \n",
      "predicted_token is 1293\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4, 1614, 24, 14, 43, 6, 23, 11, 94, 2599, 8, 1293]\n",
      "-------\n",
      "output is tensor([[[-4.2045, -4.0026, -3.2736,  ..., -4.0736, -4.8115, -3.9424],\n",
      "         [-3.9506, -3.9314, -3.8288,  ..., -3.6489, -4.2930, -4.0638],\n",
      "         [-3.6172, -3.1041, -4.0267,  ..., -3.9891, -3.9348, -3.9399],\n",
      "         ...,\n",
      "         [-4.0178, -3.7667, -3.6512,  ..., -3.3116, -2.4068, -3.3297],\n",
      "         [-3.9656, -3.4080, -2.4661,  ..., -2.5919, -3.9234, -3.3342],\n",
      "         [-5.2593, -3.1912, -3.1148,  ..., -2.8347, -4.0008, -3.9984]]]), \n",
      "output shape is torch.Size([1, 47, 5000]) \n",
      "predicted_token is 32\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4, 1614, 24, 14, 43, 6, 23, 11, 94, 2599, 8, 1293, 32]\n",
      "-------\n",
      "output is tensor([[[-4.7552, -4.6213, -3.9725,  ..., -3.9949, -4.3300, -4.0859],\n",
      "         [-3.7675, -3.6839, -3.8581,  ..., -4.7378, -4.3180, -3.6588],\n",
      "         [-4.6274, -5.0394, -5.4515,  ..., -6.2318, -5.8809, -5.4547],\n",
      "         ...,\n",
      "         [-4.1670, -3.7475, -3.4337,  ..., -3.6336, -4.7877, -3.6814],\n",
      "         [-3.6711, -2.2054, -1.6943,  ..., -1.2035, -1.7376, -2.2069],\n",
      "         [-5.0107, -4.6229, -4.2428,  ..., -4.4974, -4.2740, -4.5949]]]), \n",
      "output shape is torch.Size([1, 48, 5000]) \n",
      "predicted_token is 14\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4, 1614, 24, 14, 43, 6, 23, 11, 94, 2599, 8, 1293, 32, 14]\n",
      "-------\n",
      "output is tensor([[[-4.3026, -3.5925, -2.9601,  ..., -3.8329, -4.3850, -3.2792],\n",
      "         [-3.9646, -3.2028, -3.3477,  ..., -4.3771, -4.2461, -3.3694],\n",
      "         [-2.9570, -2.7214, -3.7442,  ..., -3.5953, -3.7089, -3.4812],\n",
      "         ...,\n",
      "         [-3.7326, -2.2676, -1.8866,  ..., -1.5612, -2.1620, -2.4998],\n",
      "         [-5.6740, -5.1790, -4.5064,  ..., -4.6050, -4.8081, -5.1409],\n",
      "         [-3.8040, -4.2553, -2.7462,  ..., -4.1447, -3.7932, -3.7875]]]), \n",
      "output shape is torch.Size([1, 49, 5000]) \n",
      "predicted_token is 802\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4, 1614, 24, 14, 43, 6, 23, 11, 94, 2599, 8, 1293, 32, 14, 802]\n",
      "-------\n",
      "output is tensor([[[-5.2742, -4.6584, -4.4756,  ..., -5.1724, -6.0400, -4.6217],\n",
      "         [-5.3368, -5.0038, -5.0401,  ..., -5.1649, -5.7850, -5.5921],\n",
      "         [-3.2144, -3.1680, -4.2477,  ..., -4.2510, -3.8695, -3.9512],\n",
      "         ...,\n",
      "         [-4.7294, -4.4805, -3.8179,  ..., -4.4060, -4.2279, -4.5222],\n",
      "         [-3.6853, -3.9074, -2.3361,  ..., -3.7875, -3.5823, -3.0480],\n",
      "         [-4.5757, -4.0099, -3.4250,  ..., -4.0635, -4.8699, -4.3136]]]), \n",
      "output shape is torch.Size([1, 50, 5000]) \n",
      "predicted_token is 3\n",
      "generated_sequence is [0, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4, 1614, 24, 14, 43, 6, 23, 11, 94, 2599, 8, 1293, 32, 14, 802, 3]\n",
      "-------\n",
      "Generated sequence:  ⁇  day, a little girl named lily found a needle in her room. she knew it was difficult to play with it because it was sharp. lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n"
     ]
    }
   ],
   "source": [
    "def word_to_token_id(word, sp_model):\n",
    "    # Convert word to token ID using SentencePiece\n",
    "    return sp_model.piece_to_id(word)\n",
    "\n",
    "def generate_text(model, sp_model, starting_word, ending_word, max_length):\n",
    "    \n",
    "    # Convert starting and ending words to token IDs\n",
    "    starting_token_id = word_to_token_id(starting_word, sp_model)\n",
    "    if starting_token_id is None:\n",
    "        raise ValueError(f\"Starting word '{starting_word}' not found in vocabulary.\")\n",
    "    \n",
    "    ending_token_id = word_to_token_id(ending_word, sp_model)\n",
    "    if ending_token_id is None:\n",
    "        raise ValueError(f\"Ending word '{ending_word}' not found in vocabulary.\")\n",
    "    \n",
    "    \n",
    "    generated_sequence = [starting_token_id]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            input_tensor = torch.tensor([generated_sequence])\n",
    "            output = model(input_tensor)\n",
    "            print(f\"output is {output}, \\noutput shape is {output.shape} \")\n",
    "            \n",
    "            predicted_token = output.argmax(-1)[:,-1].item()\n",
    "            print(f\"predicted_token is {predicted_token}\")\n",
    "            \n",
    "            generated_sequence.append(predicted_token)\n",
    "            print(f\"generated_sequence is {generated_sequence}\")\n",
    "            \n",
    "            if predicted_token == ending_token_id:\n",
    "                break\n",
    "            print(\"-------\")\n",
    "            \n",
    "    # Convert token IDs to words using SentencePiece\n",
    "    generated_text = sp_model.decode_ids(generated_sequence)\n",
    "    return generated_text\n",
    "\n",
    "# Example usage:\n",
    "starting_word = \"<sos>\"\n",
    "ending_word = \"</sos>\"\n",
    "max_length = 50\n",
    "generated_sequence = generate_text(transformer, sp, starting_word, ending_word, max_length)\n",
    "print(\"Generated sequence:\", generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual trained sentence is \n",
      "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The actual trained sentence is \\n{train_df['text'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated_sequence for 50 words \n",
      " ⁇  day, a little girl named lily found a needle in her room. she knew it was difficult to play with it because it was sharp. lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n"
     ]
    }
   ],
   "source": [
    "print(f\"generated_sequence for 50 words \\n{generated_sequence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\n ⁇  day, a her mom, a button on her shirt. lily, a little girl name a button on her mom and said, \\n lily wanted to share the needle and fix your shirt. lily went to her mom and sew a button on her mom a'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "100 epochs\n",
    "⁇  day, a her mom, a button on her shirt. lily, a little girl name a button on her mom and said, \n",
    "lily wanted to share the needle and fix your shirt. lily went to her mom and sew a button on her mom\n",
    " \n",
    " \n",
    "1000 epochs\n",
    "⁇  day, a little girl named lily found a needle in her room. she knew it was difficult to play with \n",
    "it because it was sharp. lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
