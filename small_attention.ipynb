{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beesamprajveenkumar/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import sentencepiece as spm\n",
    "import gensim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json('train_data.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One day, a little girl named Lily found a need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there was a little car named...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One day, a little fish named Fin was swimming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, in a land full of trees, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there was a little girl name...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  One day, a little girl named Lily found a need...\n",
       "1  Once upon a time, there was a little car named...\n",
       "2  One day, a little fish named Fin was swimming ...\n",
       "3  Once upon a time, in a land full of trees, the...\n",
       "4  Once upon a time, there was a little girl name..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One day, a little girl named Lily found a need...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  One day, a little girl named Lily found a need..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df[:1]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inputs_and_labels(df, sp):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        tokens = text.split()  # Tokenize the text by splitting on whitespace\n",
    "\n",
    "        # Generate input by adding start of sentence token at the beginning\n",
    "        input_sequence = tokens\n",
    "\n",
    "        # Generate label by adding end of sentence token at the end\n",
    "        label_sequence = tokens\n",
    "\n",
    "        inputs.append(input_sequence)\n",
    "        labels.append(label_sequence)\n",
    "\n",
    "    # Convert input and label sequences to strings\n",
    "    input_strings = [' '.join(sequence) for sequence in inputs]\n",
    "    label_strings = [' '.join(sequence) for sequence in labels]\n",
    "\n",
    "    # Tokenize input strings and add <sos> token at the beginning\n",
    "    tokenized_inputs = []\n",
    "    input_ids = []\n",
    "    for sequence in input_strings:\n",
    "        tokenized_sequence = sp.encode_as_pieces(sequence)\n",
    "        tokenized_sequence = ['<sos>'] + tokenized_sequence  # Add <sos> token manually\n",
    "        input_ids.append([sp.piece_to_id('<sos>')] + sp.encode_as_ids(sequence))  # Get token IDs\n",
    "        tokenized_inputs.append(tokenized_sequence)\n",
    "\n",
    "    # Tokenize label strings and add </sos> token at the end\n",
    "    tokenized_labels = []\n",
    "    label_ids = []\n",
    "    for sequence in label_strings:\n",
    "        tokenized_sequence = sp.encode_as_pieces(sequence)\n",
    "        tokenized_sequence.append('</sos>')  # Add </sos> token manually at the end\n",
    "        label_ids.append(sp.encode_as_ids(sequence) + [sp.piece_to_id('</sos>')])  # Get token IDs\n",
    "        tokenized_labels.append(tokenized_sequence)\n",
    "\n",
    "    # Print tokenized input and label sequences\n",
    "    for i in range(len(inputs)):\n",
    "        print(\"Input Text:\", input_strings[i])\n",
    "        print(\"Tokenized Input:\", tokenized_inputs[i])\n",
    "        print(\"Input IDs:\", input_ids[i])\n",
    "        print(\"Label Text:\", label_strings[i])\n",
    "        print(\"Tokenized Label:\", tokenized_labels[i])\n",
    "        print(\"Label IDs:\", label_ids[i])\n",
    "\n",
    "    return input_ids, label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt. Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\" Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n",
      "Tokenized Input: ['<sos>', '▁one', '▁day', ',', '▁a', '▁little', '▁girl', '▁name', 'd', '▁lily', '▁found', '▁a', '▁needle', '▁in', '▁her', '▁room', '.', '▁she', '▁knew', '▁it', '▁was', '▁difficult', '▁to', '▁play', '▁with', '▁it', '▁', 'because', '▁it', '▁was', '▁sharp', '.', '▁lily', '▁wanted', '▁to', '▁share', '▁the', '▁needle', '▁with', '▁her', '▁mom', ',', '▁so', '▁she', '▁could', '▁sew', '▁a', '▁button', '▁on', '▁her', '▁shirt', '.', '▁lily', '▁went', '▁to', '▁her', '▁mom', '▁and', '▁said', ',', '▁\"', 'mom', ',', '▁i', '▁found', '▁this', '▁needle', '.', '▁can', '▁you', '▁share', '▁it', '▁with', '▁me', '▁and', '▁sew', '▁my', '▁shirt', '?\"', '▁her', '▁mom', '▁smiled', '▁and', '▁said', ',', '▁\"', 'yes', ',', '▁lily', ',', '▁we', '▁can', '▁share', '▁the', '▁needle', '▁and', '▁fix', '▁your', '▁shirt', '.\"', '▁together', ',', '▁they', '▁shared', '▁the', '▁needle', '▁and', '▁sew', 'ed', '▁the', '▁button', '▁on', '▁lily', \"'\", 's', '▁shirt', '.', '▁it', '▁was', '▁not', '▁difficult', '▁for', '▁them', '▁', 'because', '▁they', '▁were', '▁sharing', '▁and', '▁helping', '▁each', '▁other', '.', '▁after', '▁they', '▁finished', ',', '▁lily', '▁thank', 'ed', '▁her', '▁mom', '▁for', '▁sharing', '▁the', '▁needle', '▁and', '▁fix', 'ing', '▁her', '▁shirt', '.', '▁they', '▁both', '▁felt', '▁happy', '▁', 'because', '▁they', '▁had', '▁shared', '▁and', '▁worked', '▁together', '.']\n",
      "Input IDs: [0, 38, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4, 1614, 24, 14, 43, 6, 23, 11, 94, 2599, 8, 1293, 32, 14, 802, 3, 31, 68, 7, 14, 43, 5, 18, 6, 16, 749, 6, 49, 119, 149, 1614, 3, 66, 25, 259, 12, 24, 145, 5, 2599, 140, 802, 82, 14, 43, 76, 5, 18, 6, 16, 257, 6, 31, 6, 96, 66, 259, 4, 1614, 5, 524, 129, 802, 46, 104, 6, 13, 678, 4, 1614, 5, 2599, 20, 4, 1293, 32, 31, 17, 15, 802, 3, 12, 9, 60, 1455, 36, 64, 19, 230, 13, 50, 1901, 5, 820, 183, 125, 3, 167, 13, 589, 6, 31, 146, 20, 14, 43, 36, 1901, 4, 1614, 5, 524, 55, 14, 802, 3, 13, 261, 90, 44, 19, 230, 13, 29, 678, 5, 521, 104, 3]\n",
      "Label Text: One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt. Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\" Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n",
      "Tokenized Label: ['▁one', '▁day', ',', '▁a', '▁little', '▁girl', '▁name', 'd', '▁lily', '▁found', '▁a', '▁needle', '▁in', '▁her', '▁room', '.', '▁she', '▁knew', '▁it', '▁was', '▁difficult', '▁to', '▁play', '▁with', '▁it', '▁', 'because', '▁it', '▁was', '▁sharp', '.', '▁lily', '▁wanted', '▁to', '▁share', '▁the', '▁needle', '▁with', '▁her', '▁mom', ',', '▁so', '▁she', '▁could', '▁sew', '▁a', '▁button', '▁on', '▁her', '▁shirt', '.', '▁lily', '▁went', '▁to', '▁her', '▁mom', '▁and', '▁said', ',', '▁\"', 'mom', ',', '▁i', '▁found', '▁this', '▁needle', '.', '▁can', '▁you', '▁share', '▁it', '▁with', '▁me', '▁and', '▁sew', '▁my', '▁shirt', '?\"', '▁her', '▁mom', '▁smiled', '▁and', '▁said', ',', '▁\"', 'yes', ',', '▁lily', ',', '▁we', '▁can', '▁share', '▁the', '▁needle', '▁and', '▁fix', '▁your', '▁shirt', '.\"', '▁together', ',', '▁they', '▁shared', '▁the', '▁needle', '▁and', '▁sew', 'ed', '▁the', '▁button', '▁on', '▁lily', \"'\", 's', '▁shirt', '.', '▁it', '▁was', '▁not', '▁difficult', '▁for', '▁them', '▁', 'because', '▁they', '▁were', '▁sharing', '▁and', '▁helping', '▁each', '▁other', '.', '▁after', '▁they', '▁finished', ',', '▁lily', '▁thank', 'ed', '▁her', '▁mom', '▁for', '▁sharing', '▁the', '▁needle', '▁and', '▁fix', 'ing', '▁her', '▁shirt', '.', '▁they', '▁both', '▁felt', '▁happy', '▁', 'because', '▁they', '▁had', '▁shared', '▁and', '▁worked', '▁together', '.', '</sos>']\n",
      "Label IDs: [38, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4, 1614, 24, 14, 43, 6, 23, 11, 94, 2599, 8, 1293, 32, 14, 802, 3, 31, 68, 7, 14, 43, 5, 18, 6, 16, 749, 6, 49, 119, 149, 1614, 3, 66, 25, 259, 12, 24, 145, 5, 2599, 140, 802, 82, 14, 43, 76, 5, 18, 6, 16, 257, 6, 31, 6, 96, 66, 259, 4, 1614, 5, 524, 129, 802, 46, 104, 6, 13, 678, 4, 1614, 5, 2599, 20, 4, 1293, 32, 31, 17, 15, 802, 3, 12, 9, 60, 1455, 36, 64, 19, 230, 13, 50, 1901, 5, 820, 183, 125, 3, 167, 13, 589, 6, 31, 146, 20, 14, 43, 36, 1901, 4, 1614, 5, 524, 55, 14, 802, 3, 13, 261, 90, 44, 19, 230, 13, 29, 678, 5, 521, 104, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('small_m.model')\n",
    "\n",
    "source_ids, target_ids = generate_inputs_and_labels(train_df, sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "        pe = self._init_pe(max_seq_length, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_length = x.size(1)\n",
    "        pe = self.pe[:seq_length, :] if seq_length <= self.max_seq_length else self._init_pe(seq_length, self.d_model)\n",
    "        return x + pe\n",
    "\n",
    "    def _init_pe(self, seq_length, d_model):\n",
    "        pe = torch.zeros(seq_length, d_model)\n",
    "        position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        return pe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, decoder_input, decoder_mask):\n",
    "        attn_output = self.self_attn(decoder_input, decoder_input, decoder_input, decoder_mask)\n",
    "        x = self.norm1(decoder_input + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, tgt):\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return tgt_mask\n",
    "\n",
    "    def forward(self, decoder_input):\n",
    "        decoder_mask = self.generate_mask(decoder_input)\n",
    "        decoder_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(decoder_input)))\n",
    "\n",
    "        dec_output = decoder_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, decoder_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "# Initialize the model\n",
    "transformer = Transformer(tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (decoder_embedding): Embedding(5000, 512)\n",
      "  (positional_encoding): PositionalEncoding()\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-5): 6 x DecoderLayer(\n",
      "      (self_attn): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (feed_forward): PositionWiseFeedForward(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=512, out_features=5000, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data shape: torch.Size([1, 165])\n",
      "Target data shape: torch.Size([1, 165])\n",
      "tensor([[   0,   38,   28,    6,    8,   37,   53,   86,   34,   31,  119,    8,\n",
      "         1614,   21,   14,  198,    3,   11,  185,   12,    9, 1455,    7,   54,\n",
      "           24,   12,   19,  230,   12,    9, 1316,    3,   31,   59,    7,  259,\n",
      "            4, 1614,   24,   14,   43,    6,   23,   11,   94, 2599,    8, 1293,\n",
      "           32,   14,  802,    3,   31,   68,    7,   14,   43,    5,   18,    6,\n",
      "           16,  749,    6,   49,  119,  149, 1614,    3,   66,   25,  259,   12,\n",
      "           24,  145,    5, 2599,  140,  802,   82,   14,   43,   76,    5,   18,\n",
      "            6,   16,  257,    6,   31,    6,   96,   66,  259,    4, 1614,    5,\n",
      "          524,  129,  802,   46,  104,    6,   13,  678,    4, 1614,    5, 2599,\n",
      "           20,    4, 1293,   32,   31,   17,   15,  802,    3,   12,    9,   60,\n",
      "         1455,   36,   64,   19,  230,   13,   50, 1901,    5,  820,  183,  125,\n",
      "            3,  167,   13,  589,    6,   31,  146,   20,   14,   43,   36, 1901,\n",
      "            4, 1614,    5,  524,   55,   14,  802,    3,   13,  261,   90,   44,\n",
      "           19,  230,   13,   29,  678,    5,  521,  104,    3]])\n",
      "tensor([[  38,   28,    6,    8,   37,   53,   86,   34,   31,  119,    8, 1614,\n",
      "           21,   14,  198,    3,   11,  185,   12,    9, 1455,    7,   54,   24,\n",
      "           12,   19,  230,   12,    9, 1316,    3,   31,   59,    7,  259,    4,\n",
      "         1614,   24,   14,   43,    6,   23,   11,   94, 2599,    8, 1293,   32,\n",
      "           14,  802,    3,   31,   68,    7,   14,   43,    5,   18,    6,   16,\n",
      "          749,    6,   49,  119,  149, 1614,    3,   66,   25,  259,   12,   24,\n",
      "          145,    5, 2599,  140,  802,   82,   14,   43,   76,    5,   18,    6,\n",
      "           16,  257,    6,   31,    6,   96,   66,  259,    4, 1614,    5,  524,\n",
      "          129,  802,   46,  104,    6,   13,  678,    4, 1614,    5, 2599,   20,\n",
      "            4, 1293,   32,   31,   17,   15,  802,    3,   12,    9,   60, 1455,\n",
      "           36,   64,   19,  230,   13,   50, 1901,    5,  820,  183,  125,    3,\n",
      "          167,   13,  589,    6,   31,  146,   20,   14,   43,   36, 1901,    4,\n",
      "         1614,    5,  524,   55,   14,  802,    3,   13,  261,   90,   44,   19,\n",
      "          230,   13,   29,  678,    5,  521,  104,    3,    0]])\n"
     ]
    }
   ],
   "source": [
    "# Convert lists to tensors\n",
    "source_data = torch.tensor(source_ids)\n",
    "target_data = torch.tensor(target_ids)\n",
    "\n",
    "# Print shapes\n",
    "print(\"Source data shape:\", source_data.shape)\n",
    "print(\"Target data shape:\", target_data.shape)\n",
    "\n",
    "print(source_data)\n",
    "print(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.730408668518066\n",
      "Epoch: 2, Loss: 6.724423885345459\n",
      "Epoch: 3, Loss: 5.932013034820557\n",
      "Epoch: 4, Loss: 4.820430278778076\n",
      "Epoch: 5, Loss: 4.6376953125\n",
      "Epoch: 6, Loss: 3.744565010070801\n",
      "Epoch: 7, Loss: 2.657294988632202\n",
      "Epoch: 8, Loss: 2.8864426612854004\n",
      "Epoch: 9, Loss: 1.6400723457336426\n",
      "Epoch: 10, Loss: 1.3415610790252686\n",
      "Epoch: 11, Loss: 1.0539830923080444\n",
      "Epoch: 12, Loss: 0.7803583741188049\n",
      "Epoch: 13, Loss: 0.6372760534286499\n",
      "Epoch: 14, Loss: 0.5447917580604553\n",
      "Epoch: 15, Loss: 0.4480459988117218\n",
      "Epoch: 16, Loss: 0.3882507085800171\n",
      "Epoch: 17, Loss: 0.33517950773239136\n",
      "Epoch: 18, Loss: 0.2567233741283417\n",
      "Epoch: 19, Loss: 0.21566767990589142\n",
      "Epoch: 20, Loss: 0.17569686472415924\n",
      "Epoch: 21, Loss: 0.13717955350875854\n",
      "Epoch: 22, Loss: 0.10817677527666092\n",
      "Epoch: 23, Loss: 0.08583451062440872\n",
      "Epoch: 24, Loss: 0.06548786908388138\n",
      "Epoch: 25, Loss: 0.06057952716946602\n",
      "Epoch: 26, Loss: 0.04204702004790306\n",
      "Epoch: 27, Loss: 0.03391093388199806\n",
      "Epoch: 28, Loss: 0.028590522706508636\n",
      "Epoch: 29, Loss: 0.02024346962571144\n",
      "Epoch: 30, Loss: 0.01563118025660515\n",
      "Epoch: 31, Loss: 0.012787721119821072\n",
      "Epoch: 32, Loss: 0.012701379135251045\n",
      "Epoch: 33, Loss: 0.009592114016413689\n",
      "Epoch: 34, Loss: 0.007322151679545641\n",
      "Epoch: 35, Loss: 0.008722612634301186\n",
      "Epoch: 36, Loss: 0.005979201756417751\n",
      "Epoch: 37, Loss: 0.005611059255897999\n",
      "Epoch: 38, Loss: 0.025334680452942848\n",
      "Epoch: 39, Loss: 0.004852797836065292\n",
      "Epoch: 40, Loss: 0.06444548815488815\n",
      "Epoch: 41, Loss: 0.004986822605133057\n",
      "Epoch: 42, Loss: 0.05605074390769005\n",
      "Epoch: 43, Loss: 0.014165446162223816\n",
      "Epoch: 44, Loss: 0.0052008177153766155\n",
      "Epoch: 45, Loss: 0.004239873960614204\n",
      "Epoch: 46, Loss: 0.010723371990025043\n",
      "Epoch: 47, Loss: 0.004581252112984657\n",
      "Epoch: 48, Loss: 0.004376203753054142\n",
      "Epoch: 49, Loss: 0.004580463748425245\n",
      "Epoch: 50, Loss: 0.0035130567848682404\n",
      "Epoch: 51, Loss: 0.004268574994057417\n",
      "Epoch: 52, Loss: 0.0037169361021369696\n",
      "Epoch: 53, Loss: 0.004059012979269028\n",
      "Epoch: 54, Loss: 0.009295220486819744\n",
      "Epoch: 55, Loss: 0.0022364393807947636\n",
      "Epoch: 56, Loss: 0.03415356203913689\n",
      "Epoch: 57, Loss: 0.0036850948818027973\n",
      "Epoch: 58, Loss: 0.00248189689591527\n",
      "Epoch: 59, Loss: 0.016896283254027367\n",
      "Epoch: 60, Loss: 0.00253578694537282\n",
      "Epoch: 61, Loss: 0.04307389259338379\n",
      "Epoch: 62, Loss: 0.008378573693335056\n",
      "Epoch: 63, Loss: 0.025532130151987076\n",
      "Epoch: 64, Loss: 0.007334653753787279\n",
      "Epoch: 65, Loss: 0.03027806431055069\n",
      "Epoch: 66, Loss: 0.002322864718735218\n",
      "Epoch: 67, Loss: 0.010304505936801434\n",
      "Epoch: 68, Loss: 0.017800861969590187\n",
      "Epoch: 69, Loss: 0.0027504898607730865\n",
      "Epoch: 70, Loss: 0.007364175748080015\n",
      "Epoch: 71, Loss: 0.01337949838489294\n",
      "Epoch: 72, Loss: 0.0019921669736504555\n",
      "Epoch: 73, Loss: 0.002303791232407093\n",
      "Epoch: 74, Loss: 0.029134899377822876\n",
      "Epoch: 75, Loss: 0.004904933273792267\n",
      "Epoch: 76, Loss: 0.002326608169823885\n",
      "Epoch: 77, Loss: 0.04939499869942665\n",
      "Epoch: 78, Loss: 0.04430096969008446\n",
      "Epoch: 79, Loss: 0.01561279408633709\n",
      "Epoch: 80, Loss: 0.0054586115293204784\n",
      "Epoch: 81, Loss: 0.021632906049489975\n",
      "Epoch: 82, Loss: 0.014447925612330437\n",
      "Epoch: 83, Loss: 0.0037687018048018217\n",
      "Epoch: 84, Loss: 0.0027862314600497484\n",
      "Epoch: 85, Loss: 0.004413417540490627\n",
      "Epoch: 86, Loss: 0.003617269219830632\n",
      "Epoch: 87, Loss: 0.009146517142653465\n",
      "Epoch: 88, Loss: 0.002529808320105076\n",
      "Epoch: 89, Loss: 0.0035886720288544893\n",
      "Epoch: 90, Loss: 0.028432447463274002\n",
      "Epoch: 91, Loss: 0.0017750859260559082\n",
      "Epoch: 92, Loss: 0.003547823755070567\n",
      "Epoch: 93, Loss: 0.01672648824751377\n",
      "Epoch: 94, Loss: 0.0016756270779296756\n",
      "Epoch: 95, Loss: 0.0024889353662729263\n",
      "Epoch: 96, Loss: 0.0036560518201440573\n",
      "Epoch: 97, Loss: 0.005094111431390047\n",
      "Epoch: 98, Loss: 0.0017897807992994785\n",
      "Epoch: 99, Loss: 0.0015410800697281957\n",
      "Epoch: 100, Loss: 0.0013190648751333356\n",
      "Epoch: 101, Loss: 0.001220215461216867\n",
      "Epoch: 102, Loss: 0.0012996273580938578\n",
      "Epoch: 103, Loss: 0.001218295656144619\n",
      "Epoch: 104, Loss: 0.0015594606520608068\n",
      "Epoch: 105, Loss: 0.0013034141156822443\n",
      "Epoch: 106, Loss: 0.001438978360965848\n",
      "Epoch: 107, Loss: 0.001614049426279962\n",
      "Epoch: 108, Loss: 0.0010838890448212624\n",
      "Epoch: 109, Loss: 0.0011071921326220036\n",
      "Epoch: 110, Loss: 0.0009127897210419178\n",
      "Epoch: 111, Loss: 0.0009744379785843194\n",
      "Epoch: 112, Loss: 0.0009319350938312709\n",
      "Epoch: 113, Loss: 0.0008693618001416326\n",
      "Epoch: 114, Loss: 0.0008954920922406018\n",
      "Epoch: 115, Loss: 0.0008397523197345436\n",
      "Epoch: 116, Loss: 0.0007656757370568812\n",
      "Epoch: 117, Loss: 0.000801530375611037\n",
      "Epoch: 118, Loss: 0.0007463031215593219\n",
      "Epoch: 119, Loss: 0.0007054338930174708\n",
      "Epoch: 120, Loss: 0.0007187614683061838\n",
      "Epoch: 121, Loss: 0.0006737066432833672\n",
      "Epoch: 122, Loss: 0.0006451108492910862\n",
      "Epoch: 123, Loss: 0.0006554078427143395\n",
      "Epoch: 124, Loss: 0.0006681278464384377\n",
      "Epoch: 125, Loss: 0.0006408162880688906\n",
      "Epoch: 126, Loss: 0.0006667615380138159\n",
      "Epoch: 127, Loss: 0.0006176752503961325\n",
      "Epoch: 128, Loss: 0.0005916693480685353\n",
      "Epoch: 129, Loss: 0.0006036656559444964\n",
      "Epoch: 130, Loss: 0.0005431182216852903\n",
      "Epoch: 131, Loss: 0.0005360068753361702\n",
      "Epoch: 132, Loss: 0.0005398839712142944\n",
      "Epoch: 133, Loss: 0.0005247703520581126\n",
      "Epoch: 134, Loss: 0.0005248262314125896\n",
      "Epoch: 135, Loss: 0.0005163797759450972\n",
      "Epoch: 136, Loss: 0.000499326444696635\n",
      "Epoch: 137, Loss: 0.0004919858765788376\n",
      "Epoch: 138, Loss: 0.00048301537754014134\n",
      "Epoch: 139, Loss: 0.00048237215378321707\n",
      "Epoch: 140, Loss: 0.0004789105150848627\n",
      "Epoch: 141, Loss: 0.00046885732444934547\n",
      "Epoch: 142, Loss: 0.00046806136379018426\n",
      "Epoch: 143, Loss: 0.00046801738790236413\n",
      "Epoch: 144, Loss: 0.0004784213670063764\n",
      "Epoch: 145, Loss: 0.0004535383777692914\n",
      "Epoch: 146, Loss: 0.00044698957935906947\n",
      "Epoch: 147, Loss: 0.000440246396465227\n",
      "Epoch: 148, Loss: 0.0004292523954063654\n",
      "Epoch: 149, Loss: 0.0004401588230393827\n",
      "Epoch: 150, Loss: 0.00042085035238415003\n",
      "Epoch: 151, Loss: 0.00043413793900981545\n",
      "Epoch: 152, Loss: 0.00041010993300005794\n",
      "Epoch: 153, Loss: 0.0004242317518219352\n",
      "Epoch: 154, Loss: 0.00039493024814873934\n",
      "Epoch: 155, Loss: 0.000409185653552413\n",
      "Epoch: 156, Loss: 0.0003916017303708941\n",
      "Epoch: 157, Loss: 0.00039499360718764365\n",
      "Epoch: 158, Loss: 0.0003763388376682997\n",
      "Epoch: 159, Loss: 0.0003879116557072848\n",
      "Epoch: 160, Loss: 0.0003754555364139378\n",
      "Epoch: 161, Loss: 0.00037722723209299147\n",
      "Epoch: 162, Loss: 0.00036924832966178656\n",
      "Epoch: 163, Loss: 0.0003763572603929788\n",
      "Epoch: 164, Loss: 0.00036061074933968484\n",
      "Epoch: 165, Loss: 0.00036614687996916473\n",
      "Epoch: 166, Loss: 0.00035338717862032354\n",
      "Epoch: 167, Loss: 0.00036716533941216767\n",
      "Epoch: 168, Loss: 0.0003526850778143853\n",
      "Epoch: 169, Loss: 0.0003421971050556749\n",
      "Epoch: 170, Loss: 0.00033983337925747037\n",
      "Epoch: 171, Loss: 0.0003340638359077275\n",
      "Epoch: 172, Loss: 0.00033110223012045026\n",
      "Epoch: 173, Loss: 0.0003379848785698414\n",
      "Epoch: 174, Loss: 0.0003304997517261654\n",
      "Epoch: 175, Loss: 0.000331406103214249\n",
      "Epoch: 176, Loss: 0.0003330291365273297\n",
      "Epoch: 177, Loss: 0.00032976188231259584\n",
      "Epoch: 178, Loss: 0.00031903028138913214\n",
      "Epoch: 179, Loss: 0.00031072917045094073\n",
      "Epoch: 180, Loss: 0.00033080700086429715\n",
      "Epoch: 181, Loss: 0.0003155624435748905\n",
      "Epoch: 182, Loss: 0.00030916096875444055\n",
      "Epoch: 183, Loss: 0.0003119056054856628\n",
      "Epoch: 184, Loss: 0.00030108800274319947\n",
      "Epoch: 185, Loss: 0.0002992328954860568\n",
      "Epoch: 186, Loss: 0.0002994331007357687\n",
      "Epoch: 187, Loss: 0.0002905552100855857\n",
      "Epoch: 188, Loss: 0.00029193301452323794\n",
      "Epoch: 189, Loss: 0.0003053140826523304\n",
      "Epoch: 190, Loss: 0.0002852506295312196\n",
      "Epoch: 191, Loss: 0.0002965462626889348\n",
      "Epoch: 192, Loss: 0.0002904021821450442\n",
      "Epoch: 193, Loss: 0.0002850183518603444\n",
      "Epoch: 194, Loss: 0.0002819760993588716\n",
      "Epoch: 195, Loss: 0.00027592849801294506\n",
      "Epoch: 196, Loss: 0.0002729427651502192\n",
      "Epoch: 197, Loss: 0.0002749811392277479\n",
      "Epoch: 198, Loss: 0.0002705191436689347\n",
      "Epoch: 199, Loss: 0.0002645612112246454\n",
      "Epoch: 200, Loss: 0.000269775977358222\n",
      "Epoch: 201, Loss: 0.0002644956111907959\n",
      "Epoch: 202, Loss: 0.0002616617421153933\n",
      "Epoch: 203, Loss: 0.0002577683189883828\n",
      "Epoch: 204, Loss: 0.00025097018806263804\n",
      "Epoch: 205, Loss: 0.00025864620693027973\n",
      "Epoch: 206, Loss: 0.0002552810765337199\n",
      "Epoch: 207, Loss: 0.0002542728034313768\n",
      "Epoch: 208, Loss: 0.0002533871156629175\n",
      "Epoch: 209, Loss: 0.00024993400438688695\n",
      "Epoch: 210, Loss: 0.00024332894827239215\n",
      "Epoch: 211, Loss: 0.00024185358779504895\n",
      "Epoch: 212, Loss: 0.00024675490567460656\n",
      "Epoch: 213, Loss: 0.00023394021263811737\n",
      "Epoch: 214, Loss: 0.00024039264826569706\n",
      "Epoch: 215, Loss: 0.0002448893210384995\n",
      "Epoch: 216, Loss: 0.0002385052212048322\n",
      "Epoch: 217, Loss: 0.0002342965453863144\n",
      "Epoch: 218, Loss: 0.00023218648857437074\n",
      "Epoch: 219, Loss: 0.00024316653434652835\n",
      "Epoch: 220, Loss: 0.00022230301692616194\n",
      "Epoch: 221, Loss: 0.0002222319453721866\n",
      "Epoch: 222, Loss: 0.00021427558385767043\n",
      "Epoch: 223, Loss: 0.00021787331206724048\n",
      "Epoch: 224, Loss: 0.0002243147901026532\n",
      "Epoch: 225, Loss: 0.00022169348085299134\n",
      "Epoch: 226, Loss: 0.00021119779557920992\n",
      "Epoch: 227, Loss: 0.0002151970547856763\n",
      "Epoch: 228, Loss: 0.0002177238347940147\n",
      "Epoch: 229, Loss: 0.00020701887842733413\n",
      "Epoch: 230, Loss: 0.00020925335411448032\n",
      "Epoch: 231, Loss: 0.00021024134184699506\n",
      "Epoch: 232, Loss: 0.00021287309937179089\n",
      "Epoch: 233, Loss: 0.0002077285898849368\n",
      "Epoch: 234, Loss: 0.0002066116576315835\n",
      "Epoch: 235, Loss: 0.00019993925525341183\n",
      "Epoch: 236, Loss: 0.0002012028417084366\n",
      "Epoch: 237, Loss: 0.00020827596017625183\n",
      "Epoch: 238, Loss: 0.00019845123460981995\n",
      "Epoch: 239, Loss: 0.00019738248374778777\n",
      "Epoch: 240, Loss: 0.0001927887205965817\n",
      "Epoch: 241, Loss: 0.0001974204642465338\n",
      "Epoch: 242, Loss: 0.00019125120888929814\n",
      "Epoch: 243, Loss: 0.00019052265270147473\n",
      "Epoch: 244, Loss: 0.0001869297120720148\n",
      "Epoch: 245, Loss: 0.0001856115850387141\n",
      "Epoch: 246, Loss: 0.0001882143405964598\n",
      "Epoch: 247, Loss: 0.00018168441602028906\n",
      "Epoch: 248, Loss: 0.00018174893921241164\n",
      "Epoch: 249, Loss: 0.00018468874623067677\n",
      "Epoch: 250, Loss: 0.00018292333697900176\n",
      "Epoch: 251, Loss: 0.00017206958727911115\n",
      "Epoch: 252, Loss: 0.00017544387083034962\n",
      "Epoch: 253, Loss: 0.00017401423247065395\n",
      "Epoch: 254, Loss: 0.00017099389515351504\n",
      "Epoch: 255, Loss: 0.0001707701594568789\n",
      "Epoch: 256, Loss: 0.000170031504239887\n",
      "Epoch: 257, Loss: 0.0001678820699453354\n",
      "Epoch: 258, Loss: 0.0001662580471020192\n",
      "Epoch: 259, Loss: 0.0001642495917622\n",
      "Epoch: 260, Loss: 0.00016679747204761952\n",
      "Epoch: 261, Loss: 0.00016065657837316394\n",
      "Epoch: 262, Loss: 0.0001587750157341361\n",
      "Epoch: 263, Loss: 0.0001586404105182737\n",
      "Epoch: 264, Loss: 0.00016580737428739667\n",
      "Epoch: 265, Loss: 0.00015603567590005696\n",
      "Epoch: 266, Loss: 0.00015815622464288026\n",
      "Epoch: 267, Loss: 0.00015387218445539474\n",
      "Epoch: 268, Loss: 0.00015542274923063815\n",
      "Epoch: 269, Loss: 0.0001543742255307734\n",
      "Epoch: 270, Loss: 0.00015439846902154386\n",
      "Epoch: 271, Loss: 0.00015100999735295773\n",
      "Epoch: 272, Loss: 0.0001527206040918827\n",
      "Epoch: 273, Loss: 0.0001506712578702718\n",
      "Epoch: 274, Loss: 0.00014811198343522847\n",
      "Epoch: 275, Loss: 0.00014871482562739402\n",
      "Epoch: 276, Loss: 0.00014535614172928035\n",
      "Epoch: 277, Loss: 0.00014019952504895627\n",
      "Epoch: 278, Loss: 0.0001411167613696307\n",
      "Epoch: 279, Loss: 0.0001461957726860419\n",
      "Epoch: 280, Loss: 0.00014028466830495745\n",
      "Epoch: 281, Loss: 0.0001389756507705897\n",
      "Epoch: 282, Loss: 0.00014115906378719956\n",
      "Epoch: 283, Loss: 0.00013573507021646947\n",
      "Epoch: 284, Loss: 0.0001383309718221426\n",
      "Epoch: 285, Loss: 0.00013511048746295273\n",
      "Epoch: 286, Loss: 0.00013523921370506287\n",
      "Epoch: 287, Loss: 0.0001320191367994994\n",
      "Epoch: 288, Loss: 0.0001319236762356013\n",
      "Epoch: 289, Loss: 0.0001321627205470577\n",
      "Epoch: 290, Loss: 0.00013862474588677287\n",
      "Epoch: 291, Loss: 0.00012934139522258192\n",
      "Epoch: 292, Loss: 0.00012867544137407094\n",
      "Epoch: 293, Loss: 0.00012885514297522604\n",
      "Epoch: 294, Loss: 0.00012767183943651617\n",
      "Epoch: 295, Loss: 0.0001219690966536291\n",
      "Epoch: 296, Loss: 0.000125586855574511\n",
      "Epoch: 297, Loss: 0.00011973504297202453\n",
      "Epoch: 298, Loss: 0.00012145160872023553\n",
      "Epoch: 299, Loss: 0.00012093555415049195\n",
      "Epoch: 300, Loss: 0.00012242377852089703\n",
      "Epoch: 301, Loss: 0.00011844919208670035\n",
      "Epoch: 302, Loss: 0.0001168641319964081\n",
      "Epoch: 303, Loss: 0.00011560009443201125\n",
      "Epoch: 304, Loss: 0.00012105517089366913\n",
      "Epoch: 305, Loss: 0.00011386886762920767\n",
      "Epoch: 306, Loss: 0.0001162208427558653\n",
      "Epoch: 307, Loss: 0.0001148623414337635\n",
      "Epoch: 308, Loss: 0.00011359200289007276\n",
      "Epoch: 309, Loss: 0.00011002129031112418\n",
      "Epoch: 310, Loss: 0.00011084909056080505\n",
      "Epoch: 311, Loss: 0.00011069789616158232\n",
      "Epoch: 312, Loss: 0.00010636631486704573\n",
      "Epoch: 313, Loss: 0.00010755230323411524\n",
      "Epoch: 314, Loss: 0.00011030898167518899\n",
      "Epoch: 315, Loss: 0.00010532258602324873\n",
      "Epoch: 316, Loss: 0.00010641710832715034\n",
      "Epoch: 317, Loss: 0.00010458409815328196\n",
      "Epoch: 318, Loss: 0.00010626220318954438\n",
      "Epoch: 319, Loss: 0.00010470829147379845\n",
      "Epoch: 320, Loss: 0.00010394008859293535\n",
      "Epoch: 321, Loss: 0.00010125529661308974\n",
      "Epoch: 322, Loss: 9.99573094304651e-05\n",
      "Epoch: 323, Loss: 9.693385800346732e-05\n",
      "Epoch: 324, Loss: 9.828476322581992e-05\n",
      "Epoch: 325, Loss: 9.843677253229544e-05\n",
      "Epoch: 326, Loss: 9.5648858405184e-05\n",
      "Epoch: 327, Loss: 9.540018800180405e-05\n",
      "Epoch: 328, Loss: 9.729008161230013e-05\n",
      "Epoch: 329, Loss: 9.664223034633324e-05\n",
      "Epoch: 330, Loss: 9.24835039768368e-05\n",
      "Epoch: 331, Loss: 9.298288205172867e-05\n",
      "Epoch: 332, Loss: 9.166148083750159e-05\n",
      "Epoch: 333, Loss: 8.92834214027971e-05\n",
      "Epoch: 334, Loss: 9.222902735928074e-05\n",
      "Epoch: 335, Loss: 9.192661673296243e-05\n",
      "Epoch: 336, Loss: 9.017512638820335e-05\n",
      "Epoch: 337, Loss: 8.86966590769589e-05\n",
      "Epoch: 338, Loss: 8.698733290657401e-05\n",
      "Epoch: 339, Loss: 9.150594269158319e-05\n",
      "Epoch: 340, Loss: 8.764790982240811e-05\n",
      "Epoch: 341, Loss: 8.573567902203649e-05\n",
      "Epoch: 342, Loss: 8.378717029700056e-05\n",
      "Epoch: 343, Loss: 8.468042506137863e-05\n",
      "Epoch: 344, Loss: 8.26584073365666e-05\n",
      "Epoch: 345, Loss: 8.183863974409178e-05\n",
      "Epoch: 346, Loss: 8.321143832290545e-05\n",
      "Epoch: 347, Loss: 8.066689042607322e-05\n",
      "Epoch: 348, Loss: 8.170412183972076e-05\n",
      "Epoch: 349, Loss: 7.93005820014514e-05\n",
      "Epoch: 350, Loss: 8.030423487070948e-05\n",
      "Epoch: 351, Loss: 7.907740655355155e-05\n",
      "Epoch: 352, Loss: 7.790059316903353e-05\n",
      "Epoch: 353, Loss: 7.936286419862881e-05\n",
      "Epoch: 354, Loss: 7.937948976177722e-05\n",
      "Epoch: 355, Loss: 7.852298585930839e-05\n",
      "Epoch: 356, Loss: 7.501298387069255e-05\n",
      "Epoch: 357, Loss: 7.226273737614974e-05\n",
      "Epoch: 358, Loss: 7.60550974518992e-05\n",
      "Epoch: 359, Loss: 7.514884055126458e-05\n",
      "Epoch: 360, Loss: 7.275182724697515e-05\n",
      "Epoch: 361, Loss: 7.335219561355188e-05\n",
      "Epoch: 362, Loss: 7.170593016780913e-05\n",
      "Epoch: 363, Loss: 7.231936615426093e-05\n",
      "Epoch: 364, Loss: 7.090491271810606e-05\n",
      "Epoch: 365, Loss: 7.121310045477003e-05\n",
      "Epoch: 366, Loss: 6.939462036825716e-05\n",
      "Epoch: 367, Loss: 6.891196244396269e-05\n",
      "Epoch: 368, Loss: 7.023833313724026e-05\n",
      "Epoch: 369, Loss: 6.91860041115433e-05\n",
      "Epoch: 370, Loss: 6.803538417443633e-05\n",
      "Epoch: 371, Loss: 6.834417581558228e-05\n",
      "Epoch: 372, Loss: 6.861605652375147e-05\n",
      "Epoch: 373, Loss: 6.633970770053566e-05\n",
      "Epoch: 374, Loss: 6.463240424636751e-05\n",
      "Epoch: 375, Loss: 6.609173578908667e-05\n",
      "Epoch: 376, Loss: 6.403640873031691e-05\n",
      "Epoch: 377, Loss: 6.375366501742974e-05\n",
      "Epoch: 378, Loss: 6.39317004242912e-05\n",
      "Epoch: 379, Loss: 6.961283361306414e-05\n",
      "Epoch: 380, Loss: 6.0497448430396616e-05\n",
      "Epoch: 381, Loss: 6.264811963774264e-05\n",
      "Epoch: 382, Loss: 6.096915603848174e-05\n",
      "Epoch: 383, Loss: 6.074674820411019e-05\n",
      "Epoch: 384, Loss: 6.170898268464953e-05\n",
      "Epoch: 385, Loss: 5.775218232884072e-05\n",
      "Epoch: 386, Loss: 6.137753371149302e-05\n",
      "Epoch: 387, Loss: 6.045229019946419e-05\n",
      "Epoch: 388, Loss: 5.7932415074901655e-05\n",
      "Epoch: 389, Loss: 5.826387132401578e-05\n",
      "Epoch: 390, Loss: 5.994626917527057e-05\n",
      "Epoch: 391, Loss: 5.7574095990275964e-05\n",
      "Epoch: 392, Loss: 5.5201697250595316e-05\n",
      "Epoch: 393, Loss: 5.6091314036166295e-05\n",
      "Epoch: 394, Loss: 5.4584605095442384e-05\n",
      "Epoch: 395, Loss: 5.519439582712948e-05\n",
      "Epoch: 396, Loss: 5.4271316912490875e-05\n",
      "Epoch: 397, Loss: 5.6394495913991705e-05\n",
      "Epoch: 398, Loss: 5.476108708535321e-05\n",
      "Epoch: 399, Loss: 5.339835479389876e-05\n",
      "Epoch: 400, Loss: 5.311926724971272e-05\n",
      "Epoch: 401, Loss: 5.2401857828954235e-05\n",
      "Epoch: 402, Loss: 5.153039455763064e-05\n",
      "Epoch: 403, Loss: 5.1449707825668156e-05\n",
      "Epoch: 404, Loss: 5.096561289974488e-05\n",
      "Epoch: 405, Loss: 5.081954077468254e-05\n",
      "Epoch: 406, Loss: 4.963549508829601e-05\n",
      "Epoch: 407, Loss: 5.006651554140262e-05\n",
      "Epoch: 408, Loss: 4.9281548854196444e-05\n",
      "Epoch: 409, Loss: 4.86484459543135e-05\n",
      "Epoch: 410, Loss: 4.7267458285205066e-05\n",
      "Epoch: 411, Loss: 4.96950488013681e-05\n",
      "Epoch: 412, Loss: 4.669396730605513e-05\n",
      "Epoch: 413, Loss: 4.6293458581203595e-05\n",
      "Epoch: 414, Loss: 4.6746295993216336e-05\n",
      "Epoch: 415, Loss: 4.5605862396769226e-05\n",
      "Epoch: 416, Loss: 4.4674779928755015e-05\n",
      "Epoch: 417, Loss: 4.493716187425889e-05\n",
      "Epoch: 418, Loss: 4.337518475949764e-05\n",
      "Epoch: 419, Loss: 4.4292453821981326e-05\n",
      "Epoch: 420, Loss: 4.388689194456674e-05\n",
      "Epoch: 421, Loss: 4.366077700979076e-05\n",
      "Epoch: 422, Loss: 4.305535549065098e-05\n",
      "Epoch: 423, Loss: 4.3524138163775206e-05\n",
      "Epoch: 424, Loss: 4.317888669902459e-05\n",
      "Epoch: 425, Loss: 4.268387783668004e-05\n",
      "Epoch: 426, Loss: 4.192872438579798e-05\n",
      "Epoch: 427, Loss: 4.1628532926552e-05\n",
      "Epoch: 428, Loss: 4.077665653312579e-05\n",
      "Epoch: 429, Loss: 4.0066537621896714e-05\n",
      "Epoch: 430, Loss: 4.152748442720622e-05\n",
      "Epoch: 431, Loss: 3.947704317397438e-05\n",
      "Epoch: 432, Loss: 3.954756539314985e-05\n",
      "Epoch: 433, Loss: 3.899441799148917e-05\n",
      "Epoch: 434, Loss: 3.968928649555892e-05\n",
      "Epoch: 435, Loss: 3.8889014831511304e-05\n",
      "Epoch: 436, Loss: 3.8436188333434984e-05\n",
      "Epoch: 437, Loss: 3.80116980522871e-05\n",
      "Epoch: 438, Loss: 3.7072615668876097e-05\n",
      "Epoch: 439, Loss: 3.695412306115031e-05\n",
      "Epoch: 440, Loss: 3.6257806641515344e-05\n",
      "Epoch: 441, Loss: 3.6467143218033016e-05\n",
      "Epoch: 442, Loss: 3.6634264688473195e-05\n",
      "Epoch: 443, Loss: 3.6097892007092014e-05\n",
      "Epoch: 444, Loss: 3.560436380212195e-05\n",
      "Epoch: 445, Loss: 3.67142420145683e-05\n",
      "Epoch: 446, Loss: 3.404380549909547e-05\n",
      "Epoch: 447, Loss: 3.394858504179865e-05\n",
      "Epoch: 448, Loss: 3.410339922993444e-05\n",
      "Epoch: 449, Loss: 3.286701758042909e-05\n",
      "Epoch: 450, Loss: 3.406414543860592e-05\n",
      "Epoch: 451, Loss: 3.4043787309201434e-05\n",
      "Epoch: 452, Loss: 3.507041037664749e-05\n",
      "Epoch: 453, Loss: 3.351608756929636e-05\n",
      "Epoch: 454, Loss: 3.2859006751095876e-05\n",
      "Epoch: 455, Loss: 3.2656211260473356e-05\n",
      "Epoch: 456, Loss: 3.2092895708046854e-05\n",
      "Epoch: 457, Loss: 3.209725400665775e-05\n",
      "Epoch: 458, Loss: 3.152449426124804e-05\n",
      "Epoch: 459, Loss: 3.14503486151807e-05\n",
      "Epoch: 460, Loss: 3.210668364772573e-05\n",
      "Epoch: 461, Loss: 2.9912329409853555e-05\n",
      "Epoch: 462, Loss: 3.06718866340816e-05\n",
      "Epoch: 463, Loss: 3.0384779165615328e-05\n",
      "Epoch: 464, Loss: 2.9944312700536102e-05\n",
      "Epoch: 465, Loss: 2.972915353893768e-05\n",
      "Epoch: 466, Loss: 2.993121415784117e-05\n",
      "Epoch: 467, Loss: 2.8855465643573552e-05\n",
      "Epoch: 468, Loss: 2.900665822380688e-05\n",
      "Epoch: 469, Loss: 2.8798771381843835e-05\n",
      "Epoch: 470, Loss: 2.874132951546926e-05\n",
      "Epoch: 471, Loss: 2.863958252419252e-05\n",
      "Epoch: 472, Loss: 2.757909896899946e-05\n",
      "Epoch: 473, Loss: 2.76888476946624e-05\n",
      "Epoch: 474, Loss: 2.7027401301893406e-05\n",
      "Epoch: 475, Loss: 2.7248357582720928e-05\n",
      "Epoch: 476, Loss: 2.71647768386174e-05\n",
      "Epoch: 477, Loss: 2.7456964744487777e-05\n",
      "Epoch: 478, Loss: 2.6513504053582437e-05\n",
      "Epoch: 479, Loss: 2.6312893169233575e-05\n",
      "Epoch: 480, Loss: 2.588258939795196e-05\n",
      "Epoch: 481, Loss: 2.6155163141083904e-05\n",
      "Epoch: 482, Loss: 2.5024168280651793e-05\n",
      "Epoch: 483, Loss: 2.535997009545099e-05\n",
      "Epoch: 484, Loss: 2.472106098139193e-05\n",
      "Epoch: 485, Loss: 2.5024881324497983e-05\n",
      "Epoch: 486, Loss: 2.4894769012462348e-05\n",
      "Epoch: 487, Loss: 2.493402462278027e-05\n",
      "Epoch: 488, Loss: 2.38691754930187e-05\n",
      "Epoch: 489, Loss: 2.4497163394698873e-05\n",
      "Epoch: 490, Loss: 2.339234561077319e-05\n",
      "Epoch: 491, Loss: 2.345921893720515e-05\n",
      "Epoch: 492, Loss: 2.3295675418921746e-05\n",
      "Epoch: 493, Loss: 2.3135033188736998e-05\n",
      "Epoch: 494, Loss: 2.3006377887213603e-05\n",
      "Epoch: 495, Loss: 2.2891532353241928e-05\n",
      "Epoch: 496, Loss: 2.2397265638574027e-05\n",
      "Epoch: 497, Loss: 2.328974733245559e-05\n",
      "Epoch: 498, Loss: 2.221045724581927e-05\n",
      "Epoch: 499, Loss: 2.155555375793483e-05\n",
      "Epoch: 500, Loss: 2.191244129789993e-05\n",
      "Epoch: 501, Loss: 2.1817941160406917e-05\n",
      "Epoch: 502, Loss: 2.20541642192984e-05\n",
      "Epoch: 503, Loss: 2.14603260246804e-05\n",
      "Epoch: 504, Loss: 2.098495497193653e-05\n",
      "Epoch: 505, Loss: 2.0672401660704054e-05\n",
      "Epoch: 506, Loss: 2.028643575613387e-05\n",
      "Epoch: 507, Loss: 2.019776002271101e-05\n",
      "Epoch: 508, Loss: 2.0644038158934563e-05\n",
      "Epoch: 509, Loss: 1.9940445781685412e-05\n",
      "Epoch: 510, Loss: 1.9324063032399863e-05\n",
      "Epoch: 511, Loss: 1.938366585818585e-05\n",
      "Epoch: 512, Loss: 1.9034769138670526e-05\n",
      "Epoch: 513, Loss: 1.961844100151211e-05\n",
      "Epoch: 514, Loss: 1.889011764433235e-05\n",
      "Epoch: 515, Loss: 1.8699673091759905e-05\n",
      "Epoch: 516, Loss: 1.854557922342792e-05\n",
      "Epoch: 517, Loss: 1.876073110906873e-05\n",
      "Epoch: 518, Loss: 1.8176329831476323e-05\n",
      "Epoch: 519, Loss: 1.827808955567889e-05\n",
      "Epoch: 520, Loss: 1.789866655599326e-05\n",
      "Epoch: 521, Loss: 1.7944454157259315e-05\n",
      "Epoch: 522, Loss: 1.7728572856867686e-05\n",
      "Epoch: 523, Loss: 1.7752563508111052e-05\n",
      "Epoch: 524, Loss: 1.7349877452943474e-05\n",
      "Epoch: 525, Loss: 1.7184875105158426e-05\n",
      "Epoch: 526, Loss: 1.7304077118751593e-05\n",
      "Epoch: 527, Loss: 1.66084664670052e-05\n",
      "Epoch: 528, Loss: 1.655177038628608e-05\n",
      "Epoch: 529, Loss: 1.689267264737282e-05\n",
      "Epoch: 530, Loss: 1.6186153516173363e-05\n",
      "Epoch: 531, Loss: 1.6628086086711846e-05\n",
      "Epoch: 532, Loss: 1.6229041648330167e-05\n",
      "Epoch: 533, Loss: 1.621086266823113e-05\n",
      "Epoch: 534, Loss: 1.560247255838476e-05\n",
      "Epoch: 535, Loss: 1.5886682376731187e-05\n",
      "Epoch: 536, Loss: 1.5662075384170748e-05\n",
      "Epoch: 537, Loss: 1.5167079254752025e-05\n",
      "Epoch: 538, Loss: 1.5038422134239227e-05\n",
      "Epoch: 539, Loss: 1.52005131894839e-05\n",
      "Epoch: 540, Loss: 1.509875164629193e-05\n",
      "Epoch: 541, Loss: 1.4966459275456145e-05\n",
      "Epoch: 542, Loss: 1.458775659557432e-05\n",
      "Epoch: 543, Loss: 1.4536877642967738e-05\n",
      "Epoch: 544, Loss: 1.4670620657852851e-05\n",
      "Epoch: 545, Loss: 1.4347889191412833e-05\n",
      "Epoch: 546, Loss: 1.4171258044370916e-05\n",
      "Epoch: 547, Loss: 1.4408946299226955e-05\n",
      "Epoch: 548, Loss: 1.4017887224326842e-05\n",
      "Epoch: 549, Loss: 1.3797644896840211e-05\n",
      "Epoch: 550, Loss: 1.3610838323074859e-05\n",
      "Epoch: 551, Loss: 1.3885597581975162e-05\n",
      "Epoch: 552, Loss: 1.3293920346768573e-05\n",
      "Epoch: 553, Loss: 1.3216144907346461e-05\n",
      "Epoch: 554, Loss: 1.3292465155245736e-05\n",
      "Epoch: 555, Loss: 1.306931608269224e-05\n",
      "Epoch: 556, Loss: 1.291449098062003e-05\n",
      "Epoch: 557, Loss: 1.2510347914940212e-05\n",
      "Epoch: 558, Loss: 1.2584488104039337e-05\n",
      "Epoch: 559, Loss: 1.246455758519005e-05\n",
      "Epoch: 560, Loss: 1.2312639228184707e-05\n",
      "Epoch: 561, Loss: 1.2494358088588342e-05\n",
      "Epoch: 562, Loss: 1.2263938515388872e-05\n",
      "Epoch: 563, Loss: 1.2309002158872318e-05\n",
      "Epoch: 564, Loss: 1.2101843822165392e-05\n",
      "Epoch: 565, Loss: 1.1943386198254302e-05\n",
      "Epoch: 566, Loss: 1.1749309123842977e-05\n",
      "Epoch: 567, Loss: 1.1690431165334303e-05\n",
      "Epoch: 568, Loss: 1.1342984180373605e-05\n",
      "Epoch: 569, Loss: 1.157485803560121e-05\n",
      "Epoch: 570, Loss: 1.1342257494106889e-05\n",
      "Epoch: 571, Loss: 1.1363332305336371e-05\n",
      "Epoch: 572, Loss: 1.1306640772090759e-05\n",
      "Epoch: 573, Loss: 1.095265270123491e-05\n",
      "Epoch: 574, Loss: 1.1079128853452858e-05\n",
      "Epoch: 575, Loss: 1.0778202522487845e-05\n",
      "Epoch: 576, Loss: 1.0798552466440015e-05\n",
      "Epoch: 577, Loss: 1.047218574967701e-05\n",
      "Epoch: 578, Loss: 1.0499807103769854e-05\n",
      "Epoch: 579, Loss: 1.0731679140008055e-05\n",
      "Epoch: 580, Loss: 1.0422031664347742e-05\n",
      "Epoch: 581, Loss: 1.015381349134259e-05\n",
      "Epoch: 582, Loss: 1.0304274837835692e-05\n",
      "Epoch: 583, Loss: 1.0359515727031976e-05\n",
      "Epoch: 584, Loss: 1.029773284244584e-05\n",
      "Epoch: 585, Loss: 9.968458471121266e-06\n",
      "Epoch: 586, Loss: 9.668985512689687e-06\n",
      "Epoch: 587, Loss: 9.656630027166102e-06\n",
      "Epoch: 588, Loss: 9.647906153986696e-06\n",
      "Epoch: 589, Loss: 9.980813956644852e-06\n",
      "Epoch: 590, Loss: 9.455283361603506e-06\n",
      "Epoch: 591, Loss: 9.413850420969538e-06\n",
      "Epoch: 592, Loss: 9.423300980415661e-06\n",
      "Epoch: 593, Loss: 9.264113032259047e-06\n",
      "Epoch: 594, Loss: 9.18561181606492e-06\n",
      "Epoch: 595, Loss: 8.963186701294035e-06\n",
      "Epoch: 596, Loss: 9.00171016837703e-06\n",
      "Epoch: 597, Loss: 8.932656783144921e-06\n",
      "Epoch: 598, Loss: 8.78364789969055e-06\n",
      "Epoch: 599, Loss: 8.746575986151583e-06\n",
      "Epoch: 600, Loss: 8.435472409473732e-06\n",
      "Epoch: 601, Loss: 8.407125278608873e-06\n",
      "Epoch: 602, Loss: 8.351154974661767e-06\n",
      "Epoch: 603, Loss: 8.629548574390355e-06\n",
      "Epoch: 604, Loss: 8.189786967705004e-06\n",
      "Epoch: 605, Loss: 8.286462616524659e-06\n",
      "Epoch: 606, Loss: 8.164344762917608e-06\n",
      "Epoch: 607, Loss: 8.077119673544075e-06\n",
      "Epoch: 608, Loss: 8.042230547289364e-06\n",
      "Epoch: 609, Loss: 7.76747128838906e-06\n",
      "Epoch: 610, Loss: 7.672975698369555e-06\n",
      "Epoch: 611, Loss: 7.614098649355583e-06\n",
      "Epoch: 612, Loss: 7.762380846543238e-06\n",
      "Epoch: 613, Loss: 7.55231258153799e-06\n",
      "Epoch: 614, Loss: 7.515242032241076e-06\n",
      "Epoch: 615, Loss: 7.475990969396662e-06\n",
      "Epoch: 616, Loss: 7.492709301004652e-06\n",
      "Epoch: 617, Loss: 7.556674063380342e-06\n",
      "Epoch: 618, Loss: 7.332796030823374e-06\n",
      "Epoch: 619, Loss: 7.159798769862391e-06\n",
      "Epoch: 620, Loss: 7.097287834767485e-06\n",
      "Epoch: 621, Loss: 7.013695721980184e-06\n",
      "Epoch: 622, Loss: 6.906117505423026e-06\n",
      "Epoch: 623, Loss: 6.9715351855847985e-06\n",
      "Epoch: 624, Loss: 6.7738242250925396e-06\n",
      "Epoch: 625, Loss: 6.821072474849643e-06\n",
      "Epoch: 626, Loss: 6.641532309004106e-06\n",
      "Epoch: 627, Loss: 6.653888704022393e-06\n",
      "Epoch: 628, Loss: 6.637897968175821e-06\n",
      "Epoch: 629, Loss: 6.504878456325969e-06\n",
      "Epoch: 630, Loss: 6.356595804390963e-06\n",
      "Epoch: 631, Loss: 6.287541964411503e-06\n",
      "Epoch: 632, Loss: 6.24538324700552e-06\n",
      "Epoch: 633, Loss: 6.536855835292954e-06\n",
      "Epoch: 634, Loss: 6.249743819353171e-06\n",
      "Epoch: 635, Loss: 6.230844519450329e-06\n",
      "Epoch: 636, Loss: 6.233025487745181e-06\n",
      "Epoch: 637, Loss: 6.098553058109246e-06\n",
      "Epoch: 638, Loss: 6.0498509810713585e-06\n",
      "Epoch: 639, Loss: 6.006965122651309e-06\n",
      "Epoch: 640, Loss: 5.7961701713793445e-06\n",
      "Epoch: 641, Loss: 5.825971584272338e-06\n",
      "Epoch: 642, Loss: 5.869584128959104e-06\n",
      "Epoch: 643, Loss: 5.806346052850131e-06\n",
      "Epoch: 644, Loss: 5.602819783234736e-06\n",
      "Epoch: 645, Loss: 5.548303306568414e-06\n",
      "Epoch: 646, Loss: 5.514866643352434e-06\n",
      "Epoch: 647, Loss: 5.519227670447435e-06\n",
      "Epoch: 648, Loss: 5.481429980136454e-06\n",
      "Epoch: 649, Loss: 5.445086117106257e-06\n",
      "Epoch: 650, Loss: 5.275722742226208e-06\n",
      "Epoch: 651, Loss: 5.331691681931261e-06\n",
      "Epoch: 652, Loss: 5.164510184840765e-06\n",
      "Epoch: 653, Loss: 5.241559392743511e-06\n",
      "Epoch: 654, Loss: 5.174686066311551e-06\n",
      "Epoch: 655, Loss: 5.097636858408805e-06\n",
      "Epoch: 656, Loss: 5.082371899334248e-06\n",
      "Epoch: 657, Loss: 4.990785328118363e-06\n",
      "Epoch: 658, Loss: 4.849043307331158e-06\n",
      "Epoch: 659, Loss: 5.204481112741632e-06\n",
      "Epoch: 660, Loss: 4.791619630850619e-06\n",
      "Epoch: 661, Loss: 4.835958861804102e-06\n",
      "Epoch: 662, Loss: 4.737830295198364e-06\n",
      "Epoch: 663, Loss: 4.704393631982384e-06\n",
      "Epoch: 664, Loss: 4.593906851368956e-06\n",
      "Epoch: 665, Loss: 4.568466920318315e-06\n",
      "Epoch: 666, Loss: 4.559743956633611e-06\n",
      "Epoch: 667, Loss: 4.524853011389496e-06\n",
      "Epoch: 668, Loss: 4.469610303203808e-06\n",
      "Epoch: 669, Loss: 4.356216777523514e-06\n",
      "Epoch: 670, Loss: 4.407825144880917e-06\n",
      "Epoch: 671, Loss: 4.261722551746061e-06\n",
      "Epoch: 672, Loss: 4.368573627289152e-06\n",
      "Epoch: 673, Loss: 4.3387713049014565e-06\n",
      "Epoch: 674, Loss: 4.214474756736308e-06\n",
      "Epoch: 675, Loss: 4.109077053726651e-06\n",
      "Epoch: 676, Loss: 4.176676611677976e-06\n",
      "Epoch: 677, Loss: 3.993503014498856e-06\n",
      "Epoch: 678, Loss: 4.084362899448024e-06\n",
      "Epoch: 679, Loss: 4.008767064078711e-06\n",
      "Epoch: 680, Loss: 3.9593392102688085e-06\n",
      "Epoch: 681, Loss: 3.934625055990182e-06\n",
      "Epoch: 682, Loss: 3.810328053077683e-06\n",
      "Epoch: 683, Loss: 3.848126198136015e-06\n",
      "Epoch: 684, Loss: 3.811054966718075e-06\n",
      "Epoch: 685, Loss: 3.7885215533606242e-06\n",
      "Epoch: 686, Loss: 3.7638073990819976e-06\n",
      "Epoch: 687, Loss: 3.826319243671605e-06\n",
      "Epoch: 688, Loss: 3.6496869597613113e-06\n",
      "Epoch: 689, Loss: 3.659863068605773e-06\n",
      "Epoch: 690, Loss: 3.6271535464038607e-06\n",
      "Epoch: 691, Loss: 3.6111621284362627e-06\n",
      "Epoch: 692, Loss: 3.545742629285087e-06\n",
      "Epoch: 693, Loss: 3.530478124957881e-06\n",
      "Epoch: 694, Loss: 3.44979457622685e-06\n",
      "Epoch: 695, Loss: 3.410542603887734e-06\n",
      "Epoch: 696, Loss: 3.502127810861566e-06\n",
      "Epoch: 697, Loss: 3.322589464005432e-06\n",
      "Epoch: 698, Loss: 3.2404516332462663e-06\n",
      "Epoch: 699, Loss: 3.2571701922279317e-06\n",
      "Epoch: 700, Loss: 3.2251871289190603e-06\n",
      "Epoch: 701, Loss: 3.2615314466966083e-06\n",
      "Epoch: 702, Loss: 3.1997465157473926e-06\n",
      "Epoch: 703, Loss: 3.195384806531365e-06\n",
      "Epoch: 704, Loss: 3.103797553194454e-06\n",
      "Epoch: 705, Loss: 3.1161546303337673e-06\n",
      "Epoch: 706, Loss: 3.023113549716072e-06\n",
      "Epoch: 707, Loss: 3.0427390811382793e-06\n",
      "Epoch: 708, Loss: 3.0798105399298947e-06\n",
      "Epoch: 709, Loss: 2.993311227328377e-06\n",
      "Epoch: 710, Loss: 2.9351606372074457e-06\n",
      "Epoch: 711, Loss: 2.871921878977446e-06\n",
      "Epoch: 712, Loss: 2.8631993700400926e-06\n",
      "Epoch: 713, Loss: 2.9177151645853883e-06\n",
      "Epoch: 714, Loss: 2.845026983777643e-06\n",
      "Epoch: 715, Loss: 2.8137712888565147e-06\n",
      "Epoch: 716, Loss: 2.8174056296847994e-06\n",
      "Epoch: 717, Loss: 2.727271976254997e-06\n",
      "Epoch: 718, Loss: 2.767977321127546e-06\n",
      "Epoch: 719, Loss: 2.6887471449299483e-06\n",
      "Epoch: 720, Loss: 2.738175226113526e-06\n",
      "Epoch: 721, Loss: 2.6429531772009796e-06\n",
      "Epoch: 722, Loss: 2.6494951725908322e-06\n",
      "Epoch: 723, Loss: 2.6262348455929896e-06\n",
      "Epoch: 724, Loss: 2.542643414926715e-06\n",
      "Epoch: 725, Loss: 2.5259250833187252e-06\n",
      "Epoch: 726, Loss: 2.504845269868383e-06\n",
      "Epoch: 727, Loss: 2.5121139515249524e-06\n",
      "Epoch: 728, Loss: 2.640045295265736e-06\n",
      "Epoch: 729, Loss: 2.5048450424947077e-06\n",
      "Epoch: 730, Loss: 2.4168923573597567e-06\n",
      "Epoch: 731, Loss: 2.44960233430902e-06\n",
      "Epoch: 732, Loss: 2.3929053440951975e-06\n",
      "Epoch: 733, Loss: 2.3282127585844137e-06\n",
      "Epoch: 734, Loss: 2.340569380976376e-06\n",
      "Epoch: 735, Loss: 2.331119958398631e-06\n",
      "Epoch: 736, Loss: 2.2933222680876497e-06\n",
      "Epoch: 737, Loss: 2.279511591041228e-06\n",
      "Epoch: 738, Loss: 2.284599531776621e-06\n",
      "Epoch: 739, Loss: 2.1835628558619646e-06\n",
      "Epoch: 740, Loss: 2.2082770101405913e-06\n",
      "Epoch: 741, Loss: 2.174840119550936e-06\n",
      "Epoch: 742, Loss: 2.1464918518177e-06\n",
      "Epoch: 743, Loss: 2.14285728361574e-06\n",
      "Epoch: 744, Loss: 2.09633685699373e-06\n",
      "Epoch: 745, Loss: 2.0956097159796627e-06\n",
      "Epoch: 746, Loss: 2.075257043543388e-06\n",
      "Epoch: 747, Loss: 2.0258289623598102e-06\n",
      "Epoch: 748, Loss: 2.0614463664969662e-06\n",
      "Epoch: 749, Loss: 2.0432742076081922e-06\n",
      "Epoch: 750, Loss: 2.0076568034710363e-06\n",
      "Epoch: 751, Loss: 1.9451445041340776e-06\n",
      "Epoch: 752, Loss: 1.9509595858835382e-06\n",
      "Epoch: 753, Loss: 1.9371489088371163e-06\n",
      "Epoch: 754, Loss: 1.9189767499483423e-06\n",
      "Epoch: 755, Loss: 1.943690904226969e-06\n",
      "Epoch: 756, Loss: 1.882632545857632e-06\n",
      "Epoch: 757, Loss: 1.8935357957161614e-06\n",
      "Epoch: 758, Loss: 1.8302969237993238e-06\n",
      "Epoch: 759, Loss: 1.8412000599710154e-06\n",
      "Epoch: 760, Loss: 1.7910450651470455e-06\n",
      "Epoch: 761, Loss: 1.8339313783144462e-06\n",
      "Epoch: 762, Loss: 1.740889956636238e-06\n",
      "Epoch: 763, Loss: 1.72489853866864e-06\n",
      "Epoch: 764, Loss: 1.7387093294018996e-06\n",
      "Epoch: 765, Loss: 1.6987305571092293e-06\n",
      "Epoch: 766, Loss: 1.7387091020282242e-06\n",
      "Epoch: 767, Loss: 1.7372551610606024e-06\n",
      "Epoch: 768, Loss: 1.6405800806751358e-06\n",
      "Epoch: 769, Loss: 1.6711090893295477e-06\n",
      "Epoch: 770, Loss: 1.6318572306772694e-06\n",
      "Epoch: 771, Loss: 1.638399226067122e-06\n",
      "Epoch: 772, Loss: 1.5911518858047202e-06\n",
      "Epoch: 773, Loss: 1.574433667883568e-06\n",
      "Epoch: 774, Loss: 1.5540808817604557e-06\n",
      "Epoch: 775, Loss: 1.5497195136049413e-06\n",
      "Epoch: 776, Loss: 1.53808935010602e-06\n",
      "Epoch: 777, Loss: 1.5068332004375407e-06\n",
      "Epoch: 778, Loss: 1.5657108178857015e-06\n",
      "Epoch: 779, Loss: 1.485753614360874e-06\n",
      "Epoch: 780, Loss: 1.4872073279548204e-06\n",
      "Epoch: 781, Loss: 1.4523169511448941e-06\n",
      "Epoch: 782, Loss: 1.4283297105066595e-06\n",
      "Epoch: 783, Loss: 1.4225146287571988e-06\n",
      "Epoch: 784, Loss: 1.4341447922561201e-06\n",
      "Epoch: 785, Loss: 1.4421406149267568e-06\n",
      "Epoch: 786, Loss: 1.3803553429170279e-06\n",
      "Epoch: 787, Loss: 1.4021617289472488e-06\n",
      "Epoch: 788, Loss: 1.346918679701048e-06\n",
      "Epoch: 789, Loss: 1.3643638112625922e-06\n",
      "Epoch: 790, Loss: 1.3316540616870043e-06\n",
      "Epoch: 791, Loss: 1.3127549891578383e-06\n",
      "Epoch: 792, Loss: 1.3105743619235e-06\n",
      "Epoch: 793, Loss: 1.2865871212852653e-06\n",
      "Epoch: 794, Loss: 1.2829527804569807e-06\n",
      "Epoch: 795, Loss: 1.2793183259418583e-06\n",
      "Epoch: 796, Loss: 1.2611460533662466e-06\n",
      "Epoch: 797, Loss: 1.24006646728958e-06\n",
      "Epoch: 798, Loss: 1.2211675084472517e-06\n",
      "Epoch: 799, Loss: 1.2291633311178884e-06\n",
      "Epoch: 800, Loss: 1.2211675084472517e-06\n",
      "Epoch: 801, Loss: 1.1768274816859048e-06\n",
      "Epoch: 802, Loss: 1.1768274816859048e-06\n",
      "Epoch: 803, Loss: 1.1513865274537238e-06\n",
      "Epoch: 804, Loss: 1.1586554364839685e-06\n",
      "Epoch: 805, Loss: 1.1433907047830871e-06\n",
      "Epoch: 806, Loss: 1.1295800277366652e-06\n",
      "Epoch: 807, Loss: 1.1310336276437738e-06\n",
      "Epoch: 808, Loss: 1.1034121598640922e-06\n",
      "Epoch: 809, Loss: 1.0990507917085779e-06\n",
      "Epoch: 810, Loss: 1.1150422096761758e-06\n",
      "Epoch: 811, Loss: 1.065614128492598e-06\n",
      "Epoch: 812, Loss: 1.1273992868154892e-06\n",
      "Epoch: 813, Loss: 1.077244405678357e-06\n",
      "Epoch: 814, Loss: 1.0423538014947553e-06\n",
      "Epoch: 815, Loss: 1.0278160971211037e-06\n",
      "Epoch: 816, Loss: 1.0343583198846318e-06\n",
      "Epoch: 817, Loss: 1.0052827974504908e-06\n",
      "Epoch: 818, Loss: 1.0190934744969127e-06\n",
      "Epoch: 819, Loss: 9.965601748262998e-07\n",
      "Epoch: 820, Loss: 9.703923069537268e-07\n",
      "Epoch: 821, Loss: 9.98740802060638e-07\n",
      "Epoch: 822, Loss: 9.587621434548055e-07\n",
      "Epoch: 823, Loss: 9.427706686437887e-07\n",
      "Epoch: 824, Loss: 9.500396345174522e-07\n",
      "Epoch: 825, Loss: 9.420438118468155e-07\n",
      "Epoch: 826, Loss: 9.27506050629745e-07\n",
      "Epoch: 827, Loss: 9.318673619418405e-07\n",
      "Epoch: 828, Loss: 9.202371984429192e-07\n",
      "Epoch: 829, Loss: 9.238716529580415e-07\n",
      "Epoch: 830, Loss: 9.122414894591202e-07\n",
      "Epoch: 831, Loss: 8.860735647431284e-07\n",
      "Epoch: 832, Loss: 8.795316261966946e-07\n",
      "Epoch: 833, Loss: 8.860735647431284e-07\n",
      "Epoch: 834, Loss: 8.773509421189374e-07\n",
      "Epoch: 835, Loss: 8.577250127927982e-07\n",
      "Epoch: 836, Loss: 8.584519264331902e-07\n",
      "Epoch: 837, Loss: 8.402798243878351e-07\n",
      "Epoch: 838, Loss: 8.271958336081298e-07\n",
      "Epoch: 839, Loss: 8.446410788565117e-07\n",
      "Epoch: 840, Loss: 8.279228040919406e-07\n",
      "Epoch: 841, Loss: 8.599056968705554e-07\n",
      "Epoch: 842, Loss: 8.184732678273576e-07\n",
      "Epoch: 843, Loss: 8.112044156405318e-07\n",
      "Epoch: 844, Loss: 8.061161906880443e-07\n",
      "Epoch: 845, Loss: 8.104775020001398e-07\n",
      "Epoch: 846, Loss: 8.097506452031666e-07\n",
      "Epoch: 847, Loss: 7.930323704385955e-07\n",
      "Epoch: 848, Loss: 7.843096341275668e-07\n",
      "Epoch: 849, Loss: 7.559611958640744e-07\n",
      "Epoch: 850, Loss: 7.872172886891349e-07\n",
      "Epoch: 851, Loss: 7.559611958640744e-07\n",
      "Epoch: 852, Loss: 7.406965778500307e-07\n",
      "Epoch: 853, Loss: 7.479654300368566e-07\n",
      "Epoch: 854, Loss: 7.174361940087692e-07\n",
      "Epoch: 855, Loss: 7.217975621642836e-07\n",
      "Epoch: 856, Loss: 7.188899644461344e-07\n",
      "Epoch: 857, Loss: 7.247051030390139e-07\n",
      "Epoch: 858, Loss: 7.050791737128748e-07\n",
      "Epoch: 859, Loss: 7.07986771431024e-07\n",
      "Epoch: 860, Loss: 7.058060305098479e-07\n",
      "Epoch: 861, Loss: 6.832725603089784e-07\n",
      "Epoch: 862, Loss: 6.832726739958161e-07\n",
      "Epoch: 863, Loss: 6.789113626837207e-07\n",
      "Epoch: 864, Loss: 6.636466878262581e-07\n",
      "Epoch: 865, Loss: 6.629198878727038e-07\n",
      "Epoch: 866, Loss: 6.578316629202163e-07\n",
      "Epoch: 867, Loss: 6.643736014666501e-07\n",
      "Epoch: 868, Loss: 6.563777787960134e-07\n",
      "Epoch: 869, Loss: 6.411132744688075e-07\n",
      "Epoch: 870, Loss: 6.309368814072513e-07\n",
      "Epoch: 871, Loss: 6.207604883456952e-07\n",
      "Epoch: 872, Loss: 6.27302426892129e-07\n",
      "Epoch: 873, Loss: 6.18579804267938e-07\n",
      "Epoch: 874, Loss: 6.134916361588694e-07\n",
      "Epoch: 875, Loss: 6.047690135346784e-07\n",
      "Epoch: 876, Loss: 6.033152430973132e-07\n",
      "Epoch: 877, Loss: 5.873237114428775e-07\n",
      "Epoch: 878, Loss: 5.822355433338089e-07\n",
      "Epoch: 879, Loss: 5.749666911469831e-07\n",
      "Epoch: 880, Loss: 5.640633844450349e-07\n",
      "Epoch: 881, Loss: 5.756935479439562e-07\n",
      "Epoch: 882, Loss: 5.54613791337033e-07\n",
      "Epoch: 883, Loss: 5.546138481804519e-07\n",
      "Epoch: 884, Loss: 5.306266075422172e-07\n",
      "Epoch: 885, Loss: 5.371686029320699e-07\n",
      "Epoch: 886, Loss: 5.306266075422172e-07\n",
      "Epoch: 887, Loss: 5.277190666674869e-07\n",
      "Epoch: 888, Loss: 5.131813622938353e-07\n",
      "Epoch: 889, Loss: 4.921017193737498e-07\n",
      "Epoch: 890, Loss: 4.971899443262373e-07\n",
      "Epoch: 891, Loss: 4.93555489811115e-07\n",
      "Epoch: 892, Loss: 4.6811447873551515e-07\n",
      "Epoch: 893, Loss: 4.6520688101736596e-07\n",
      "Epoch: 894, Loss: 4.6520688101736596e-07\n",
      "Epoch: 895, Loss: 4.586649708926416e-07\n",
      "Epoch: 896, Loss: 4.5284983229976206e-07\n",
      "Epoch: 897, Loss: 4.426734392382059e-07\n",
      "Epoch: 898, Loss: 4.383121279261104e-07\n",
      "Epoch: 899, Loss: 4.3976589836347557e-07\n",
      "Epoch: 900, Loss: 4.1650557136563293e-07\n",
      "Epoch: 901, Loss: 4.0778294874144194e-07\n",
      "Epoch: 902, Loss: 4.1505180092826777e-07\n",
      "Epoch: 903, Loss: 4.1359800206919317e-07\n",
      "Epoch: 904, Loss: 4.0705606352275936e-07\n",
      "Epoch: 905, Loss: 3.9179147393042513e-07\n",
      "Epoch: 906, Loss: 3.830688228845247e-07\n",
      "Epoch: 907, Loss: 3.867032489779376e-07\n",
      "Epoch: 908, Loss: 3.699848889482382e-07\n",
      "Epoch: 909, Loss: 3.6126226632404723e-07\n",
      "Epoch: 910, Loss: 3.539934141372214e-07\n",
      "Epoch: 911, Loss: 3.525396152781468e-07\n",
      "Epoch: 912, Loss: 3.496321028251259e-07\n",
      "Epoch: 913, Loss: 3.3436748481108225e-07\n",
      "Epoch: 914, Loss: 3.3654814046713e-07\n",
      "Epoch: 915, Loss: 3.263717189838644e-07\n",
      "Epoch: 916, Loss: 3.249179769682087e-07\n",
      "Epoch: 917, Loss: 3.1328778504757793e-07\n",
      "Epoch: 918, Loss: 3.0965338737587444e-07\n",
      "Epoch: 919, Loss: 2.9584253979919595e-07\n",
      "Epoch: 920, Loss: 3.0020385111129144e-07\n",
      "Epoch: 921, Loss: 2.8711991717500496e-07\n",
      "Epoch: 922, Loss: 2.929349989244656e-07\n",
      "Epoch: 923, Loss: 2.885736876123701e-07\n",
      "Epoch: 924, Loss: 2.791241513477871e-07\n",
      "Epoch: 925, Loss: 2.718552991609613e-07\n",
      "Epoch: 926, Loss: 2.776704093321314e-07\n",
      "Epoch: 927, Loss: 2.631326765367703e-07\n",
      "Epoch: 928, Loss: 2.6894772986452153e-07\n",
      "Epoch: 929, Loss: 2.769435241134488e-07\n",
      "Epoch: 930, Loss: 2.500487426004838e-07\n",
      "Epoch: 931, Loss: 2.4350674721063115e-07\n",
      "Epoch: 932, Loss: 2.471412017257535e-07\n",
      "Epoch: 933, Loss: 2.5368316869389673e-07\n",
      "Epoch: 934, Loss: 2.551369391312619e-07\n",
      "Epoch: 935, Loss: 2.1806575034588604e-07\n",
      "Epoch: 936, Loss: 2.1443132425247313e-07\n",
      "Epoch: 937, Loss: 2.202464060019338e-07\n",
      "Epoch: 938, Loss: 2.2315394687666412e-07\n",
      "Epoch: 939, Loss: 2.2242706165798154e-07\n",
      "Epoch: 940, Loss: 2.1079689815906022e-07\n",
      "Epoch: 941, Loss: 2.1370443903379055e-07\n",
      "Epoch: 942, Loss: 2.0498180219874484e-07\n",
      "Epoch: 943, Loss: 2.0498180219874484e-07\n",
      "Epoch: 944, Loss: 2.0788934307347517e-07\n",
      "Epoch: 945, Loss: 1.9698606479323644e-07\n",
      "Epoch: 946, Loss: 2.0062049088664935e-07\n",
      "Epoch: 947, Loss: 1.9625917957455385e-07\n",
      "Epoch: 948, Loss: 1.897171983955559e-07\n",
      "Epoch: 949, Loss: 1.8535590129431512e-07\n",
      "Epoch: 950, Loss: 2.0062049088664935e-07\n",
      "Epoch: 951, Loss: 1.8317524563826737e-07\n",
      "Epoch: 952, Loss: 1.809945757713649e-07\n",
      "Epoch: 953, Loss: 1.8244834620873007e-07\n",
      "Epoch: 954, Loss: 1.7881390590446244e-07\n",
      "Epoch: 955, Loss: 1.7881392011531716e-07\n",
      "Epoch: 956, Loss: 1.7081818270980875e-07\n",
      "Epoch: 957, Loss: 1.693644122724436e-07\n",
      "Epoch: 958, Loss: 1.68637527053761e-07\n",
      "Epoch: 959, Loss: 1.635493163121282e-07\n",
      "Epoch: 960, Loss: 1.7009129749112617e-07\n",
      "Epoch: 961, Loss: 1.5773423456266755e-07\n",
      "Epoch: 962, Loss: 1.5773423456266755e-07\n",
      "Epoch: 963, Loss: 1.5482669368793722e-07\n",
      "Epoch: 964, Loss: 1.511922533836696e-07\n",
      "Epoch: 965, Loss: 1.5409980846925464e-07\n",
      "Epoch: 966, Loss: 1.5337292325057206e-07\n",
      "Epoch: 967, Loss: 1.5264602382103476e-07\n",
      "Epoch: 968, Loss: 1.4828471250893926e-07\n",
      "Epoch: 969, Loss: 1.4610405685289152e-07\n",
      "Epoch: 970, Loss: 1.4610405685289152e-07\n",
      "Epoch: 971, Loss: 1.424696307594786e-07\n",
      "Epoch: 972, Loss: 1.4901159772762185e-07\n",
      "Epoch: 973, Loss: 1.5482669368793722e-07\n",
      "Epoch: 974, Loss: 1.4101586032211344e-07\n",
      "Epoch: 975, Loss: 1.4755782729025668e-07\n",
      "Epoch: 976, Loss: 1.4028897510343086e-07\n",
      "Epoch: 977, Loss: 1.337469939244329e-07\n",
      "Epoch: 978, Loss: 1.3956208988474827e-07\n",
      "Epoch: 979, Loss: 1.3592764958048065e-07\n",
      "Epoch: 980, Loss: 1.381083052365284e-07\n",
      "Epoch: 981, Loss: 1.381083052365284e-07\n",
      "Epoch: 982, Loss: 1.4028897510343086e-07\n",
      "Epoch: 983, Loss: 1.3083945304970257e-07\n",
      "Epoch: 984, Loss: 1.381083052365284e-07\n",
      "Epoch: 985, Loss: 1.206630457772917e-07\n",
      "Epoch: 986, Loss: 1.3156633826838515e-07\n",
      "Epoch: 987, Loss: 1.206630457772917e-07\n",
      "Epoch: 988, Loss: 1.2793191217497224e-07\n",
      "Epoch: 989, Loss: 1.3011256783102e-07\n",
      "Epoch: 990, Loss: 1.163017344651962e-07\n",
      "Epoch: 991, Loss: 1.163017344651962e-07\n",
      "Epoch: 992, Loss: 1.1702861968387879e-07\n",
      "Epoch: 993, Loss: 1.1702861968387879e-07\n",
      "Epoch: 994, Loss: 1.1775550490256137e-07\n",
      "Epoch: 995, Loss: 1.1775550490256137e-07\n",
      "Epoch: 996, Loss: 1.097597603916256e-07\n",
      "Epoch: 997, Loss: 1.1194041604767335e-07\n",
      "Epoch: 998, Loss: 1.163017344651962e-07\n",
      "Epoch: 999, Loss: 1.0685221951689527e-07\n",
      "Epoch: 1000, Loss: 1.03217786318055e-07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transformer = Transformer( tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(source_data)\n",
    "    \n",
    "    # Adjust target_data to have the same length as model output\n",
    "    target_data_adjusted = target_data[:, :output.size(1)]\n",
    "    \n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), target_data_adjusted.contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence:  ⁇  one day, a little girl named lily found a needle in her room. she knew it was difficult to play with it because it was sharp. lily wanted to share the needle with her mom, so she could sew a button on her shirt. lily went to her mom and said, \"mom, i found this needle. can you share it with me and sew my shirt?\" her mom smiled and said, \"yes, lily, we can share the needle and fix your shirt.\" together, they shared the needle and sewed the button on lily's shirt. it was not difficult for them because they were sharing and helping each other. after they finished, lily thanked her mom for sharing the needle and fixing her shirt. they both felt happy because they had shared and worked together. they had shared and worked together. they had shared and worked together. they had shared and worked together. they had shared and worked together. they had shared and fixing her mom\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "def word_to_token_id(word, sp_model):\n",
    "    # Convert word to token ID using SentencePiece\n",
    "    return sp_model.piece_to_id(word)\n",
    "\n",
    "def generate_text(model, sp_model, starting_word, ending_word, max_length=200):\n",
    "    # Convert starting and ending words to token IDs\n",
    "    starting_token_id = word_to_token_id(starting_word, sp_model)\n",
    "    if starting_token_id is None:\n",
    "        raise ValueError(f\"Starting word '{starting_word}' not found in vocabulary.\")\n",
    "    ending_token_id = word_to_token_id(ending_word, sp_model)\n",
    "    if ending_token_id is None:\n",
    "        raise ValueError(f\"Ending word '{ending_word}' not found in vocabulary.\")\n",
    "    \n",
    "    generated_sequence = [starting_token_id]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            input_tensor = torch.tensor([generated_sequence])\n",
    "            output = model(input_tensor)\n",
    "            predicted_token = output.argmax(-1)[:,-1].item()\n",
    "            generated_sequence.append(predicted_token)\n",
    "            if predicted_token == ending_token_id:\n",
    "                break\n",
    "    \n",
    "    # Convert token IDs to words using SentencePiece\n",
    "    generated_text = sp_model.decode_ids(generated_sequence)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "starting_word = \"<sos>\"\n",
    "ending_word = \"</sos>\"\n",
    "generated_sequence = generate_text(transformer, sp, starting_word, ending_word)\n",
    "print(\"Generated sequence:\", generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(transformer.state_dict(), 'transformer_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
